{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMuTI77FfZxGb3lCGrMsmon",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aing-HyeonWoo/mitmit/blob/main/realNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zQkSANNFnxeB",
        "outputId": "76b58166-9d13-490a-dedb-370c0565b324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Collecting numpy==1.26.0\n",
            "  Using cached numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 1.11.0, 1.14.0rc1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy==1.13.3 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0, 1.8.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy==1.13.3\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting spacy==3.7.2\n",
            "  Using cached spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: en-core-web-sm==3.7.1 in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (3.0.9)\n",
            "Collecting thinc<8.3.0,>=8.1.8 (from spacy==3.7.2)\n",
            "  Using cached thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2) (2.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2024.12.14)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy==3.7.2)\n",
            "  Using cached blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy==3.7.2)\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.2) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.7.2) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.2.1)\n",
            "Using cached spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Using cached thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Using cached blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "Installing collected packages: numpy, blis, thinc, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.1.0\n",
            "    Uninstalling blis-1.1.0:\n",
            "      Successfully uninstalled blis-1.1.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.3\n",
            "    Uninstalling thinc-8.3.3:\n",
            "      Successfully uninstalled thinc-8.3.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.3\n",
            "    Uninstalling spacy-3.8.3:\n",
            "      Successfully uninstalled spacy-3.8.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "convokit 3.1.0 requires numpy>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "convokit 3.1.0 requires spacy>=3.8.2, but you have spacy 3.7.2 which is incompatible.\n",
            "convokit 3.1.0 requires thinc<8.4.0,>=8.3.0, but you have thinc 8.2.5 which is incompatible.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 numpy-1.26.4 spacy-3.7.2 thinc-8.2.5\n",
            "Requirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: langchain==0.3.12 in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: pytensor==2.26.4 in /usr/local/lib/python3.10/dist-packages (2.26.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim==4.3.3)\n",
            "  Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3) (6.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (0.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.12) (9.0.0)\n",
            "Requirement already satisfied: setuptools>=59.0.0 in /usr/local/lib/python3.10/dist-packages (from pytensor==2.26.4) (75.1.0)\n",
            "Requirement already satisfied: filelock>=3.15 in /usr/local/lib/python3.10/dist-packages (from pytensor==2.26.4) (3.16.1)\n",
            "Requirement already satisfied: etuples in /usr/local/lib/python3.10/dist-packages (from pytensor==2.26.4) (0.3.9)\n",
            "Requirement already satisfied: logical-unification in /usr/local/lib/python3.10/dist-packages (from pytensor==2.26.4) (0.4.6)\n",
            "Requirement already satisfied: miniKanren in /usr/local/lib/python3.10/dist-packages (from pytensor==2.26.4) (1.0.3)\n",
            "Requirement already satisfied: cons in /usr/local/lib/python3.10/dist-packages (from pytensor==2.26.4) (0.4.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.12) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.12) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.12) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.12) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.12) (3.1.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from logical-unification->pytensor==2.26.4) (0.12.1)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from logical-unification->pytensor==2.26.4) (1.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.2.2)\n",
            "Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.0\n",
            "    Uninstalling scipy-1.15.0:\n",
            "      Successfully uninstalled scipy-1.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "convokit 3.1.0 requires numpy>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "convokit 3.1.0 requires scipy>1.14, but you have scipy 1.13.1 which is incompatible.\n",
            "convokit 3.1.0 requires spacy>=3.8.2, but you have spacy 3.7.2 which is incompatible.\n",
            "convokit 3.1.0 requires thinc<8.4.0,>=8.3.0, but you have thinc 8.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.13.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: convokit in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.10.0)\n",
            "Collecting scipy>1.14 (from convokit)\n",
            "  Using cached scipy-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.2.2)\n",
            "Collecting numpy>=2.0.0 (from convokit)\n",
            "  Using cached numpy-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.4.8)\n",
            "Collecting spacy>=3.8.2 (from convokit)\n",
            "  Using cached spacy-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.6.0)\n",
            "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.3.8)\n",
            "Requirement already satisfied: clean-text>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.6.0)\n",
            "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.8)\n",
            "Requirement already satisfied: pymongo>=4.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.2)\n",
            "Requirement already satisfied: dnspython>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.7.0)\n",
            "Collecting thinc<8.4.0,>=8.3.0 (from convokit)\n",
            "  Using cached thinc-8.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: h5py==3.12.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.12.1)\n",
            "Requirement already satisfied: numexpr>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.10.2)\n",
            "Requirement already satisfied: ruff>=0.4.8 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.8.6)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.10/dist-packages (from convokit) (1.4.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from convokit) (0.14.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from convokit) (0.45.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from convokit) (4.47.1)\n",
            "Requirement already satisfied: trl>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.13.0)\n",
            "Requirement already satisfied: tensorflow>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.18.0)\n",
            "Requirement already satisfied: tf-keras<3.0.0,>=2.17.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.18.0)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from clean-text>=0.6.0->convokit) (6.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->convokit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->convokit) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->convokit) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (0.3.4)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (0.9.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (4.25.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (3.5.0)\n",
            "Collecting numpy>=2.0.0 (from convokit)\n",
            "  Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (0.37.1)\n",
            "Collecting blis<1.2.0,>=1.1.0 (from thinc<8.4.0,>=8.3.0->convokit)\n",
            "  Using cached blis-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->convokit) (0.1.5)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from trl>=0.12.2->convokit) (3.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl>=0.12.2->convokit) (13.9.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->convokit) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->convokit) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->convokit) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->convokit) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->convokit) (3.16.1)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->convokit) (0.21.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.18.0->convokit) (0.45.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl>=0.12.2->convokit) (17.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl>=0.12.2->convokit) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl>=0.12.2->convokit) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl>=0.12.2->convokit) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl>=0.12.2->convokit) (3.11.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit) (0.13.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.18.0->convokit) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.18.0->convokit) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.18.0->convokit) (3.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->convokit) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->convokit) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate->convokit) (1.3.0)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (0.16.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.8.2->convokit) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl>=0.12.2->convokit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl>=0.12.2->convokit) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl>=0.12.2->convokit) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl>=0.12.2->convokit) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl>=0.12.2->convokit) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl>=0.12.2->convokit) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl>=0.12.2->convokit) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl>=0.12.2->convokit) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl>=0.12.2->convokit) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl>=0.12.2->convokit) (1.18.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl>=0.12.2->convokit) (0.1.2)\n",
            "Using cached scipy-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
            "Using cached spacy-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n",
            "Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "Using cached thinc-8.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "Using cached blis-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "Installing collected packages: numpy, scipy, blis, thinc, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.2\n",
            "    Uninstalling spacy-3.7.2:\n",
            "      Successfully uninstalled spacy-3.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.3 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.0 which is incompatible.\n",
            "langchain 0.3.12 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.1.0 numpy-2.0.2 scipy-1.15.0 spacy-3.8.3 thinc-8.3.3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
            "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 146, in _get_module_details\n",
            "    return _get_module_details(pkg_main_name, error)\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
            "    __import__(pkg_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/compat.py\", line 4, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/config.py\", line 2, in <module>\n",
            "    import confection\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/confection/__init__.py\", line 35, in <module>\n",
            "    import srsly\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/srsly/__init__.py\", line 5, in <module>\n",
            "    from ._msgpack_api import read_msgpack, write_msgpack, msgpack_dumps, msgpack_loads\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/srsly/_msgpack_api.py\", line 3, in <module>\n",
            "    from . import msgpack\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/srsly/msgpack/__init__.py\", line 18, in <module>\n",
            "    from ._msgpack_numpy import encode_numpy as _encode_numpy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/srsly/msgpack/_msgpack_numpy.py\", line 19, in <module>\n",
            "    import cupy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/cupy/__init__.py\", line 17, in <module>\n",
            "    from cupy import _core  # NOQA\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/cupy/_core/__init__.py\", line 3, in <module>\n",
            "    from cupy._core import core  # NOQA\n",
            "  File \"cupy/_core/core.pyx\", line 1, in init cupy._core.core\n",
            "  File \"cupy/_core/_scalar.pyx\", line 64, in init cupy._core._scalar\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\", line 397, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\n"
          ]
        }
      ],
      "source": [
        "# 1. pip 업그레이드 (최신 pip는 의존성 문제를 해결하는 데 더 나음)\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# 2. 특정 패키지의 호환 가능한 버전 설치\n",
        "!pip install numpy==1.26.0 scipy==1.13.3 fsspec==2024.10.0\n",
        "\n",
        "# 3. spacy 및 관련 패키지 설치\n",
        "!pip install spacy==3.7.2 en-core-web-sm==3.7.1\n",
        "\n",
        "# 4. 기타 의존성 패키지 설치\n",
        "!pip install gensim==4.3.3 langchain==0.3.12 pytensor==2.26.4\n",
        "\n",
        "# 5. 추가 설치: nltk 및 Convokit\n",
        "!pip install nltk convokit\n",
        "!python3 -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3!pip install jsonlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrDUBYemt4c-",
        "outputId": "e58b91e5-c405-499b-cbf9-a5b10db3da31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (24.3.0)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "from convokit import Corpus, download\n",
        "#nltk.download('punkt')\n",
        "dataset_path = download(\"movie-corpus\")\n",
        "corpus = Corpus(filename=dataset_path)# import dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21PQ91CDn0Bk",
        "outputId": "3b399bdd-97cf-4698-c1d2-00e7662ca3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading movie-corpus to /root/.convokit/saved-corpora/movie-corpus\n",
            "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import codecs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import unicodedata\n",
        "import random\n",
        "import itertools\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "I-pCYnstvD2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 확인\n",
        "import os\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(dataset_path, \"utterances.jsonl\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6qxSqMMpjC2",
        "outputId": "1c3c37e9-9455-48dc-b63d-c9152e04a2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\'s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you\\'re gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"\\'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I\\'m kidding.  You know how sometimes you just become this \\\\\"persona\\\\\"?  And you don\\'t know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"\\'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\\\"\", \"tag\": \"\\'\\'\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n\\'t\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "def loadLinesAndConversations(fileName):\n",
        "    lines = {}\n",
        "    conversations = {}\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            lineJson = json.loads(line)\n",
        "            lineObj = {}\n",
        "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
        "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
        "            lineObj[\"text\"] = lineJson[\"text\"]\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "\n",
        "            if lineJson[\"conversation_id\"] not in conversations:\n",
        "                convObj = {}\n",
        "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
        "                #convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
        "                convObj[\"lines\"] = [lineObj]\n",
        "            else:\n",
        "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
        "                convObj[\"lines\"].insert(0, lineObj)\n",
        "            conversations[convObj[\"conversationID\"]] = convObj\n",
        "\n",
        "    return lines, conversations\n",
        "\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations.values():\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs\n",
        "\n",
        "# 새 파일에 대한 경로를 정의합니다\n",
        "datafile = os.path.join(dataset_path, \"formatted_movie_lines.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "# 구분자에 대해 unescape 함수를 호출합니다\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# 대사 사전(lines dict)과 대화 사전(conversations dict)을 초기화합니다\n",
        "lines = {}\n",
        "conversations = {}\n",
        "# 대사와 대화를 불러옵니다\n",
        "print(\"\\nProcessing corpus into lines and conversations...\")\n",
        "lines, conversations = loadLinesAndConversations(os.path.join(dataset_path, \"utterances.jsonl\"))\n",
        "\n",
        "# 결과를 새로운 csv 파일로 저장합니다\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "# 몇 줄을 예제 삼아 출력해 봅니다\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geHOt2mrqKBh",
        "outputId": "fcd52b85-77e4-4ed5-9600-8b5148f7d437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing corpus into lines and conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "b'They do to!\\tThey do not!\\n'\n",
            "b'She okay?\\tI hope so.\\n'\n",
            "b\"Wow\\tLet's go.\\n\"\n",
            "b'\"I\\'m kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don\\'t know how to quit?\"\\tNo\\n'\n",
            "b\"No\\tOkay -- you're gonna need to learn how to lie.\\n\"\n",
            "b\"I figured you'd get to the good stuff eventually.\\tWhat good stuff?\\n\"\n",
            "b'What good stuff?\\t\"The \"\"real you\"\".\"\\n'\n",
            "b'\"The \"\"real you\"\".\"\\tLike my fear of wearing pastels?\\n'\n",
            "b'do you listen to this crap?\\tWhat crap?\\n'\n",
            "b\"What crap?\\tMe.  This endless ...blonde babble. I'm like, boring myself.\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 -> 인덱스\n",
        "# 기본 단어 토큰 값\n",
        "PAD_token = 0  # 짧은 문장을 채울(패딩, PADding) 때 사용할 제로 토큰\n",
        "SOS_token = 1  # 문장의 시작(SOS, Start Of Sentence)을 나타내는 토큰\n",
        "EOS_token = 2  # 문장의 끝(EOS, End Of Sentence)을 나태는 토큰\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # SOS, EOS, PAD를 센 것\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # 등장 횟수가 기준 이하인 단어를 정리합니다\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # 사전을 다시 초기화합니다\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # 기본 토큰을 센 것\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "# 전처리\n",
        "MAX_LENGTH = 10  # 고려할 문장의 최대 길이\n",
        "\n",
        "# 유니코드 문자열을 아스키로 변환합니다\n",
        "# https://stackoverflow.com/a/518232/2809427 참고\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# 소문자로 만들고, 공백을 넣고, 알파벳 외의 글자를 제거합니다\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# 질의/응답 쌍을 읽어서 voc 객체를 반환합니다\n",
        "# 질의/응답 쌍을 읽어서 voc 객체를 반환합니다\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    # 파일을 읽고, 쪼개어 lines에 저장합니다\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # 각 줄을 쪼개어 pairs에 저장하고 정규화합니다\n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "        parts = l.split('\\t')\n",
        "        if len(parts) == 2:  # 올바른 형식(탭으로 나뉘어진 두 부분)을 가진 줄만 처리\n",
        "            pairs.append([normalizeString(s) for s in parts])\n",
        "\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "\n",
        "# 문장의 쌍 'p'에 포함된 두 문장이 모두 MAX_LENGTH라는 기준보다 짧은지를 반환합니다\n",
        "# 문장의 쌍 'p'에 포함된 두 문장이 모두 MAX_LENGTH라는 기준보다 짧은지를 반환합니다\n",
        "def filterPair(p):\n",
        "    # 데이터가 올바르게 포함되어 있는지 확인\n",
        "    if len(p) != 2:\n",
        "        return False\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# 조건식 ``filterPair`` 에 따라 pairs를 필터링합니다\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# 앞에서 정의한 함수를 이용하여 만든 voc 객체와 리스트 pairs를 반환합니다\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "\n",
        "# voc와 pairs를 읽고 재구성합니다\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, \"movie-corpus\", datafile, save_dir)\n",
        "# 검증을 위해 pairs의 일부 내용을 출력해 봅니다\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfAx2o75u6OH",
        "outputId": "9f68ffb4-724d-496e-cb51-9d53ef9277ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221085 sentence pairs\n",
            "Trimmed to 64223 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 18066\n",
            "\n",
            "pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['the real you .', 'like my fear of wearing pastels ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n",
            "['but', 'you always been this selfish ?']\n",
            "['have fun tonight ?', 'tons']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 많이 안등장하면 날리기\n",
        "MIN_COUNT = 3    # 제외할 단어의 기준이 되는 등장 횟수\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # MIN_COUNT 미만으로 사용된 단어는 voc에서 제외합니다\n",
        "    voc.trim(MIN_COUNT)\n",
        "    # 제외할 단어가 포함된 경우를 pairs에서도 제외합니다\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # 입력 문장을 검사합니다\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # 출력 문장을 검사합니다\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # 입출력 문장에 제외하기로 한 단어를 포함하지 않는 경우만을 남겨둡니다\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# voc와 pairs를 정돈합니다\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASXmXxkhwAnz",
        "outputId": "cddf4cbd-3a54-4397-8b45-62e162c98d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 7828 / 18063 = 0.4334\n",
            "Trimmed from 64223 pairs to 53057, 0.8261 of total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# 입력 시퀀스 텐서에 패딩한 결과와 lengths를 반환합니다\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# 패딩한 목표 시퀀스 텐서, 패딩 마스크, 그리고 최대 목표 길이를 반환합니다\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# 입력 배치를 이루는 쌍에 대한 모든 아이템을 반환합니다\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "\n",
        "# 검증용 예시\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYv2Xqxuw7cj",
        "outputId": "cba7084a-f443-4623-e46f-6397f98511a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[   8,  128, 3058, 3028,  175],\n",
            "        [ 111,   11,  104,   14,   10],\n",
            "        [ 135,  381,  269,    2,    2],\n",
            "        [ 592, 1001,   36,    0,    0],\n",
            "        [  24,   49,   14,    0,    0],\n",
            "        [ 197,   72,    2,    0,    0],\n",
            "        [  87,   14,    0,    0,    0],\n",
            "        [  14,    2,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "lengths: tensor([9, 8, 6, 3, 3])\n",
            "target_variable: tensor([[   8, 2397,   19,  213,   85],\n",
            "        [ 111,   10,    4, 2489,  105],\n",
            "        [  14,   85,   24,    6, 7093],\n",
            "        [   2,   31,  265,    2,   17],\n",
            "        [   0,   37,   10,    0,  178],\n",
            "        [   0,   50,    2,    0,   14],\n",
            "        [   0,   44,    0,    0,    3],\n",
            "        [   0,   10,    0,    0,  276],\n",
            "        [   0,    2,    0,    0,   14],\n",
            "        [   0,    0,    0,    0,    2]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [False,  True,  True, False,  True],\n",
            "        [False,  True,  True, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False, False, False, False,  True]])\n",
            "max_target_len: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # GRU를 초기화합니다. input_size와 hidden_size 매개변수는 둘 다 'hidden_size'로\n",
        "        # 둡니다. 이는 우리 입력의 크기가 hideen_size 만큼의 피처를 갖는 단어 임베딩이기\n",
        "        # 때문입니다.\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # 단어 인덱스를 임베딩으로 변환합니다\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # RNN 모듈을 위한 패딩된 배치 시퀀스를 패킹합니다\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # GRU로 포워드 패스를 수행합니다\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # 패딩을 언패킹합니다\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # 양방향 GRU의 출력을 합산합니다\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # 출력과 마지막 은닉 상태를 반환합니다\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "# Luong 어텐션 레이어\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Attention 가중치(에너지)를 제안된 방법에 따라 계산합니다\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # max_length와 batch_size의 차원을 뒤집습니다\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # 정규화된 softmax 확률 점수를 반환합니다 (차원을 늘려서)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # 참조를 보존해 둡니다\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # 레이어를 정의합니다\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # 주의: 한 단위 시간에 대해 한 단계(단어)만을 수행합니다\n",
        "        # 현재의 입력 단어에 대한 임베딩을 구합니다\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # 무방향 GRU로 포워드 패스를 수행합니다\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # 현재의 GRU 출력을 바탕으로 어텐션 가중치를 계산합니다\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # 인코더 출력에 어텐션을 곱하여 새로운 \"가중치 합\" 문맥 벡터를 구합니다\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Luong의 논문에 나온 식 5를 이용하여 가중치 문맥 벡터와 GRU 출력을 결합합니다\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Luong의 논문에 나온 식 6을 이용하여 다음 단어를 예측합니다\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # 출력과 마지막 은닉 상태를 반환합니다\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "ivdJVG7bxDBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "metadata": {
        "id": "Ml_XLxO9xYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # 제로 그라디언트\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # device 옵션을 설정합니다\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # RNN 패킹의 길이는 항상 CPU에 위치해야 합니다\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # 변수를 초기화합니다\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # 인코더로 포워드 패스를 수행합니다\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # 초기 디코더 입력을 생성합니다(각 문장을 SOS 토큰으로 시작합니다)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # 디코더의 초기 은닉 상태를 인코더의 마지막 은닉 상태로 둡니다\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # 이번 반복에서 teacher forcing을 사용할지를 결정합니다\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # 배치 시퀀스를 한 번에 하나씩 디코더로 포워드 패스합니다\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing 사용: 다음 입력을 현재의 목표로 둡니다\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # 손실을 계산하고 누적합니다\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing 미사용: 다음 입력을 디코더의 출력으로 둡니다\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # 손실을 계산하고 누적합니다\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # 역전파를 수행합니다\n",
        "    loss.backward()\n",
        "\n",
        "    # 그라디언트 클리핑: 그라디언트를 제자리에서 수정합니다\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # 모델의 가중치를 수정합니다\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "metadata": {
        "id": "5pvur-CayaHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    loss_list = []\n",
        "    # 각 단계에 대한 배치를 읽어옵니다\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # 초기화\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if not loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # 학습 루프\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # 배치에서 각 필드를 읽어옵니다\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # 배치에 대해 학습을 한 단계 진행합니다\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # 경과를 출력합니다\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            loss_list.append(print_loss_avg)\n",
        "            print_loss = 0\n",
        "\n",
        "        # Checkpoint를 저장합니다\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
        "\n",
        "    return loss_list\n"
      ],
      "metadata": {
        "id": "kLeps9aOyoUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # 인코더 모델로 입력을 포워드 패스합니다\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # 인코더의 마지막 은닉 레이어가 디코더의 첫 번째 은닉 레이어의 입력이 되도록 준비합니다\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # 디코더의 첫 번째 입력을 SOS_token으로 초기화합니다\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # 디코더가 단어를 덧붙여 나갈 텐서를 초기화합니다\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # 반복적으로 각 단계마다 하나의 단어 토큰을 디코딩합니다\n",
        "        for _ in range(max_length):\n",
        "            # 디코더로의 포워드 패스를 수행합니다\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # 가장 가능성 높은 단어 토큰과 그 softmax 점수를 구합니다\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # 토큰과 점수를 기록합니다\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # 현재의 토큰을 디코더의 다음 입력으로 준비시킵니다(차원을 증가시켜서)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # 단어 토큰과 점수를 모아서 반환합니다\n",
        "        return all_tokens, all_scores"
      ],
      "metadata": {
        "id": "BYjp5WYsyzS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### 입력 시퀀스를 배치 형태로 만듭니다\n",
        "    # 단어 -> 인덱스\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # lengths 텐서를 만듭니다\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # 배치의 차원을 뒤집어서 모델이 사용하는 형태로 만듭니다\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # 적절한 디바이스를 사용합니다\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    # searcher를 이용하여 문장을 디코딩합니다\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # 인덱스 -> 단어\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # 입력 문장을 받아옵니다\n",
        "            input_sentence = input('> ')\n",
        "            # 종료 조건인지 검사합니다\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # 문장을 정규화합니다\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # 문장을 평가합니다\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # 응답 문장을 형식에 맞춰 출력합니다\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "metadata": {
        "id": "KwoE9Paqy8H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 설정합니다\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#``attn_model = 'general'``\n",
        "#``attn_model = 'concat'``\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# 불러올 checkpoint를 설정합니다. 처음부터 시작할 때는 None으로 둡니다.\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "\n",
        "loadFilename = os.path.join(save_dir, model_name, \"movie-corpus\",\n",
        "                    '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "                    '{}_checkpoint.tar'.format(checkpoint_iter))"
      ],
      "metadata": {
        "id": "tAD01nOdzHB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ``loadFilename`` 이 존재하는 경우에는 모델을 불러옵니다\n",
        "if not loadFilename:\n",
        "    # 모델을 학습할 때와 같은 기기에서 불러오는 경우\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # GPU에서 학습한 모델을 CPU로 불러오는 경우\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# 단어 임베딩을 초기화합니다\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if not loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# 인코더 및 디코더 모델을 초기화합니다\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if not loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# 적절한 디바이스를 사용합니다\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb_RT4UPzLAj",
        "outputId": "8c83ed6c-37ef-4176-9f41-1937cfc85e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 및 최적화 설정\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 15000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Dropout 레이어를 학습 모드로 둡니다\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Optimizer를 초기화합니다\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if not loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# CUDA가 있으면 CUDA를 설정합니다\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# 학습 단계를 수행합니다\n",
        "print(\"Starting Training!\")\n",
        "loss = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, \"movie-corpus\", loadFilename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vVxHyxmizUBn",
        "outputId": "6e10355f-b498-4d6e-87b2-3b978dc216d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Iteration: 10001; Percent complete: 66.7%; Average loss: 1.2362\n",
            "Iteration: 10002; Percent complete: 66.7%; Average loss: 1.2686\n",
            "Iteration: 10003; Percent complete: 66.7%; Average loss: 1.3680\n",
            "Iteration: 10004; Percent complete: 66.7%; Average loss: 1.3441\n",
            "Iteration: 10005; Percent complete: 66.7%; Average loss: 1.3902\n",
            "Iteration: 10006; Percent complete: 66.7%; Average loss: 1.2081\n",
            "Iteration: 10007; Percent complete: 66.7%; Average loss: 1.4220\n",
            "Iteration: 10008; Percent complete: 66.7%; Average loss: 1.2522\n",
            "Iteration: 10009; Percent complete: 66.7%; Average loss: 1.1798\n",
            "Iteration: 10010; Percent complete: 66.7%; Average loss: 1.4544\n",
            "Iteration: 10011; Percent complete: 66.7%; Average loss: 1.2008\n",
            "Iteration: 10012; Percent complete: 66.7%; Average loss: 1.2792\n",
            "Iteration: 10013; Percent complete: 66.8%; Average loss: 1.3331\n",
            "Iteration: 10014; Percent complete: 66.8%; Average loss: 1.2653\n",
            "Iteration: 10015; Percent complete: 66.8%; Average loss: 1.2709\n",
            "Iteration: 10016; Percent complete: 66.8%; Average loss: 1.3504\n",
            "Iteration: 10017; Percent complete: 66.8%; Average loss: 1.2375\n",
            "Iteration: 10018; Percent complete: 66.8%; Average loss: 1.3604\n",
            "Iteration: 10019; Percent complete: 66.8%; Average loss: 1.2808\n",
            "Iteration: 10020; Percent complete: 66.8%; Average loss: 1.3126\n",
            "Iteration: 10021; Percent complete: 66.8%; Average loss: 1.3002\n",
            "Iteration: 10022; Percent complete: 66.8%; Average loss: 1.3121\n",
            "Iteration: 10023; Percent complete: 66.8%; Average loss: 1.2037\n",
            "Iteration: 10024; Percent complete: 66.8%; Average loss: 1.2934\n",
            "Iteration: 10025; Percent complete: 66.8%; Average loss: 1.1907\n",
            "Iteration: 10026; Percent complete: 66.8%; Average loss: 1.2076\n",
            "Iteration: 10027; Percent complete: 66.8%; Average loss: 1.3785\n",
            "Iteration: 10028; Percent complete: 66.9%; Average loss: 1.3686\n",
            "Iteration: 10029; Percent complete: 66.9%; Average loss: 1.2430\n",
            "Iteration: 10030; Percent complete: 66.9%; Average loss: 1.3348\n",
            "Iteration: 10031; Percent complete: 66.9%; Average loss: 1.3724\n",
            "Iteration: 10032; Percent complete: 66.9%; Average loss: 1.4941\n",
            "Iteration: 10033; Percent complete: 66.9%; Average loss: 1.3684\n",
            "Iteration: 10034; Percent complete: 66.9%; Average loss: 1.3748\n",
            "Iteration: 10035; Percent complete: 66.9%; Average loss: 1.0980\n",
            "Iteration: 10036; Percent complete: 66.9%; Average loss: 1.3383\n",
            "Iteration: 10037; Percent complete: 66.9%; Average loss: 1.3819\n",
            "Iteration: 10038; Percent complete: 66.9%; Average loss: 1.3480\n",
            "Iteration: 10039; Percent complete: 66.9%; Average loss: 1.1701\n",
            "Iteration: 10040; Percent complete: 66.9%; Average loss: 1.1022\n",
            "Iteration: 10041; Percent complete: 66.9%; Average loss: 1.2085\n",
            "Iteration: 10042; Percent complete: 66.9%; Average loss: 1.3441\n",
            "Iteration: 10043; Percent complete: 67.0%; Average loss: 1.4132\n",
            "Iteration: 10044; Percent complete: 67.0%; Average loss: 1.3248\n",
            "Iteration: 10045; Percent complete: 67.0%; Average loss: 1.3926\n",
            "Iteration: 10046; Percent complete: 67.0%; Average loss: 1.3546\n",
            "Iteration: 10047; Percent complete: 67.0%; Average loss: 1.4155\n",
            "Iteration: 10048; Percent complete: 67.0%; Average loss: 1.4530\n",
            "Iteration: 10049; Percent complete: 67.0%; Average loss: 1.4218\n",
            "Iteration: 10050; Percent complete: 67.0%; Average loss: 1.2884\n",
            "Iteration: 10051; Percent complete: 67.0%; Average loss: 1.3873\n",
            "Iteration: 10052; Percent complete: 67.0%; Average loss: 1.2966\n",
            "Iteration: 10053; Percent complete: 67.0%; Average loss: 1.2934\n",
            "Iteration: 10054; Percent complete: 67.0%; Average loss: 1.3662\n",
            "Iteration: 10055; Percent complete: 67.0%; Average loss: 1.2115\n",
            "Iteration: 10056; Percent complete: 67.0%; Average loss: 1.3450\n",
            "Iteration: 10057; Percent complete: 67.0%; Average loss: 1.3722\n",
            "Iteration: 10058; Percent complete: 67.1%; Average loss: 1.4098\n",
            "Iteration: 10059; Percent complete: 67.1%; Average loss: 1.1905\n",
            "Iteration: 10060; Percent complete: 67.1%; Average loss: 1.2344\n",
            "Iteration: 10061; Percent complete: 67.1%; Average loss: 1.3841\n",
            "Iteration: 10062; Percent complete: 67.1%; Average loss: 1.2057\n",
            "Iteration: 10063; Percent complete: 67.1%; Average loss: 1.3492\n",
            "Iteration: 10064; Percent complete: 67.1%; Average loss: 1.0491\n",
            "Iteration: 10065; Percent complete: 67.1%; Average loss: 1.4602\n",
            "Iteration: 10066; Percent complete: 67.1%; Average loss: 1.3674\n",
            "Iteration: 10067; Percent complete: 67.1%; Average loss: 1.4470\n",
            "Iteration: 10068; Percent complete: 67.1%; Average loss: 1.1721\n",
            "Iteration: 10069; Percent complete: 67.1%; Average loss: 1.4807\n",
            "Iteration: 10070; Percent complete: 67.1%; Average loss: 1.3825\n",
            "Iteration: 10071; Percent complete: 67.1%; Average loss: 1.1966\n",
            "Iteration: 10072; Percent complete: 67.1%; Average loss: 1.4260\n",
            "Iteration: 10073; Percent complete: 67.2%; Average loss: 1.2944\n",
            "Iteration: 10074; Percent complete: 67.2%; Average loss: 1.4655\n",
            "Iteration: 10075; Percent complete: 67.2%; Average loss: 1.1833\n",
            "Iteration: 10076; Percent complete: 67.2%; Average loss: 1.3551\n",
            "Iteration: 10077; Percent complete: 67.2%; Average loss: 1.3341\n",
            "Iteration: 10078; Percent complete: 67.2%; Average loss: 1.2675\n",
            "Iteration: 10079; Percent complete: 67.2%; Average loss: 1.5300\n",
            "Iteration: 10080; Percent complete: 67.2%; Average loss: 1.3830\n",
            "Iteration: 10081; Percent complete: 67.2%; Average loss: 1.1836\n",
            "Iteration: 10082; Percent complete: 67.2%; Average loss: 1.2834\n",
            "Iteration: 10083; Percent complete: 67.2%; Average loss: 1.3103\n",
            "Iteration: 10084; Percent complete: 67.2%; Average loss: 1.1977\n",
            "Iteration: 10085; Percent complete: 67.2%; Average loss: 1.2355\n",
            "Iteration: 10086; Percent complete: 67.2%; Average loss: 1.2646\n",
            "Iteration: 10087; Percent complete: 67.2%; Average loss: 1.3853\n",
            "Iteration: 10088; Percent complete: 67.3%; Average loss: 1.2025\n",
            "Iteration: 10089; Percent complete: 67.3%; Average loss: 1.3397\n",
            "Iteration: 10090; Percent complete: 67.3%; Average loss: 1.2293\n",
            "Iteration: 10091; Percent complete: 67.3%; Average loss: 1.0800\n",
            "Iteration: 10092; Percent complete: 67.3%; Average loss: 1.5354\n",
            "Iteration: 10093; Percent complete: 67.3%; Average loss: 1.4521\n",
            "Iteration: 10094; Percent complete: 67.3%; Average loss: 1.3264\n",
            "Iteration: 10095; Percent complete: 67.3%; Average loss: 1.3954\n",
            "Iteration: 10096; Percent complete: 67.3%; Average loss: 1.3739\n",
            "Iteration: 10097; Percent complete: 67.3%; Average loss: 1.2230\n",
            "Iteration: 10098; Percent complete: 67.3%; Average loss: 1.4603\n",
            "Iteration: 10099; Percent complete: 67.3%; Average loss: 1.2302\n",
            "Iteration: 10100; Percent complete: 67.3%; Average loss: 1.3609\n",
            "Iteration: 10101; Percent complete: 67.3%; Average loss: 1.2718\n",
            "Iteration: 10102; Percent complete: 67.3%; Average loss: 1.1932\n",
            "Iteration: 10103; Percent complete: 67.4%; Average loss: 1.4408\n",
            "Iteration: 10104; Percent complete: 67.4%; Average loss: 1.4008\n",
            "Iteration: 10105; Percent complete: 67.4%; Average loss: 1.3031\n",
            "Iteration: 10106; Percent complete: 67.4%; Average loss: 1.3598\n",
            "Iteration: 10107; Percent complete: 67.4%; Average loss: 1.2590\n",
            "Iteration: 10108; Percent complete: 67.4%; Average loss: 1.3480\n",
            "Iteration: 10109; Percent complete: 67.4%; Average loss: 1.3371\n",
            "Iteration: 10110; Percent complete: 67.4%; Average loss: 1.4618\n",
            "Iteration: 10111; Percent complete: 67.4%; Average loss: 1.2499\n",
            "Iteration: 10112; Percent complete: 67.4%; Average loss: 1.3714\n",
            "Iteration: 10113; Percent complete: 67.4%; Average loss: 1.4353\n",
            "Iteration: 10114; Percent complete: 67.4%; Average loss: 1.3786\n",
            "Iteration: 10115; Percent complete: 67.4%; Average loss: 1.2823\n",
            "Iteration: 10116; Percent complete: 67.4%; Average loss: 1.4229\n",
            "Iteration: 10117; Percent complete: 67.4%; Average loss: 1.4323\n",
            "Iteration: 10118; Percent complete: 67.5%; Average loss: 1.2818\n",
            "Iteration: 10119; Percent complete: 67.5%; Average loss: 1.3205\n",
            "Iteration: 10120; Percent complete: 67.5%; Average loss: 1.3583\n",
            "Iteration: 10121; Percent complete: 67.5%; Average loss: 1.2403\n",
            "Iteration: 10122; Percent complete: 67.5%; Average loss: 1.1815\n",
            "Iteration: 10123; Percent complete: 67.5%; Average loss: 1.1514\n",
            "Iteration: 10124; Percent complete: 67.5%; Average loss: 1.2521\n",
            "Iteration: 10125; Percent complete: 67.5%; Average loss: 1.3016\n",
            "Iteration: 10126; Percent complete: 67.5%; Average loss: 1.2478\n",
            "Iteration: 10127; Percent complete: 67.5%; Average loss: 1.4111\n",
            "Iteration: 10128; Percent complete: 67.5%; Average loss: 1.3006\n",
            "Iteration: 10129; Percent complete: 67.5%; Average loss: 1.4558\n",
            "Iteration: 10130; Percent complete: 67.5%; Average loss: 1.1905\n",
            "Iteration: 10131; Percent complete: 67.5%; Average loss: 1.3555\n",
            "Iteration: 10132; Percent complete: 67.5%; Average loss: 1.2548\n",
            "Iteration: 10133; Percent complete: 67.6%; Average loss: 1.4178\n",
            "Iteration: 10134; Percent complete: 67.6%; Average loss: 1.4467\n",
            "Iteration: 10135; Percent complete: 67.6%; Average loss: 1.2612\n",
            "Iteration: 10136; Percent complete: 67.6%; Average loss: 1.3061\n",
            "Iteration: 10137; Percent complete: 67.6%; Average loss: 1.2766\n",
            "Iteration: 10138; Percent complete: 67.6%; Average loss: 1.3408\n",
            "Iteration: 10139; Percent complete: 67.6%; Average loss: 1.2927\n",
            "Iteration: 10140; Percent complete: 67.6%; Average loss: 1.4699\n",
            "Iteration: 10141; Percent complete: 67.6%; Average loss: 1.3890\n",
            "Iteration: 10142; Percent complete: 67.6%; Average loss: 1.3129\n",
            "Iteration: 10143; Percent complete: 67.6%; Average loss: 1.3147\n",
            "Iteration: 10144; Percent complete: 67.6%; Average loss: 1.2937\n",
            "Iteration: 10145; Percent complete: 67.6%; Average loss: 1.4344\n",
            "Iteration: 10146; Percent complete: 67.6%; Average loss: 1.3098\n",
            "Iteration: 10147; Percent complete: 67.6%; Average loss: 1.2662\n",
            "Iteration: 10148; Percent complete: 67.7%; Average loss: 1.3028\n",
            "Iteration: 10149; Percent complete: 67.7%; Average loss: 1.2452\n",
            "Iteration: 10150; Percent complete: 67.7%; Average loss: 1.2695\n",
            "Iteration: 10151; Percent complete: 67.7%; Average loss: 1.2176\n",
            "Iteration: 10152; Percent complete: 67.7%; Average loss: 1.4039\n",
            "Iteration: 10153; Percent complete: 67.7%; Average loss: 1.3410\n",
            "Iteration: 10154; Percent complete: 67.7%; Average loss: 1.2029\n",
            "Iteration: 10155; Percent complete: 67.7%; Average loss: 1.3155\n",
            "Iteration: 10156; Percent complete: 67.7%; Average loss: 1.4650\n",
            "Iteration: 10157; Percent complete: 67.7%; Average loss: 1.5013\n",
            "Iteration: 10158; Percent complete: 67.7%; Average loss: 1.2884\n",
            "Iteration: 10159; Percent complete: 67.7%; Average loss: 1.1964\n",
            "Iteration: 10160; Percent complete: 67.7%; Average loss: 1.3543\n",
            "Iteration: 10161; Percent complete: 67.7%; Average loss: 1.2933\n",
            "Iteration: 10162; Percent complete: 67.7%; Average loss: 1.1179\n",
            "Iteration: 10163; Percent complete: 67.8%; Average loss: 1.2735\n",
            "Iteration: 10164; Percent complete: 67.8%; Average loss: 1.3192\n",
            "Iteration: 10165; Percent complete: 67.8%; Average loss: 1.4425\n",
            "Iteration: 10166; Percent complete: 67.8%; Average loss: 1.4230\n",
            "Iteration: 10167; Percent complete: 67.8%; Average loss: 1.3583\n",
            "Iteration: 10168; Percent complete: 67.8%; Average loss: 1.3119\n",
            "Iteration: 10169; Percent complete: 67.8%; Average loss: 1.4809\n",
            "Iteration: 10170; Percent complete: 67.8%; Average loss: 1.2804\n",
            "Iteration: 10171; Percent complete: 67.8%; Average loss: 1.5012\n",
            "Iteration: 10172; Percent complete: 67.8%; Average loss: 1.4227\n",
            "Iteration: 10173; Percent complete: 67.8%; Average loss: 1.0774\n",
            "Iteration: 10174; Percent complete: 67.8%; Average loss: 1.3686\n",
            "Iteration: 10175; Percent complete: 67.8%; Average loss: 1.3138\n",
            "Iteration: 10176; Percent complete: 67.8%; Average loss: 1.2136\n",
            "Iteration: 10177; Percent complete: 67.8%; Average loss: 1.2604\n",
            "Iteration: 10178; Percent complete: 67.9%; Average loss: 1.4478\n",
            "Iteration: 10179; Percent complete: 67.9%; Average loss: 1.2216\n",
            "Iteration: 10180; Percent complete: 67.9%; Average loss: 1.3625\n",
            "Iteration: 10181; Percent complete: 67.9%; Average loss: 1.3767\n",
            "Iteration: 10182; Percent complete: 67.9%; Average loss: 1.6147\n",
            "Iteration: 10183; Percent complete: 67.9%; Average loss: 1.1582\n",
            "Iteration: 10184; Percent complete: 67.9%; Average loss: 1.3270\n",
            "Iteration: 10185; Percent complete: 67.9%; Average loss: 1.4629\n",
            "Iteration: 10186; Percent complete: 67.9%; Average loss: 1.2746\n",
            "Iteration: 10187; Percent complete: 67.9%; Average loss: 1.3235\n",
            "Iteration: 10188; Percent complete: 67.9%; Average loss: 1.5311\n",
            "Iteration: 10189; Percent complete: 67.9%; Average loss: 1.3146\n",
            "Iteration: 10190; Percent complete: 67.9%; Average loss: 1.2970\n",
            "Iteration: 10191; Percent complete: 67.9%; Average loss: 1.3420\n",
            "Iteration: 10192; Percent complete: 67.9%; Average loss: 1.3653\n",
            "Iteration: 10193; Percent complete: 68.0%; Average loss: 1.6835\n",
            "Iteration: 10194; Percent complete: 68.0%; Average loss: 1.2951\n",
            "Iteration: 10195; Percent complete: 68.0%; Average loss: 1.1816\n",
            "Iteration: 10196; Percent complete: 68.0%; Average loss: 1.4743\n",
            "Iteration: 10197; Percent complete: 68.0%; Average loss: 1.3375\n",
            "Iteration: 10198; Percent complete: 68.0%; Average loss: 1.3760\n",
            "Iteration: 10199; Percent complete: 68.0%; Average loss: 1.2614\n",
            "Iteration: 10200; Percent complete: 68.0%; Average loss: 1.2738\n",
            "Iteration: 10201; Percent complete: 68.0%; Average loss: 1.3955\n",
            "Iteration: 10202; Percent complete: 68.0%; Average loss: 1.3714\n",
            "Iteration: 10203; Percent complete: 68.0%; Average loss: 1.2254\n",
            "Iteration: 10204; Percent complete: 68.0%; Average loss: 1.2136\n",
            "Iteration: 10205; Percent complete: 68.0%; Average loss: 1.4378\n",
            "Iteration: 10206; Percent complete: 68.0%; Average loss: 1.5509\n",
            "Iteration: 10207; Percent complete: 68.0%; Average loss: 1.1957\n",
            "Iteration: 10208; Percent complete: 68.1%; Average loss: 1.1391\n",
            "Iteration: 10209; Percent complete: 68.1%; Average loss: 1.1825\n",
            "Iteration: 10210; Percent complete: 68.1%; Average loss: 1.2449\n",
            "Iteration: 10211; Percent complete: 68.1%; Average loss: 1.3280\n",
            "Iteration: 10212; Percent complete: 68.1%; Average loss: 1.2652\n",
            "Iteration: 10213; Percent complete: 68.1%; Average loss: 1.3870\n",
            "Iteration: 10214; Percent complete: 68.1%; Average loss: 1.2978\n",
            "Iteration: 10215; Percent complete: 68.1%; Average loss: 1.3802\n",
            "Iteration: 10216; Percent complete: 68.1%; Average loss: 1.3060\n",
            "Iteration: 10217; Percent complete: 68.1%; Average loss: 1.4601\n",
            "Iteration: 10218; Percent complete: 68.1%; Average loss: 1.4261\n",
            "Iteration: 10219; Percent complete: 68.1%; Average loss: 1.4149\n",
            "Iteration: 10220; Percent complete: 68.1%; Average loss: 1.4149\n",
            "Iteration: 10221; Percent complete: 68.1%; Average loss: 1.3217\n",
            "Iteration: 10222; Percent complete: 68.1%; Average loss: 1.2498\n",
            "Iteration: 10223; Percent complete: 68.2%; Average loss: 1.3482\n",
            "Iteration: 10224; Percent complete: 68.2%; Average loss: 1.3433\n",
            "Iteration: 10225; Percent complete: 68.2%; Average loss: 1.2479\n",
            "Iteration: 10226; Percent complete: 68.2%; Average loss: 1.3396\n",
            "Iteration: 10227; Percent complete: 68.2%; Average loss: 1.4923\n",
            "Iteration: 10228; Percent complete: 68.2%; Average loss: 1.4191\n",
            "Iteration: 10229; Percent complete: 68.2%; Average loss: 1.3313\n",
            "Iteration: 10230; Percent complete: 68.2%; Average loss: 1.2423\n",
            "Iteration: 10231; Percent complete: 68.2%; Average loss: 1.5080\n",
            "Iteration: 10232; Percent complete: 68.2%; Average loss: 1.2838\n",
            "Iteration: 10233; Percent complete: 68.2%; Average loss: 1.1979\n",
            "Iteration: 10234; Percent complete: 68.2%; Average loss: 1.3280\n",
            "Iteration: 10235; Percent complete: 68.2%; Average loss: 1.4184\n",
            "Iteration: 10236; Percent complete: 68.2%; Average loss: 1.3122\n",
            "Iteration: 10237; Percent complete: 68.2%; Average loss: 1.2793\n",
            "Iteration: 10238; Percent complete: 68.3%; Average loss: 1.3167\n",
            "Iteration: 10239; Percent complete: 68.3%; Average loss: 1.1948\n",
            "Iteration: 10240; Percent complete: 68.3%; Average loss: 1.2589\n",
            "Iteration: 10241; Percent complete: 68.3%; Average loss: 1.2908\n",
            "Iteration: 10242; Percent complete: 68.3%; Average loss: 1.1193\n",
            "Iteration: 10243; Percent complete: 68.3%; Average loss: 1.3063\n",
            "Iteration: 10244; Percent complete: 68.3%; Average loss: 1.2760\n",
            "Iteration: 10245; Percent complete: 68.3%; Average loss: 1.1840\n",
            "Iteration: 10246; Percent complete: 68.3%; Average loss: 1.4599\n",
            "Iteration: 10247; Percent complete: 68.3%; Average loss: 1.1872\n",
            "Iteration: 10248; Percent complete: 68.3%; Average loss: 1.2961\n",
            "Iteration: 10249; Percent complete: 68.3%; Average loss: 1.3230\n",
            "Iteration: 10250; Percent complete: 68.3%; Average loss: 1.2926\n",
            "Iteration: 10251; Percent complete: 68.3%; Average loss: 1.1452\n",
            "Iteration: 10252; Percent complete: 68.3%; Average loss: 1.2904\n",
            "Iteration: 10253; Percent complete: 68.4%; Average loss: 1.2615\n",
            "Iteration: 10254; Percent complete: 68.4%; Average loss: 1.4409\n",
            "Iteration: 10255; Percent complete: 68.4%; Average loss: 1.2592\n",
            "Iteration: 10256; Percent complete: 68.4%; Average loss: 1.4024\n",
            "Iteration: 10257; Percent complete: 68.4%; Average loss: 1.2383\n",
            "Iteration: 10258; Percent complete: 68.4%; Average loss: 1.1534\n",
            "Iteration: 10259; Percent complete: 68.4%; Average loss: 1.4213\n",
            "Iteration: 10260; Percent complete: 68.4%; Average loss: 1.6071\n",
            "Iteration: 10261; Percent complete: 68.4%; Average loss: 1.2994\n",
            "Iteration: 10262; Percent complete: 68.4%; Average loss: 1.2645\n",
            "Iteration: 10263; Percent complete: 68.4%; Average loss: 1.0742\n",
            "Iteration: 10264; Percent complete: 68.4%; Average loss: 1.3118\n",
            "Iteration: 10265; Percent complete: 68.4%; Average loss: 1.3916\n",
            "Iteration: 10266; Percent complete: 68.4%; Average loss: 1.2718\n",
            "Iteration: 10267; Percent complete: 68.4%; Average loss: 1.3157\n",
            "Iteration: 10268; Percent complete: 68.5%; Average loss: 1.3147\n",
            "Iteration: 10269; Percent complete: 68.5%; Average loss: 1.3882\n",
            "Iteration: 10270; Percent complete: 68.5%; Average loss: 1.2610\n",
            "Iteration: 10271; Percent complete: 68.5%; Average loss: 1.2566\n",
            "Iteration: 10272; Percent complete: 68.5%; Average loss: 1.1522\n",
            "Iteration: 10273; Percent complete: 68.5%; Average loss: 1.2003\n",
            "Iteration: 10274; Percent complete: 68.5%; Average loss: 1.3645\n",
            "Iteration: 10275; Percent complete: 68.5%; Average loss: 1.3193\n",
            "Iteration: 10276; Percent complete: 68.5%; Average loss: 1.5368\n",
            "Iteration: 10277; Percent complete: 68.5%; Average loss: 1.1930\n",
            "Iteration: 10278; Percent complete: 68.5%; Average loss: 1.2890\n",
            "Iteration: 10279; Percent complete: 68.5%; Average loss: 1.3611\n",
            "Iteration: 10280; Percent complete: 68.5%; Average loss: 1.2941\n",
            "Iteration: 10281; Percent complete: 68.5%; Average loss: 1.3652\n",
            "Iteration: 10282; Percent complete: 68.5%; Average loss: 1.1579\n",
            "Iteration: 10283; Percent complete: 68.6%; Average loss: 1.1863\n",
            "Iteration: 10284; Percent complete: 68.6%; Average loss: 1.1502\n",
            "Iteration: 10285; Percent complete: 68.6%; Average loss: 1.4075\n",
            "Iteration: 10286; Percent complete: 68.6%; Average loss: 1.2639\n",
            "Iteration: 10287; Percent complete: 68.6%; Average loss: 1.2002\n",
            "Iteration: 10288; Percent complete: 68.6%; Average loss: 1.3515\n",
            "Iteration: 10289; Percent complete: 68.6%; Average loss: 1.3180\n",
            "Iteration: 10290; Percent complete: 68.6%; Average loss: 1.3483\n",
            "Iteration: 10291; Percent complete: 68.6%; Average loss: 1.2904\n",
            "Iteration: 10292; Percent complete: 68.6%; Average loss: 1.1317\n",
            "Iteration: 10293; Percent complete: 68.6%; Average loss: 1.4227\n",
            "Iteration: 10294; Percent complete: 68.6%; Average loss: 1.2531\n",
            "Iteration: 10295; Percent complete: 68.6%; Average loss: 1.2971\n",
            "Iteration: 10296; Percent complete: 68.6%; Average loss: 1.1531\n",
            "Iteration: 10297; Percent complete: 68.6%; Average loss: 1.2887\n",
            "Iteration: 10298; Percent complete: 68.7%; Average loss: 1.3266\n",
            "Iteration: 10299; Percent complete: 68.7%; Average loss: 1.3377\n",
            "Iteration: 10300; Percent complete: 68.7%; Average loss: 1.2482\n",
            "Iteration: 10301; Percent complete: 68.7%; Average loss: 1.2856\n",
            "Iteration: 10302; Percent complete: 68.7%; Average loss: 1.1883\n",
            "Iteration: 10303; Percent complete: 68.7%; Average loss: 1.2546\n",
            "Iteration: 10304; Percent complete: 68.7%; Average loss: 1.4142\n",
            "Iteration: 10305; Percent complete: 68.7%; Average loss: 1.0617\n",
            "Iteration: 10306; Percent complete: 68.7%; Average loss: 1.4830\n",
            "Iteration: 10307; Percent complete: 68.7%; Average loss: 1.4781\n",
            "Iteration: 10308; Percent complete: 68.7%; Average loss: 1.2764\n",
            "Iteration: 10309; Percent complete: 68.7%; Average loss: 1.1974\n",
            "Iteration: 10310; Percent complete: 68.7%; Average loss: 1.3111\n",
            "Iteration: 10311; Percent complete: 68.7%; Average loss: 1.2128\n",
            "Iteration: 10312; Percent complete: 68.7%; Average loss: 1.3576\n",
            "Iteration: 10313; Percent complete: 68.8%; Average loss: 1.2577\n",
            "Iteration: 10314; Percent complete: 68.8%; Average loss: 1.4452\n",
            "Iteration: 10315; Percent complete: 68.8%; Average loss: 1.2129\n",
            "Iteration: 10316; Percent complete: 68.8%; Average loss: 1.2154\n",
            "Iteration: 10317; Percent complete: 68.8%; Average loss: 1.2650\n",
            "Iteration: 10318; Percent complete: 68.8%; Average loss: 1.4100\n",
            "Iteration: 10319; Percent complete: 68.8%; Average loss: 1.4032\n",
            "Iteration: 10320; Percent complete: 68.8%; Average loss: 1.2802\n",
            "Iteration: 10321; Percent complete: 68.8%; Average loss: 1.3827\n",
            "Iteration: 10322; Percent complete: 68.8%; Average loss: 1.2130\n",
            "Iteration: 10323; Percent complete: 68.8%; Average loss: 1.2470\n",
            "Iteration: 10324; Percent complete: 68.8%; Average loss: 1.1962\n",
            "Iteration: 10325; Percent complete: 68.8%; Average loss: 1.2707\n",
            "Iteration: 10326; Percent complete: 68.8%; Average loss: 1.2540\n",
            "Iteration: 10327; Percent complete: 68.8%; Average loss: 1.3575\n",
            "Iteration: 10328; Percent complete: 68.9%; Average loss: 1.3066\n",
            "Iteration: 10329; Percent complete: 68.9%; Average loss: 1.2851\n",
            "Iteration: 10330; Percent complete: 68.9%; Average loss: 1.2340\n",
            "Iteration: 10331; Percent complete: 68.9%; Average loss: 1.4156\n",
            "Iteration: 10332; Percent complete: 68.9%; Average loss: 1.4361\n",
            "Iteration: 10333; Percent complete: 68.9%; Average loss: 1.4683\n",
            "Iteration: 10334; Percent complete: 68.9%; Average loss: 1.2507\n",
            "Iteration: 10335; Percent complete: 68.9%; Average loss: 1.2109\n",
            "Iteration: 10336; Percent complete: 68.9%; Average loss: 1.3359\n",
            "Iteration: 10337; Percent complete: 68.9%; Average loss: 1.2999\n",
            "Iteration: 10338; Percent complete: 68.9%; Average loss: 1.4071\n",
            "Iteration: 10339; Percent complete: 68.9%; Average loss: 1.4136\n",
            "Iteration: 10340; Percent complete: 68.9%; Average loss: 1.3213\n",
            "Iteration: 10341; Percent complete: 68.9%; Average loss: 1.2938\n",
            "Iteration: 10342; Percent complete: 68.9%; Average loss: 1.1896\n",
            "Iteration: 10343; Percent complete: 69.0%; Average loss: 1.2704\n",
            "Iteration: 10344; Percent complete: 69.0%; Average loss: 1.2015\n",
            "Iteration: 10345; Percent complete: 69.0%; Average loss: 1.3773\n",
            "Iteration: 10346; Percent complete: 69.0%; Average loss: 1.2806\n",
            "Iteration: 10347; Percent complete: 69.0%; Average loss: 1.1776\n",
            "Iteration: 10348; Percent complete: 69.0%; Average loss: 1.2938\n",
            "Iteration: 10349; Percent complete: 69.0%; Average loss: 1.1972\n",
            "Iteration: 10350; Percent complete: 69.0%; Average loss: 1.0771\n",
            "Iteration: 10351; Percent complete: 69.0%; Average loss: 1.2697\n",
            "Iteration: 10352; Percent complete: 69.0%; Average loss: 1.4061\n",
            "Iteration: 10353; Percent complete: 69.0%; Average loss: 1.4015\n",
            "Iteration: 10354; Percent complete: 69.0%; Average loss: 1.2359\n",
            "Iteration: 10355; Percent complete: 69.0%; Average loss: 1.4800\n",
            "Iteration: 10356; Percent complete: 69.0%; Average loss: 1.3343\n",
            "Iteration: 10357; Percent complete: 69.0%; Average loss: 1.3827\n",
            "Iteration: 10358; Percent complete: 69.1%; Average loss: 1.0542\n",
            "Iteration: 10359; Percent complete: 69.1%; Average loss: 1.4033\n",
            "Iteration: 10360; Percent complete: 69.1%; Average loss: 1.3617\n",
            "Iteration: 10361; Percent complete: 69.1%; Average loss: 1.2654\n",
            "Iteration: 10362; Percent complete: 69.1%; Average loss: 1.2687\n",
            "Iteration: 10363; Percent complete: 69.1%; Average loss: 1.1407\n",
            "Iteration: 10364; Percent complete: 69.1%; Average loss: 1.1438\n",
            "Iteration: 10365; Percent complete: 69.1%; Average loss: 1.4784\n",
            "Iteration: 10366; Percent complete: 69.1%; Average loss: 1.4122\n",
            "Iteration: 10367; Percent complete: 69.1%; Average loss: 1.1790\n",
            "Iteration: 10368; Percent complete: 69.1%; Average loss: 1.1899\n",
            "Iteration: 10369; Percent complete: 69.1%; Average loss: 1.3971\n",
            "Iteration: 10370; Percent complete: 69.1%; Average loss: 1.4026\n",
            "Iteration: 10371; Percent complete: 69.1%; Average loss: 1.0950\n",
            "Iteration: 10372; Percent complete: 69.1%; Average loss: 1.1395\n",
            "Iteration: 10373; Percent complete: 69.2%; Average loss: 1.3610\n",
            "Iteration: 10374; Percent complete: 69.2%; Average loss: 1.2644\n",
            "Iteration: 10375; Percent complete: 69.2%; Average loss: 1.1739\n",
            "Iteration: 10376; Percent complete: 69.2%; Average loss: 1.2746\n",
            "Iteration: 10377; Percent complete: 69.2%; Average loss: 1.0683\n",
            "Iteration: 10378; Percent complete: 69.2%; Average loss: 1.2938\n",
            "Iteration: 10379; Percent complete: 69.2%; Average loss: 1.2025\n",
            "Iteration: 10380; Percent complete: 69.2%; Average loss: 1.3910\n",
            "Iteration: 10381; Percent complete: 69.2%; Average loss: 1.3478\n",
            "Iteration: 10382; Percent complete: 69.2%; Average loss: 1.0447\n",
            "Iteration: 10383; Percent complete: 69.2%; Average loss: 1.3569\n",
            "Iteration: 10384; Percent complete: 69.2%; Average loss: 1.2726\n",
            "Iteration: 10385; Percent complete: 69.2%; Average loss: 1.2960\n",
            "Iteration: 10386; Percent complete: 69.2%; Average loss: 1.2550\n",
            "Iteration: 10387; Percent complete: 69.2%; Average loss: 1.1434\n",
            "Iteration: 10388; Percent complete: 69.3%; Average loss: 1.2912\n",
            "Iteration: 10389; Percent complete: 69.3%; Average loss: 1.3668\n",
            "Iteration: 10390; Percent complete: 69.3%; Average loss: 1.2306\n",
            "Iteration: 10391; Percent complete: 69.3%; Average loss: 1.2085\n",
            "Iteration: 10392; Percent complete: 69.3%; Average loss: 1.0842\n",
            "Iteration: 10393; Percent complete: 69.3%; Average loss: 1.2463\n",
            "Iteration: 10394; Percent complete: 69.3%; Average loss: 1.3250\n",
            "Iteration: 10395; Percent complete: 69.3%; Average loss: 1.2512\n",
            "Iteration: 10396; Percent complete: 69.3%; Average loss: 1.3117\n",
            "Iteration: 10397; Percent complete: 69.3%; Average loss: 1.3568\n",
            "Iteration: 10398; Percent complete: 69.3%; Average loss: 1.4286\n",
            "Iteration: 10399; Percent complete: 69.3%; Average loss: 1.3721\n",
            "Iteration: 10400; Percent complete: 69.3%; Average loss: 1.2074\n",
            "Iteration: 10401; Percent complete: 69.3%; Average loss: 1.3182\n",
            "Iteration: 10402; Percent complete: 69.3%; Average loss: 1.4310\n",
            "Iteration: 10403; Percent complete: 69.4%; Average loss: 1.2052\n",
            "Iteration: 10404; Percent complete: 69.4%; Average loss: 1.2781\n",
            "Iteration: 10405; Percent complete: 69.4%; Average loss: 1.2759\n",
            "Iteration: 10406; Percent complete: 69.4%; Average loss: 1.4100\n",
            "Iteration: 10407; Percent complete: 69.4%; Average loss: 1.1578\n",
            "Iteration: 10408; Percent complete: 69.4%; Average loss: 1.3402\n",
            "Iteration: 10409; Percent complete: 69.4%; Average loss: 1.1623\n",
            "Iteration: 10410; Percent complete: 69.4%; Average loss: 1.2393\n",
            "Iteration: 10411; Percent complete: 69.4%; Average loss: 1.1865\n",
            "Iteration: 10412; Percent complete: 69.4%; Average loss: 1.2956\n",
            "Iteration: 10413; Percent complete: 69.4%; Average loss: 1.1762\n",
            "Iteration: 10414; Percent complete: 69.4%; Average loss: 1.3846\n",
            "Iteration: 10415; Percent complete: 69.4%; Average loss: 1.4406\n",
            "Iteration: 10416; Percent complete: 69.4%; Average loss: 1.2302\n",
            "Iteration: 10417; Percent complete: 69.4%; Average loss: 1.1580\n",
            "Iteration: 10418; Percent complete: 69.5%; Average loss: 1.1032\n",
            "Iteration: 10419; Percent complete: 69.5%; Average loss: 1.3303\n",
            "Iteration: 10420; Percent complete: 69.5%; Average loss: 1.2847\n",
            "Iteration: 10421; Percent complete: 69.5%; Average loss: 1.2019\n",
            "Iteration: 10422; Percent complete: 69.5%; Average loss: 1.4110\n",
            "Iteration: 10423; Percent complete: 69.5%; Average loss: 1.2782\n",
            "Iteration: 10424; Percent complete: 69.5%; Average loss: 1.2486\n",
            "Iteration: 10425; Percent complete: 69.5%; Average loss: 1.1025\n",
            "Iteration: 10426; Percent complete: 69.5%; Average loss: 1.1923\n",
            "Iteration: 10427; Percent complete: 69.5%; Average loss: 1.3678\n",
            "Iteration: 10428; Percent complete: 69.5%; Average loss: 1.2308\n",
            "Iteration: 10429; Percent complete: 69.5%; Average loss: 1.2762\n",
            "Iteration: 10430; Percent complete: 69.5%; Average loss: 1.1783\n",
            "Iteration: 10431; Percent complete: 69.5%; Average loss: 1.4009\n",
            "Iteration: 10432; Percent complete: 69.5%; Average loss: 1.3215\n",
            "Iteration: 10433; Percent complete: 69.6%; Average loss: 1.0500\n",
            "Iteration: 10434; Percent complete: 69.6%; Average loss: 1.2297\n",
            "Iteration: 10435; Percent complete: 69.6%; Average loss: 1.2208\n",
            "Iteration: 10436; Percent complete: 69.6%; Average loss: 1.3064\n",
            "Iteration: 10437; Percent complete: 69.6%; Average loss: 1.3653\n",
            "Iteration: 10438; Percent complete: 69.6%; Average loss: 1.1873\n",
            "Iteration: 10439; Percent complete: 69.6%; Average loss: 1.2516\n",
            "Iteration: 10440; Percent complete: 69.6%; Average loss: 1.2603\n",
            "Iteration: 10441; Percent complete: 69.6%; Average loss: 1.4047\n",
            "Iteration: 10442; Percent complete: 69.6%; Average loss: 1.3930\n",
            "Iteration: 10443; Percent complete: 69.6%; Average loss: 1.3012\n",
            "Iteration: 10444; Percent complete: 69.6%; Average loss: 1.2340\n",
            "Iteration: 10445; Percent complete: 69.6%; Average loss: 1.1893\n",
            "Iteration: 10446; Percent complete: 69.6%; Average loss: 1.1078\n",
            "Iteration: 10447; Percent complete: 69.6%; Average loss: 1.2711\n",
            "Iteration: 10448; Percent complete: 69.7%; Average loss: 1.2173\n",
            "Iteration: 10449; Percent complete: 69.7%; Average loss: 1.3178\n",
            "Iteration: 10450; Percent complete: 69.7%; Average loss: 1.3632\n",
            "Iteration: 10451; Percent complete: 69.7%; Average loss: 1.2679\n",
            "Iteration: 10452; Percent complete: 69.7%; Average loss: 1.2278\n",
            "Iteration: 10453; Percent complete: 69.7%; Average loss: 1.2771\n",
            "Iteration: 10454; Percent complete: 69.7%; Average loss: 1.1903\n",
            "Iteration: 10455; Percent complete: 69.7%; Average loss: 1.0480\n",
            "Iteration: 10456; Percent complete: 69.7%; Average loss: 1.0608\n",
            "Iteration: 10457; Percent complete: 69.7%; Average loss: 1.3686\n",
            "Iteration: 10458; Percent complete: 69.7%; Average loss: 1.3012\n",
            "Iteration: 10459; Percent complete: 69.7%; Average loss: 1.1692\n",
            "Iteration: 10460; Percent complete: 69.7%; Average loss: 1.3315\n",
            "Iteration: 10461; Percent complete: 69.7%; Average loss: 1.2308\n",
            "Iteration: 10462; Percent complete: 69.7%; Average loss: 1.2490\n",
            "Iteration: 10463; Percent complete: 69.8%; Average loss: 1.3105\n",
            "Iteration: 10464; Percent complete: 69.8%; Average loss: 1.4256\n",
            "Iteration: 10465; Percent complete: 69.8%; Average loss: 1.2389\n",
            "Iteration: 10466; Percent complete: 69.8%; Average loss: 1.2671\n",
            "Iteration: 10467; Percent complete: 69.8%; Average loss: 1.1754\n",
            "Iteration: 10468; Percent complete: 69.8%; Average loss: 1.1496\n",
            "Iteration: 10469; Percent complete: 69.8%; Average loss: 1.1558\n",
            "Iteration: 10470; Percent complete: 69.8%; Average loss: 1.2006\n",
            "Iteration: 10471; Percent complete: 69.8%; Average loss: 1.1559\n",
            "Iteration: 10472; Percent complete: 69.8%; Average loss: 1.1947\n",
            "Iteration: 10473; Percent complete: 69.8%; Average loss: 1.3907\n",
            "Iteration: 10474; Percent complete: 69.8%; Average loss: 1.2057\n",
            "Iteration: 10475; Percent complete: 69.8%; Average loss: 1.2635\n",
            "Iteration: 10476; Percent complete: 69.8%; Average loss: 1.3860\n",
            "Iteration: 10477; Percent complete: 69.8%; Average loss: 1.2040\n",
            "Iteration: 10478; Percent complete: 69.9%; Average loss: 1.1626\n",
            "Iteration: 10479; Percent complete: 69.9%; Average loss: 1.3261\n",
            "Iteration: 10480; Percent complete: 69.9%; Average loss: 1.3135\n",
            "Iteration: 10481; Percent complete: 69.9%; Average loss: 1.1485\n",
            "Iteration: 10482; Percent complete: 69.9%; Average loss: 1.2246\n",
            "Iteration: 10483; Percent complete: 69.9%; Average loss: 1.3443\n",
            "Iteration: 10484; Percent complete: 69.9%; Average loss: 1.2679\n",
            "Iteration: 10485; Percent complete: 69.9%; Average loss: 1.2978\n",
            "Iteration: 10486; Percent complete: 69.9%; Average loss: 1.3354\n",
            "Iteration: 10487; Percent complete: 69.9%; Average loss: 1.2795\n",
            "Iteration: 10488; Percent complete: 69.9%; Average loss: 1.2831\n",
            "Iteration: 10489; Percent complete: 69.9%; Average loss: 1.1403\n",
            "Iteration: 10490; Percent complete: 69.9%; Average loss: 1.1746\n",
            "Iteration: 10491; Percent complete: 69.9%; Average loss: 1.2233\n",
            "Iteration: 10492; Percent complete: 69.9%; Average loss: 1.2070\n",
            "Iteration: 10493; Percent complete: 70.0%; Average loss: 1.2569\n",
            "Iteration: 10494; Percent complete: 70.0%; Average loss: 1.4026\n",
            "Iteration: 10495; Percent complete: 70.0%; Average loss: 1.2323\n",
            "Iteration: 10496; Percent complete: 70.0%; Average loss: 1.2103\n",
            "Iteration: 10497; Percent complete: 70.0%; Average loss: 1.2053\n",
            "Iteration: 10498; Percent complete: 70.0%; Average loss: 1.1843\n",
            "Iteration: 10499; Percent complete: 70.0%; Average loss: 1.2669\n",
            "Iteration: 10500; Percent complete: 70.0%; Average loss: 1.3714\n",
            "Iteration: 10501; Percent complete: 70.0%; Average loss: 1.1792\n",
            "Iteration: 10502; Percent complete: 70.0%; Average loss: 1.2295\n",
            "Iteration: 10503; Percent complete: 70.0%; Average loss: 1.2471\n",
            "Iteration: 10504; Percent complete: 70.0%; Average loss: 1.1463\n",
            "Iteration: 10505; Percent complete: 70.0%; Average loss: 1.1027\n",
            "Iteration: 10506; Percent complete: 70.0%; Average loss: 1.2418\n",
            "Iteration: 10507; Percent complete: 70.0%; Average loss: 1.3100\n",
            "Iteration: 10508; Percent complete: 70.1%; Average loss: 1.2242\n",
            "Iteration: 10509; Percent complete: 70.1%; Average loss: 1.3594\n",
            "Iteration: 10510; Percent complete: 70.1%; Average loss: 1.4122\n",
            "Iteration: 10511; Percent complete: 70.1%; Average loss: 1.0917\n",
            "Iteration: 10512; Percent complete: 70.1%; Average loss: 1.2684\n",
            "Iteration: 10513; Percent complete: 70.1%; Average loss: 1.2394\n",
            "Iteration: 10514; Percent complete: 70.1%; Average loss: 1.3111\n",
            "Iteration: 10515; Percent complete: 70.1%; Average loss: 1.3368\n",
            "Iteration: 10516; Percent complete: 70.1%; Average loss: 1.3403\n",
            "Iteration: 10517; Percent complete: 70.1%; Average loss: 1.2452\n",
            "Iteration: 10518; Percent complete: 70.1%; Average loss: 1.4737\n",
            "Iteration: 10519; Percent complete: 70.1%; Average loss: 1.2511\n",
            "Iteration: 10520; Percent complete: 70.1%; Average loss: 1.1681\n",
            "Iteration: 10521; Percent complete: 70.1%; Average loss: 1.3151\n",
            "Iteration: 10522; Percent complete: 70.1%; Average loss: 1.2676\n",
            "Iteration: 10523; Percent complete: 70.2%; Average loss: 1.3113\n",
            "Iteration: 10524; Percent complete: 70.2%; Average loss: 1.3306\n",
            "Iteration: 10525; Percent complete: 70.2%; Average loss: 1.4484\n",
            "Iteration: 10526; Percent complete: 70.2%; Average loss: 1.2275\n",
            "Iteration: 10527; Percent complete: 70.2%; Average loss: 1.2670\n",
            "Iteration: 10528; Percent complete: 70.2%; Average loss: 1.1999\n",
            "Iteration: 10529; Percent complete: 70.2%; Average loss: 1.2583\n",
            "Iteration: 10530; Percent complete: 70.2%; Average loss: 1.1815\n",
            "Iteration: 10531; Percent complete: 70.2%; Average loss: 1.1906\n",
            "Iteration: 10532; Percent complete: 70.2%; Average loss: 1.2721\n",
            "Iteration: 10533; Percent complete: 70.2%; Average loss: 1.1833\n",
            "Iteration: 10534; Percent complete: 70.2%; Average loss: 1.2075\n",
            "Iteration: 10535; Percent complete: 70.2%; Average loss: 1.2047\n",
            "Iteration: 10536; Percent complete: 70.2%; Average loss: 1.3261\n",
            "Iteration: 10537; Percent complete: 70.2%; Average loss: 1.2801\n",
            "Iteration: 10538; Percent complete: 70.3%; Average loss: 1.2795\n",
            "Iteration: 10539; Percent complete: 70.3%; Average loss: 1.2861\n",
            "Iteration: 10540; Percent complete: 70.3%; Average loss: 1.2209\n",
            "Iteration: 10541; Percent complete: 70.3%; Average loss: 1.2568\n",
            "Iteration: 10542; Percent complete: 70.3%; Average loss: 1.0897\n",
            "Iteration: 10543; Percent complete: 70.3%; Average loss: 1.0767\n",
            "Iteration: 10544; Percent complete: 70.3%; Average loss: 1.3938\n",
            "Iteration: 10545; Percent complete: 70.3%; Average loss: 1.2844\n",
            "Iteration: 10546; Percent complete: 70.3%; Average loss: 1.3544\n",
            "Iteration: 10547; Percent complete: 70.3%; Average loss: 1.3410\n",
            "Iteration: 10548; Percent complete: 70.3%; Average loss: 1.3458\n",
            "Iteration: 10549; Percent complete: 70.3%; Average loss: 1.2277\n",
            "Iteration: 10550; Percent complete: 70.3%; Average loss: 1.3207\n",
            "Iteration: 10551; Percent complete: 70.3%; Average loss: 1.2305\n",
            "Iteration: 10552; Percent complete: 70.3%; Average loss: 1.1786\n",
            "Iteration: 10553; Percent complete: 70.4%; Average loss: 1.1710\n",
            "Iteration: 10554; Percent complete: 70.4%; Average loss: 1.2002\n",
            "Iteration: 10555; Percent complete: 70.4%; Average loss: 1.3656\n",
            "Iteration: 10556; Percent complete: 70.4%; Average loss: 1.1691\n",
            "Iteration: 10557; Percent complete: 70.4%; Average loss: 1.1183\n",
            "Iteration: 10558; Percent complete: 70.4%; Average loss: 1.1972\n",
            "Iteration: 10559; Percent complete: 70.4%; Average loss: 1.2981\n",
            "Iteration: 10560; Percent complete: 70.4%; Average loss: 1.4556\n",
            "Iteration: 10561; Percent complete: 70.4%; Average loss: 1.2256\n",
            "Iteration: 10562; Percent complete: 70.4%; Average loss: 1.3338\n",
            "Iteration: 10563; Percent complete: 70.4%; Average loss: 1.1972\n",
            "Iteration: 10564; Percent complete: 70.4%; Average loss: 1.1222\n",
            "Iteration: 10565; Percent complete: 70.4%; Average loss: 1.2612\n",
            "Iteration: 10566; Percent complete: 70.4%; Average loss: 1.2349\n",
            "Iteration: 10567; Percent complete: 70.4%; Average loss: 1.2149\n",
            "Iteration: 10568; Percent complete: 70.5%; Average loss: 1.3593\n",
            "Iteration: 10569; Percent complete: 70.5%; Average loss: 1.2367\n",
            "Iteration: 10570; Percent complete: 70.5%; Average loss: 1.2653\n",
            "Iteration: 10571; Percent complete: 70.5%; Average loss: 1.3023\n",
            "Iteration: 10572; Percent complete: 70.5%; Average loss: 1.1433\n",
            "Iteration: 10573; Percent complete: 70.5%; Average loss: 1.0902\n",
            "Iteration: 10574; Percent complete: 70.5%; Average loss: 1.3548\n",
            "Iteration: 10575; Percent complete: 70.5%; Average loss: 1.2752\n",
            "Iteration: 10576; Percent complete: 70.5%; Average loss: 1.1275\n",
            "Iteration: 10577; Percent complete: 70.5%; Average loss: 1.2468\n",
            "Iteration: 10578; Percent complete: 70.5%; Average loss: 1.2556\n",
            "Iteration: 10579; Percent complete: 70.5%; Average loss: 1.4263\n",
            "Iteration: 10580; Percent complete: 70.5%; Average loss: 1.1476\n",
            "Iteration: 10581; Percent complete: 70.5%; Average loss: 1.2327\n",
            "Iteration: 10582; Percent complete: 70.5%; Average loss: 1.1630\n",
            "Iteration: 10583; Percent complete: 70.6%; Average loss: 1.3194\n",
            "Iteration: 10584; Percent complete: 70.6%; Average loss: 1.2796\n",
            "Iteration: 10585; Percent complete: 70.6%; Average loss: 1.4045\n",
            "Iteration: 10586; Percent complete: 70.6%; Average loss: 1.2341\n",
            "Iteration: 10587; Percent complete: 70.6%; Average loss: 1.1401\n",
            "Iteration: 10588; Percent complete: 70.6%; Average loss: 1.1602\n",
            "Iteration: 10589; Percent complete: 70.6%; Average loss: 1.4146\n",
            "Iteration: 10590; Percent complete: 70.6%; Average loss: 1.1000\n",
            "Iteration: 10591; Percent complete: 70.6%; Average loss: 1.3597\n",
            "Iteration: 10592; Percent complete: 70.6%; Average loss: 1.1529\n",
            "Iteration: 10593; Percent complete: 70.6%; Average loss: 1.2378\n",
            "Iteration: 10594; Percent complete: 70.6%; Average loss: 1.1001\n",
            "Iteration: 10595; Percent complete: 70.6%; Average loss: 1.2187\n",
            "Iteration: 10596; Percent complete: 70.6%; Average loss: 1.2005\n",
            "Iteration: 10597; Percent complete: 70.6%; Average loss: 1.2143\n",
            "Iteration: 10598; Percent complete: 70.7%; Average loss: 1.2872\n",
            "Iteration: 10599; Percent complete: 70.7%; Average loss: 1.2230\n",
            "Iteration: 10600; Percent complete: 70.7%; Average loss: 1.2898\n",
            "Iteration: 10601; Percent complete: 70.7%; Average loss: 1.1807\n",
            "Iteration: 10602; Percent complete: 70.7%; Average loss: 1.3170\n",
            "Iteration: 10603; Percent complete: 70.7%; Average loss: 1.3844\n",
            "Iteration: 10604; Percent complete: 70.7%; Average loss: 1.3528\n",
            "Iteration: 10605; Percent complete: 70.7%; Average loss: 1.2209\n",
            "Iteration: 10606; Percent complete: 70.7%; Average loss: 1.1844\n",
            "Iteration: 10607; Percent complete: 70.7%; Average loss: 1.4028\n",
            "Iteration: 10608; Percent complete: 70.7%; Average loss: 1.0966\n",
            "Iteration: 10609; Percent complete: 70.7%; Average loss: 1.4529\n",
            "Iteration: 10610; Percent complete: 70.7%; Average loss: 1.2289\n",
            "Iteration: 10611; Percent complete: 70.7%; Average loss: 1.3142\n",
            "Iteration: 10612; Percent complete: 70.7%; Average loss: 1.2997\n",
            "Iteration: 10613; Percent complete: 70.8%; Average loss: 1.2652\n",
            "Iteration: 10614; Percent complete: 70.8%; Average loss: 1.0149\n",
            "Iteration: 10615; Percent complete: 70.8%; Average loss: 1.1063\n",
            "Iteration: 10616; Percent complete: 70.8%; Average loss: 1.5325\n",
            "Iteration: 10617; Percent complete: 70.8%; Average loss: 1.1650\n",
            "Iteration: 10618; Percent complete: 70.8%; Average loss: 1.1658\n",
            "Iteration: 10619; Percent complete: 70.8%; Average loss: 1.1448\n",
            "Iteration: 10620; Percent complete: 70.8%; Average loss: 1.1155\n",
            "Iteration: 10621; Percent complete: 70.8%; Average loss: 1.2370\n",
            "Iteration: 10622; Percent complete: 70.8%; Average loss: 1.1567\n",
            "Iteration: 10623; Percent complete: 70.8%; Average loss: 1.1441\n",
            "Iteration: 10624; Percent complete: 70.8%; Average loss: 1.2358\n",
            "Iteration: 10625; Percent complete: 70.8%; Average loss: 1.3646\n",
            "Iteration: 10626; Percent complete: 70.8%; Average loss: 1.2648\n",
            "Iteration: 10627; Percent complete: 70.8%; Average loss: 1.2622\n",
            "Iteration: 10628; Percent complete: 70.9%; Average loss: 1.0700\n",
            "Iteration: 10629; Percent complete: 70.9%; Average loss: 1.0862\n",
            "Iteration: 10630; Percent complete: 70.9%; Average loss: 1.1175\n",
            "Iteration: 10631; Percent complete: 70.9%; Average loss: 1.0708\n",
            "Iteration: 10632; Percent complete: 70.9%; Average loss: 1.1316\n",
            "Iteration: 10633; Percent complete: 70.9%; Average loss: 1.2000\n",
            "Iteration: 10634; Percent complete: 70.9%; Average loss: 1.0900\n",
            "Iteration: 10635; Percent complete: 70.9%; Average loss: 1.3318\n",
            "Iteration: 10636; Percent complete: 70.9%; Average loss: 1.3373\n",
            "Iteration: 10637; Percent complete: 70.9%; Average loss: 1.1940\n",
            "Iteration: 10638; Percent complete: 70.9%; Average loss: 1.2167\n",
            "Iteration: 10639; Percent complete: 70.9%; Average loss: 1.1914\n",
            "Iteration: 10640; Percent complete: 70.9%; Average loss: 1.3323\n",
            "Iteration: 10641; Percent complete: 70.9%; Average loss: 1.3219\n",
            "Iteration: 10642; Percent complete: 70.9%; Average loss: 1.2825\n",
            "Iteration: 10643; Percent complete: 71.0%; Average loss: 1.2032\n",
            "Iteration: 10644; Percent complete: 71.0%; Average loss: 1.1728\n",
            "Iteration: 10645; Percent complete: 71.0%; Average loss: 1.0959\n",
            "Iteration: 10646; Percent complete: 71.0%; Average loss: 1.2933\n",
            "Iteration: 10647; Percent complete: 71.0%; Average loss: 1.1276\n",
            "Iteration: 10648; Percent complete: 71.0%; Average loss: 1.0688\n",
            "Iteration: 10649; Percent complete: 71.0%; Average loss: 1.0722\n",
            "Iteration: 10650; Percent complete: 71.0%; Average loss: 1.2068\n",
            "Iteration: 10651; Percent complete: 71.0%; Average loss: 1.2943\n",
            "Iteration: 10652; Percent complete: 71.0%; Average loss: 1.3671\n",
            "Iteration: 10653; Percent complete: 71.0%; Average loss: 1.3414\n",
            "Iteration: 10654; Percent complete: 71.0%; Average loss: 1.2906\n",
            "Iteration: 10655; Percent complete: 71.0%; Average loss: 1.3326\n",
            "Iteration: 10656; Percent complete: 71.0%; Average loss: 1.1639\n",
            "Iteration: 10657; Percent complete: 71.0%; Average loss: 1.1471\n",
            "Iteration: 10658; Percent complete: 71.1%; Average loss: 1.2673\n",
            "Iteration: 10659; Percent complete: 71.1%; Average loss: 1.3002\n",
            "Iteration: 10660; Percent complete: 71.1%; Average loss: 1.3007\n",
            "Iteration: 10661; Percent complete: 71.1%; Average loss: 1.3155\n",
            "Iteration: 10662; Percent complete: 71.1%; Average loss: 1.3567\n",
            "Iteration: 10663; Percent complete: 71.1%; Average loss: 1.4003\n",
            "Iteration: 10664; Percent complete: 71.1%; Average loss: 1.2446\n",
            "Iteration: 10665; Percent complete: 71.1%; Average loss: 1.2970\n",
            "Iteration: 10666; Percent complete: 71.1%; Average loss: 1.1842\n",
            "Iteration: 10667; Percent complete: 71.1%; Average loss: 1.2515\n",
            "Iteration: 10668; Percent complete: 71.1%; Average loss: 1.3147\n",
            "Iteration: 10669; Percent complete: 71.1%; Average loss: 1.0799\n",
            "Iteration: 10670; Percent complete: 71.1%; Average loss: 1.1420\n",
            "Iteration: 10671; Percent complete: 71.1%; Average loss: 1.2181\n",
            "Iteration: 10672; Percent complete: 71.1%; Average loss: 1.2587\n",
            "Iteration: 10673; Percent complete: 71.2%; Average loss: 1.0076\n",
            "Iteration: 10674; Percent complete: 71.2%; Average loss: 1.1848\n",
            "Iteration: 10675; Percent complete: 71.2%; Average loss: 1.3148\n",
            "Iteration: 10676; Percent complete: 71.2%; Average loss: 1.1009\n",
            "Iteration: 10677; Percent complete: 71.2%; Average loss: 1.1656\n",
            "Iteration: 10678; Percent complete: 71.2%; Average loss: 1.2392\n",
            "Iteration: 10679; Percent complete: 71.2%; Average loss: 1.1565\n",
            "Iteration: 10680; Percent complete: 71.2%; Average loss: 1.0376\n",
            "Iteration: 10681; Percent complete: 71.2%; Average loss: 1.2702\n",
            "Iteration: 10682; Percent complete: 71.2%; Average loss: 1.2211\n",
            "Iteration: 10683; Percent complete: 71.2%; Average loss: 1.2674\n",
            "Iteration: 10684; Percent complete: 71.2%; Average loss: 1.1743\n",
            "Iteration: 10685; Percent complete: 71.2%; Average loss: 1.2119\n",
            "Iteration: 10686; Percent complete: 71.2%; Average loss: 1.0812\n",
            "Iteration: 10687; Percent complete: 71.2%; Average loss: 1.2447\n",
            "Iteration: 10688; Percent complete: 71.3%; Average loss: 1.2948\n",
            "Iteration: 10689; Percent complete: 71.3%; Average loss: 1.2094\n",
            "Iteration: 10690; Percent complete: 71.3%; Average loss: 1.1049\n",
            "Iteration: 10691; Percent complete: 71.3%; Average loss: 1.1876\n",
            "Iteration: 10692; Percent complete: 71.3%; Average loss: 1.2825\n",
            "Iteration: 10693; Percent complete: 71.3%; Average loss: 1.1154\n",
            "Iteration: 10694; Percent complete: 71.3%; Average loss: 1.1576\n",
            "Iteration: 10695; Percent complete: 71.3%; Average loss: 1.2534\n",
            "Iteration: 10696; Percent complete: 71.3%; Average loss: 1.1996\n",
            "Iteration: 10697; Percent complete: 71.3%; Average loss: 1.1499\n",
            "Iteration: 10698; Percent complete: 71.3%; Average loss: 1.3085\n",
            "Iteration: 10699; Percent complete: 71.3%; Average loss: 1.1929\n",
            "Iteration: 10700; Percent complete: 71.3%; Average loss: 1.2742\n",
            "Iteration: 10701; Percent complete: 71.3%; Average loss: 1.0003\n",
            "Iteration: 10702; Percent complete: 71.3%; Average loss: 1.1465\n",
            "Iteration: 10703; Percent complete: 71.4%; Average loss: 1.1702\n",
            "Iteration: 10704; Percent complete: 71.4%; Average loss: 1.0776\n",
            "Iteration: 10705; Percent complete: 71.4%; Average loss: 1.1999\n",
            "Iteration: 10706; Percent complete: 71.4%; Average loss: 1.0043\n",
            "Iteration: 10707; Percent complete: 71.4%; Average loss: 1.0878\n",
            "Iteration: 10708; Percent complete: 71.4%; Average loss: 1.1602\n",
            "Iteration: 10709; Percent complete: 71.4%; Average loss: 1.3936\n",
            "Iteration: 10710; Percent complete: 71.4%; Average loss: 1.2395\n",
            "Iteration: 10711; Percent complete: 71.4%; Average loss: 1.1805\n",
            "Iteration: 10712; Percent complete: 71.4%; Average loss: 1.0561\n",
            "Iteration: 10713; Percent complete: 71.4%; Average loss: 1.2397\n",
            "Iteration: 10714; Percent complete: 71.4%; Average loss: 1.1593\n",
            "Iteration: 10715; Percent complete: 71.4%; Average loss: 1.1344\n",
            "Iteration: 10716; Percent complete: 71.4%; Average loss: 1.4218\n",
            "Iteration: 10717; Percent complete: 71.4%; Average loss: 1.1180\n",
            "Iteration: 10718; Percent complete: 71.5%; Average loss: 1.0440\n",
            "Iteration: 10719; Percent complete: 71.5%; Average loss: 1.1844\n",
            "Iteration: 10720; Percent complete: 71.5%; Average loss: 1.1299\n",
            "Iteration: 10721; Percent complete: 71.5%; Average loss: 1.2383\n",
            "Iteration: 10722; Percent complete: 71.5%; Average loss: 1.0723\n",
            "Iteration: 10723; Percent complete: 71.5%; Average loss: 1.1659\n",
            "Iteration: 10724; Percent complete: 71.5%; Average loss: 1.3558\n",
            "Iteration: 10725; Percent complete: 71.5%; Average loss: 1.0998\n",
            "Iteration: 10726; Percent complete: 71.5%; Average loss: 1.4057\n",
            "Iteration: 10727; Percent complete: 71.5%; Average loss: 1.1125\n",
            "Iteration: 10728; Percent complete: 71.5%; Average loss: 1.0884\n",
            "Iteration: 10729; Percent complete: 71.5%; Average loss: 1.2514\n",
            "Iteration: 10730; Percent complete: 71.5%; Average loss: 1.2219\n",
            "Iteration: 10731; Percent complete: 71.5%; Average loss: 1.1628\n",
            "Iteration: 10732; Percent complete: 71.5%; Average loss: 1.2199\n",
            "Iteration: 10733; Percent complete: 71.6%; Average loss: 1.3272\n",
            "Iteration: 10734; Percent complete: 71.6%; Average loss: 1.2888\n",
            "Iteration: 10735; Percent complete: 71.6%; Average loss: 1.4373\n",
            "Iteration: 10736; Percent complete: 71.6%; Average loss: 1.2001\n",
            "Iteration: 10737; Percent complete: 71.6%; Average loss: 1.3772\n",
            "Iteration: 10738; Percent complete: 71.6%; Average loss: 1.2163\n",
            "Iteration: 10739; Percent complete: 71.6%; Average loss: 1.2271\n",
            "Iteration: 10740; Percent complete: 71.6%; Average loss: 1.3937\n",
            "Iteration: 10741; Percent complete: 71.6%; Average loss: 1.2920\n",
            "Iteration: 10742; Percent complete: 71.6%; Average loss: 1.2116\n",
            "Iteration: 10743; Percent complete: 71.6%; Average loss: 1.0522\n",
            "Iteration: 10744; Percent complete: 71.6%; Average loss: 1.2349\n",
            "Iteration: 10745; Percent complete: 71.6%; Average loss: 1.3353\n",
            "Iteration: 10746; Percent complete: 71.6%; Average loss: 1.0706\n",
            "Iteration: 10747; Percent complete: 71.6%; Average loss: 1.1817\n",
            "Iteration: 10748; Percent complete: 71.7%; Average loss: 1.2738\n",
            "Iteration: 10749; Percent complete: 71.7%; Average loss: 1.1076\n",
            "Iteration: 10750; Percent complete: 71.7%; Average loss: 1.0755\n",
            "Iteration: 10751; Percent complete: 71.7%; Average loss: 1.0794\n",
            "Iteration: 10752; Percent complete: 71.7%; Average loss: 1.2153\n",
            "Iteration: 10753; Percent complete: 71.7%; Average loss: 1.1596\n",
            "Iteration: 10754; Percent complete: 71.7%; Average loss: 1.4051\n",
            "Iteration: 10755; Percent complete: 71.7%; Average loss: 1.3888\n",
            "Iteration: 10756; Percent complete: 71.7%; Average loss: 1.1559\n",
            "Iteration: 10757; Percent complete: 71.7%; Average loss: 1.2382\n",
            "Iteration: 10758; Percent complete: 71.7%; Average loss: 1.1444\n",
            "Iteration: 10759; Percent complete: 71.7%; Average loss: 1.2064\n",
            "Iteration: 10760; Percent complete: 71.7%; Average loss: 1.0965\n",
            "Iteration: 10761; Percent complete: 71.7%; Average loss: 1.2180\n",
            "Iteration: 10762; Percent complete: 71.7%; Average loss: 1.1804\n",
            "Iteration: 10763; Percent complete: 71.8%; Average loss: 1.1938\n",
            "Iteration: 10764; Percent complete: 71.8%; Average loss: 1.0925\n",
            "Iteration: 10765; Percent complete: 71.8%; Average loss: 1.0524\n",
            "Iteration: 10766; Percent complete: 71.8%; Average loss: 1.0630\n",
            "Iteration: 10767; Percent complete: 71.8%; Average loss: 1.3145\n",
            "Iteration: 10768; Percent complete: 71.8%; Average loss: 1.0245\n",
            "Iteration: 10769; Percent complete: 71.8%; Average loss: 1.2560\n",
            "Iteration: 10770; Percent complete: 71.8%; Average loss: 1.2669\n",
            "Iteration: 10771; Percent complete: 71.8%; Average loss: 1.2657\n",
            "Iteration: 10772; Percent complete: 71.8%; Average loss: 1.1393\n",
            "Iteration: 10773; Percent complete: 71.8%; Average loss: 1.1928\n",
            "Iteration: 10774; Percent complete: 71.8%; Average loss: 1.1296\n",
            "Iteration: 10775; Percent complete: 71.8%; Average loss: 1.2764\n",
            "Iteration: 10776; Percent complete: 71.8%; Average loss: 1.1306\n",
            "Iteration: 10777; Percent complete: 71.8%; Average loss: 1.1375\n",
            "Iteration: 10778; Percent complete: 71.9%; Average loss: 1.2180\n",
            "Iteration: 10779; Percent complete: 71.9%; Average loss: 1.2372\n",
            "Iteration: 10780; Percent complete: 71.9%; Average loss: 1.3001\n",
            "Iteration: 10781; Percent complete: 71.9%; Average loss: 1.3624\n",
            "Iteration: 10782; Percent complete: 71.9%; Average loss: 1.3388\n",
            "Iteration: 10783; Percent complete: 71.9%; Average loss: 1.2267\n",
            "Iteration: 10784; Percent complete: 71.9%; Average loss: 1.3689\n",
            "Iteration: 10785; Percent complete: 71.9%; Average loss: 1.1229\n",
            "Iteration: 10786; Percent complete: 71.9%; Average loss: 1.1710\n",
            "Iteration: 10787; Percent complete: 71.9%; Average loss: 1.2195\n",
            "Iteration: 10788; Percent complete: 71.9%; Average loss: 1.1906\n",
            "Iteration: 10789; Percent complete: 71.9%; Average loss: 1.1576\n",
            "Iteration: 10790; Percent complete: 71.9%; Average loss: 1.0081\n",
            "Iteration: 10791; Percent complete: 71.9%; Average loss: 1.0362\n",
            "Iteration: 10792; Percent complete: 71.9%; Average loss: 1.0845\n",
            "Iteration: 10793; Percent complete: 72.0%; Average loss: 1.2468\n",
            "Iteration: 10794; Percent complete: 72.0%; Average loss: 1.3149\n",
            "Iteration: 10795; Percent complete: 72.0%; Average loss: 1.1645\n",
            "Iteration: 10796; Percent complete: 72.0%; Average loss: 1.1025\n",
            "Iteration: 10797; Percent complete: 72.0%; Average loss: 1.3540\n",
            "Iteration: 10798; Percent complete: 72.0%; Average loss: 1.1984\n",
            "Iteration: 10799; Percent complete: 72.0%; Average loss: 1.1765\n",
            "Iteration: 10800; Percent complete: 72.0%; Average loss: 1.2775\n",
            "Iteration: 10801; Percent complete: 72.0%; Average loss: 1.1961\n",
            "Iteration: 10802; Percent complete: 72.0%; Average loss: 1.3170\n",
            "Iteration: 10803; Percent complete: 72.0%; Average loss: 1.0934\n",
            "Iteration: 10804; Percent complete: 72.0%; Average loss: 1.3565\n",
            "Iteration: 10805; Percent complete: 72.0%; Average loss: 1.3719\n",
            "Iteration: 10806; Percent complete: 72.0%; Average loss: 1.1636\n",
            "Iteration: 10807; Percent complete: 72.0%; Average loss: 1.2165\n",
            "Iteration: 10808; Percent complete: 72.1%; Average loss: 1.1492\n",
            "Iteration: 10809; Percent complete: 72.1%; Average loss: 1.1846\n",
            "Iteration: 10810; Percent complete: 72.1%; Average loss: 1.0772\n",
            "Iteration: 10811; Percent complete: 72.1%; Average loss: 1.3707\n",
            "Iteration: 10812; Percent complete: 72.1%; Average loss: 1.1816\n",
            "Iteration: 10813; Percent complete: 72.1%; Average loss: 1.2010\n",
            "Iteration: 10814; Percent complete: 72.1%; Average loss: 1.2420\n",
            "Iteration: 10815; Percent complete: 72.1%; Average loss: 1.2181\n",
            "Iteration: 10816; Percent complete: 72.1%; Average loss: 1.1825\n",
            "Iteration: 10817; Percent complete: 72.1%; Average loss: 1.3490\n",
            "Iteration: 10818; Percent complete: 72.1%; Average loss: 1.1886\n",
            "Iteration: 10819; Percent complete: 72.1%; Average loss: 1.0886\n",
            "Iteration: 10820; Percent complete: 72.1%; Average loss: 1.0728\n",
            "Iteration: 10821; Percent complete: 72.1%; Average loss: 1.1557\n",
            "Iteration: 10822; Percent complete: 72.1%; Average loss: 1.1921\n",
            "Iteration: 10823; Percent complete: 72.2%; Average loss: 0.9892\n",
            "Iteration: 10824; Percent complete: 72.2%; Average loss: 1.1168\n",
            "Iteration: 10825; Percent complete: 72.2%; Average loss: 1.1139\n",
            "Iteration: 10826; Percent complete: 72.2%; Average loss: 1.2625\n",
            "Iteration: 10827; Percent complete: 72.2%; Average loss: 1.1097\n",
            "Iteration: 10828; Percent complete: 72.2%; Average loss: 1.4163\n",
            "Iteration: 10829; Percent complete: 72.2%; Average loss: 1.2143\n",
            "Iteration: 10830; Percent complete: 72.2%; Average loss: 1.1847\n",
            "Iteration: 10831; Percent complete: 72.2%; Average loss: 1.2165\n",
            "Iteration: 10832; Percent complete: 72.2%; Average loss: 1.2628\n",
            "Iteration: 10833; Percent complete: 72.2%; Average loss: 1.0119\n",
            "Iteration: 10834; Percent complete: 72.2%; Average loss: 1.1476\n",
            "Iteration: 10835; Percent complete: 72.2%; Average loss: 1.2695\n",
            "Iteration: 10836; Percent complete: 72.2%; Average loss: 1.2063\n",
            "Iteration: 10837; Percent complete: 72.2%; Average loss: 1.2356\n",
            "Iteration: 10838; Percent complete: 72.3%; Average loss: 1.1658\n",
            "Iteration: 10839; Percent complete: 72.3%; Average loss: 1.4283\n",
            "Iteration: 10840; Percent complete: 72.3%; Average loss: 1.3075\n",
            "Iteration: 10841; Percent complete: 72.3%; Average loss: 1.4082\n",
            "Iteration: 10842; Percent complete: 72.3%; Average loss: 1.2787\n",
            "Iteration: 10843; Percent complete: 72.3%; Average loss: 1.3409\n",
            "Iteration: 10844; Percent complete: 72.3%; Average loss: 1.0367\n",
            "Iteration: 10845; Percent complete: 72.3%; Average loss: 1.0510\n",
            "Iteration: 10846; Percent complete: 72.3%; Average loss: 1.3329\n",
            "Iteration: 10847; Percent complete: 72.3%; Average loss: 1.0001\n",
            "Iteration: 10848; Percent complete: 72.3%; Average loss: 1.1479\n",
            "Iteration: 10849; Percent complete: 72.3%; Average loss: 1.3414\n",
            "Iteration: 10850; Percent complete: 72.3%; Average loss: 1.1219\n",
            "Iteration: 10851; Percent complete: 72.3%; Average loss: 1.1633\n",
            "Iteration: 10852; Percent complete: 72.3%; Average loss: 1.2131\n",
            "Iteration: 10853; Percent complete: 72.4%; Average loss: 1.1061\n",
            "Iteration: 10854; Percent complete: 72.4%; Average loss: 1.1775\n",
            "Iteration: 10855; Percent complete: 72.4%; Average loss: 1.1749\n",
            "Iteration: 10856; Percent complete: 72.4%; Average loss: 1.2119\n",
            "Iteration: 10857; Percent complete: 72.4%; Average loss: 1.0286\n",
            "Iteration: 10858; Percent complete: 72.4%; Average loss: 1.1696\n",
            "Iteration: 10859; Percent complete: 72.4%; Average loss: 1.3372\n",
            "Iteration: 10860; Percent complete: 72.4%; Average loss: 1.1866\n",
            "Iteration: 10861; Percent complete: 72.4%; Average loss: 1.2387\n",
            "Iteration: 10862; Percent complete: 72.4%; Average loss: 1.2379\n",
            "Iteration: 10863; Percent complete: 72.4%; Average loss: 1.1654\n",
            "Iteration: 10864; Percent complete: 72.4%; Average loss: 1.1821\n",
            "Iteration: 10865; Percent complete: 72.4%; Average loss: 1.2504\n",
            "Iteration: 10866; Percent complete: 72.4%; Average loss: 1.2766\n",
            "Iteration: 10867; Percent complete: 72.4%; Average loss: 0.9780\n",
            "Iteration: 10868; Percent complete: 72.5%; Average loss: 1.2294\n",
            "Iteration: 10869; Percent complete: 72.5%; Average loss: 1.2600\n",
            "Iteration: 10870; Percent complete: 72.5%; Average loss: 1.1707\n",
            "Iteration: 10871; Percent complete: 72.5%; Average loss: 1.1008\n",
            "Iteration: 10872; Percent complete: 72.5%; Average loss: 1.1756\n",
            "Iteration: 10873; Percent complete: 72.5%; Average loss: 1.1139\n",
            "Iteration: 10874; Percent complete: 72.5%; Average loss: 1.0443\n",
            "Iteration: 10875; Percent complete: 72.5%; Average loss: 1.3016\n",
            "Iteration: 10876; Percent complete: 72.5%; Average loss: 1.2223\n",
            "Iteration: 10877; Percent complete: 72.5%; Average loss: 1.0434\n",
            "Iteration: 10878; Percent complete: 72.5%; Average loss: 1.2078\n",
            "Iteration: 10879; Percent complete: 72.5%; Average loss: 1.1937\n",
            "Iteration: 10880; Percent complete: 72.5%; Average loss: 1.2363\n",
            "Iteration: 10881; Percent complete: 72.5%; Average loss: 1.2331\n",
            "Iteration: 10882; Percent complete: 72.5%; Average loss: 1.1811\n",
            "Iteration: 10883; Percent complete: 72.6%; Average loss: 1.2688\n",
            "Iteration: 10884; Percent complete: 72.6%; Average loss: 1.1674\n",
            "Iteration: 10885; Percent complete: 72.6%; Average loss: 1.1856\n",
            "Iteration: 10886; Percent complete: 72.6%; Average loss: 1.1564\n",
            "Iteration: 10887; Percent complete: 72.6%; Average loss: 1.2957\n",
            "Iteration: 10888; Percent complete: 72.6%; Average loss: 1.0120\n",
            "Iteration: 10889; Percent complete: 72.6%; Average loss: 1.2662\n",
            "Iteration: 10890; Percent complete: 72.6%; Average loss: 1.3459\n",
            "Iteration: 10891; Percent complete: 72.6%; Average loss: 1.2234\n",
            "Iteration: 10892; Percent complete: 72.6%; Average loss: 1.3690\n",
            "Iteration: 10893; Percent complete: 72.6%; Average loss: 1.0897\n",
            "Iteration: 10894; Percent complete: 72.6%; Average loss: 1.2850\n",
            "Iteration: 10895; Percent complete: 72.6%; Average loss: 1.2356\n",
            "Iteration: 10896; Percent complete: 72.6%; Average loss: 1.0265\n",
            "Iteration: 10897; Percent complete: 72.6%; Average loss: 1.2417\n",
            "Iteration: 10898; Percent complete: 72.7%; Average loss: 1.3376\n",
            "Iteration: 10899; Percent complete: 72.7%; Average loss: 1.1190\n",
            "Iteration: 10900; Percent complete: 72.7%; Average loss: 1.2670\n",
            "Iteration: 10901; Percent complete: 72.7%; Average loss: 1.3010\n",
            "Iteration: 10902; Percent complete: 72.7%; Average loss: 1.1805\n",
            "Iteration: 10903; Percent complete: 72.7%; Average loss: 1.2062\n",
            "Iteration: 10904; Percent complete: 72.7%; Average loss: 1.3325\n",
            "Iteration: 10905; Percent complete: 72.7%; Average loss: 1.2790\n",
            "Iteration: 10906; Percent complete: 72.7%; Average loss: 1.0338\n",
            "Iteration: 10907; Percent complete: 72.7%; Average loss: 1.0341\n",
            "Iteration: 10908; Percent complete: 72.7%; Average loss: 1.1946\n",
            "Iteration: 10909; Percent complete: 72.7%; Average loss: 1.3357\n",
            "Iteration: 10910; Percent complete: 72.7%; Average loss: 1.3049\n",
            "Iteration: 10911; Percent complete: 72.7%; Average loss: 1.1402\n",
            "Iteration: 10912; Percent complete: 72.7%; Average loss: 1.2686\n",
            "Iteration: 10913; Percent complete: 72.8%; Average loss: 1.2200\n",
            "Iteration: 10914; Percent complete: 72.8%; Average loss: 1.0200\n",
            "Iteration: 10915; Percent complete: 72.8%; Average loss: 1.2849\n",
            "Iteration: 10916; Percent complete: 72.8%; Average loss: 1.1964\n",
            "Iteration: 10917; Percent complete: 72.8%; Average loss: 1.1401\n",
            "Iteration: 10918; Percent complete: 72.8%; Average loss: 1.1987\n",
            "Iteration: 10919; Percent complete: 72.8%; Average loss: 1.2230\n",
            "Iteration: 10920; Percent complete: 72.8%; Average loss: 1.1360\n",
            "Iteration: 10921; Percent complete: 72.8%; Average loss: 1.1160\n",
            "Iteration: 10922; Percent complete: 72.8%; Average loss: 1.2033\n",
            "Iteration: 10923; Percent complete: 72.8%; Average loss: 1.2380\n",
            "Iteration: 10924; Percent complete: 72.8%; Average loss: 1.1471\n",
            "Iteration: 10925; Percent complete: 72.8%; Average loss: 0.9883\n",
            "Iteration: 10926; Percent complete: 72.8%; Average loss: 1.1279\n",
            "Iteration: 10927; Percent complete: 72.8%; Average loss: 1.2366\n",
            "Iteration: 10928; Percent complete: 72.9%; Average loss: 1.1821\n",
            "Iteration: 10929; Percent complete: 72.9%; Average loss: 1.0298\n",
            "Iteration: 10930; Percent complete: 72.9%; Average loss: 1.0366\n",
            "Iteration: 10931; Percent complete: 72.9%; Average loss: 1.1671\n",
            "Iteration: 10932; Percent complete: 72.9%; Average loss: 1.2532\n",
            "Iteration: 10933; Percent complete: 72.9%; Average loss: 1.2592\n",
            "Iteration: 10934; Percent complete: 72.9%; Average loss: 0.9892\n",
            "Iteration: 10935; Percent complete: 72.9%; Average loss: 1.1891\n",
            "Iteration: 10936; Percent complete: 72.9%; Average loss: 1.2394\n",
            "Iteration: 10937; Percent complete: 72.9%; Average loss: 0.9824\n",
            "Iteration: 10938; Percent complete: 72.9%; Average loss: 1.2167\n",
            "Iteration: 10939; Percent complete: 72.9%; Average loss: 1.2832\n",
            "Iteration: 10940; Percent complete: 72.9%; Average loss: 1.2664\n",
            "Iteration: 10941; Percent complete: 72.9%; Average loss: 1.0634\n",
            "Iteration: 10942; Percent complete: 72.9%; Average loss: 1.3169\n",
            "Iteration: 10943; Percent complete: 73.0%; Average loss: 1.2591\n",
            "Iteration: 10944; Percent complete: 73.0%; Average loss: 1.1631\n",
            "Iteration: 10945; Percent complete: 73.0%; Average loss: 1.2571\n",
            "Iteration: 10946; Percent complete: 73.0%; Average loss: 1.0888\n",
            "Iteration: 10947; Percent complete: 73.0%; Average loss: 1.1946\n",
            "Iteration: 10948; Percent complete: 73.0%; Average loss: 1.0469\n",
            "Iteration: 10949; Percent complete: 73.0%; Average loss: 1.0863\n",
            "Iteration: 10950; Percent complete: 73.0%; Average loss: 1.2424\n",
            "Iteration: 10951; Percent complete: 73.0%; Average loss: 1.1474\n",
            "Iteration: 10952; Percent complete: 73.0%; Average loss: 1.2831\n",
            "Iteration: 10953; Percent complete: 73.0%; Average loss: 1.4598\n",
            "Iteration: 10954; Percent complete: 73.0%; Average loss: 1.2580\n",
            "Iteration: 10955; Percent complete: 73.0%; Average loss: 1.1323\n",
            "Iteration: 10956; Percent complete: 73.0%; Average loss: 1.2207\n",
            "Iteration: 10957; Percent complete: 73.0%; Average loss: 1.1804\n",
            "Iteration: 10958; Percent complete: 73.1%; Average loss: 1.1685\n",
            "Iteration: 10959; Percent complete: 73.1%; Average loss: 1.0387\n",
            "Iteration: 10960; Percent complete: 73.1%; Average loss: 1.1961\n",
            "Iteration: 10961; Percent complete: 73.1%; Average loss: 1.2475\n",
            "Iteration: 10962; Percent complete: 73.1%; Average loss: 1.1827\n",
            "Iteration: 10963; Percent complete: 73.1%; Average loss: 1.2233\n",
            "Iteration: 10964; Percent complete: 73.1%; Average loss: 1.3395\n",
            "Iteration: 10965; Percent complete: 73.1%; Average loss: 1.4392\n",
            "Iteration: 10966; Percent complete: 73.1%; Average loss: 1.1550\n",
            "Iteration: 10967; Percent complete: 73.1%; Average loss: 1.1345\n",
            "Iteration: 10968; Percent complete: 73.1%; Average loss: 1.1335\n",
            "Iteration: 10969; Percent complete: 73.1%; Average loss: 1.3403\n",
            "Iteration: 10970; Percent complete: 73.1%; Average loss: 1.1569\n",
            "Iteration: 10971; Percent complete: 73.1%; Average loss: 1.4479\n",
            "Iteration: 10972; Percent complete: 73.1%; Average loss: 1.3457\n",
            "Iteration: 10973; Percent complete: 73.2%; Average loss: 1.0547\n",
            "Iteration: 10974; Percent complete: 73.2%; Average loss: 1.2748\n",
            "Iteration: 10975; Percent complete: 73.2%; Average loss: 1.1389\n",
            "Iteration: 10976; Percent complete: 73.2%; Average loss: 1.0416\n",
            "Iteration: 10977; Percent complete: 73.2%; Average loss: 1.2473\n",
            "Iteration: 10978; Percent complete: 73.2%; Average loss: 1.0906\n",
            "Iteration: 10979; Percent complete: 73.2%; Average loss: 1.0673\n",
            "Iteration: 10980; Percent complete: 73.2%; Average loss: 1.3914\n",
            "Iteration: 10981; Percent complete: 73.2%; Average loss: 1.1233\n",
            "Iteration: 10982; Percent complete: 73.2%; Average loss: 1.2092\n",
            "Iteration: 10983; Percent complete: 73.2%; Average loss: 1.1250\n",
            "Iteration: 10984; Percent complete: 73.2%; Average loss: 1.3500\n",
            "Iteration: 10985; Percent complete: 73.2%; Average loss: 1.0230\n",
            "Iteration: 10986; Percent complete: 73.2%; Average loss: 1.0412\n",
            "Iteration: 10987; Percent complete: 73.2%; Average loss: 1.2063\n",
            "Iteration: 10988; Percent complete: 73.3%; Average loss: 1.1251\n",
            "Iteration: 10989; Percent complete: 73.3%; Average loss: 1.1534\n",
            "Iteration: 10990; Percent complete: 73.3%; Average loss: 1.2215\n",
            "Iteration: 10991; Percent complete: 73.3%; Average loss: 1.2561\n",
            "Iteration: 10992; Percent complete: 73.3%; Average loss: 1.1554\n",
            "Iteration: 10993; Percent complete: 73.3%; Average loss: 1.1103\n",
            "Iteration: 10994; Percent complete: 73.3%; Average loss: 1.2889\n",
            "Iteration: 10995; Percent complete: 73.3%; Average loss: 1.1772\n",
            "Iteration: 10996; Percent complete: 73.3%; Average loss: 1.0797\n",
            "Iteration: 10997; Percent complete: 73.3%; Average loss: 1.4473\n",
            "Iteration: 10998; Percent complete: 73.3%; Average loss: 1.1805\n",
            "Iteration: 10999; Percent complete: 73.3%; Average loss: 1.2164\n",
            "Iteration: 11000; Percent complete: 73.3%; Average loss: 1.3005\n",
            "Iteration: 11001; Percent complete: 73.3%; Average loss: 1.0798\n",
            "Iteration: 11002; Percent complete: 73.3%; Average loss: 1.3314\n",
            "Iteration: 11003; Percent complete: 73.4%; Average loss: 1.3304\n",
            "Iteration: 11004; Percent complete: 73.4%; Average loss: 1.1367\n",
            "Iteration: 11005; Percent complete: 73.4%; Average loss: 1.2765\n",
            "Iteration: 11006; Percent complete: 73.4%; Average loss: 1.1825\n",
            "Iteration: 11007; Percent complete: 73.4%; Average loss: 1.0764\n",
            "Iteration: 11008; Percent complete: 73.4%; Average loss: 1.1239\n",
            "Iteration: 11009; Percent complete: 73.4%; Average loss: 1.0949\n",
            "Iteration: 11010; Percent complete: 73.4%; Average loss: 1.3205\n",
            "Iteration: 11011; Percent complete: 73.4%; Average loss: 1.2444\n",
            "Iteration: 11012; Percent complete: 73.4%; Average loss: 1.1625\n",
            "Iteration: 11013; Percent complete: 73.4%; Average loss: 1.1233\n",
            "Iteration: 11014; Percent complete: 73.4%; Average loss: 1.1660\n",
            "Iteration: 11015; Percent complete: 73.4%; Average loss: 1.2044\n",
            "Iteration: 11016; Percent complete: 73.4%; Average loss: 1.1837\n",
            "Iteration: 11017; Percent complete: 73.4%; Average loss: 1.1955\n",
            "Iteration: 11018; Percent complete: 73.5%; Average loss: 1.2439\n",
            "Iteration: 11019; Percent complete: 73.5%; Average loss: 1.1953\n",
            "Iteration: 11020; Percent complete: 73.5%; Average loss: 1.0623\n",
            "Iteration: 11021; Percent complete: 73.5%; Average loss: 1.1725\n",
            "Iteration: 11022; Percent complete: 73.5%; Average loss: 1.1248\n",
            "Iteration: 11023; Percent complete: 73.5%; Average loss: 0.9345\n",
            "Iteration: 11024; Percent complete: 73.5%; Average loss: 0.9854\n",
            "Iteration: 11025; Percent complete: 73.5%; Average loss: 1.0825\n",
            "Iteration: 11026; Percent complete: 73.5%; Average loss: 1.1127\n",
            "Iteration: 11027; Percent complete: 73.5%; Average loss: 1.0789\n",
            "Iteration: 11028; Percent complete: 73.5%; Average loss: 1.2741\n",
            "Iteration: 11029; Percent complete: 73.5%; Average loss: 1.0501\n",
            "Iteration: 11030; Percent complete: 73.5%; Average loss: 1.1097\n",
            "Iteration: 11031; Percent complete: 73.5%; Average loss: 1.1699\n",
            "Iteration: 11032; Percent complete: 73.5%; Average loss: 1.1361\n",
            "Iteration: 11033; Percent complete: 73.6%; Average loss: 1.1747\n",
            "Iteration: 11034; Percent complete: 73.6%; Average loss: 1.1700\n",
            "Iteration: 11035; Percent complete: 73.6%; Average loss: 1.1654\n",
            "Iteration: 11036; Percent complete: 73.6%; Average loss: 1.1149\n",
            "Iteration: 11037; Percent complete: 73.6%; Average loss: 1.3379\n",
            "Iteration: 11038; Percent complete: 73.6%; Average loss: 1.2305\n",
            "Iteration: 11039; Percent complete: 73.6%; Average loss: 1.0388\n",
            "Iteration: 11040; Percent complete: 73.6%; Average loss: 1.1739\n",
            "Iteration: 11041; Percent complete: 73.6%; Average loss: 1.2030\n",
            "Iteration: 11042; Percent complete: 73.6%; Average loss: 1.2024\n",
            "Iteration: 11043; Percent complete: 73.6%; Average loss: 1.2181\n",
            "Iteration: 11044; Percent complete: 73.6%; Average loss: 1.1076\n",
            "Iteration: 11045; Percent complete: 73.6%; Average loss: 1.0891\n",
            "Iteration: 11046; Percent complete: 73.6%; Average loss: 1.2019\n",
            "Iteration: 11047; Percent complete: 73.6%; Average loss: 1.0715\n",
            "Iteration: 11048; Percent complete: 73.7%; Average loss: 1.2057\n",
            "Iteration: 11049; Percent complete: 73.7%; Average loss: 1.1788\n",
            "Iteration: 11050; Percent complete: 73.7%; Average loss: 1.0671\n",
            "Iteration: 11051; Percent complete: 73.7%; Average loss: 1.3001\n",
            "Iteration: 11052; Percent complete: 73.7%; Average loss: 1.1039\n",
            "Iteration: 11053; Percent complete: 73.7%; Average loss: 0.9854\n",
            "Iteration: 11054; Percent complete: 73.7%; Average loss: 1.0099\n",
            "Iteration: 11055; Percent complete: 73.7%; Average loss: 1.1894\n",
            "Iteration: 11056; Percent complete: 73.7%; Average loss: 1.2789\n",
            "Iteration: 11057; Percent complete: 73.7%; Average loss: 0.9897\n",
            "Iteration: 11058; Percent complete: 73.7%; Average loss: 1.1422\n",
            "Iteration: 11059; Percent complete: 73.7%; Average loss: 1.0886\n",
            "Iteration: 11060; Percent complete: 73.7%; Average loss: 1.2117\n",
            "Iteration: 11061; Percent complete: 73.7%; Average loss: 1.2431\n",
            "Iteration: 11062; Percent complete: 73.7%; Average loss: 1.2823\n",
            "Iteration: 11063; Percent complete: 73.8%; Average loss: 1.2433\n",
            "Iteration: 11064; Percent complete: 73.8%; Average loss: 1.1385\n",
            "Iteration: 11065; Percent complete: 73.8%; Average loss: 1.0095\n",
            "Iteration: 11066; Percent complete: 73.8%; Average loss: 0.9250\n",
            "Iteration: 11067; Percent complete: 73.8%; Average loss: 1.1062\n",
            "Iteration: 11068; Percent complete: 73.8%; Average loss: 1.3662\n",
            "Iteration: 11069; Percent complete: 73.8%; Average loss: 1.1698\n",
            "Iteration: 11070; Percent complete: 73.8%; Average loss: 1.1910\n",
            "Iteration: 11071; Percent complete: 73.8%; Average loss: 1.0124\n",
            "Iteration: 11072; Percent complete: 73.8%; Average loss: 1.1758\n",
            "Iteration: 11073; Percent complete: 73.8%; Average loss: 0.8986\n",
            "Iteration: 11074; Percent complete: 73.8%; Average loss: 1.1441\n",
            "Iteration: 11075; Percent complete: 73.8%; Average loss: 1.0757\n",
            "Iteration: 11076; Percent complete: 73.8%; Average loss: 1.2634\n",
            "Iteration: 11077; Percent complete: 73.8%; Average loss: 1.0019\n",
            "Iteration: 11078; Percent complete: 73.9%; Average loss: 1.0908\n",
            "Iteration: 11079; Percent complete: 73.9%; Average loss: 1.1225\n",
            "Iteration: 11080; Percent complete: 73.9%; Average loss: 1.0584\n",
            "Iteration: 11081; Percent complete: 73.9%; Average loss: 1.2452\n",
            "Iteration: 11082; Percent complete: 73.9%; Average loss: 0.9710\n",
            "Iteration: 11083; Percent complete: 73.9%; Average loss: 1.1590\n",
            "Iteration: 11084; Percent complete: 73.9%; Average loss: 1.2026\n",
            "Iteration: 11085; Percent complete: 73.9%; Average loss: 1.1795\n",
            "Iteration: 11086; Percent complete: 73.9%; Average loss: 1.2417\n",
            "Iteration: 11087; Percent complete: 73.9%; Average loss: 1.2765\n",
            "Iteration: 11088; Percent complete: 73.9%; Average loss: 1.1118\n",
            "Iteration: 11089; Percent complete: 73.9%; Average loss: 1.0887\n",
            "Iteration: 11090; Percent complete: 73.9%; Average loss: 1.1604\n",
            "Iteration: 11091; Percent complete: 73.9%; Average loss: 1.1951\n",
            "Iteration: 11092; Percent complete: 73.9%; Average loss: 1.0853\n",
            "Iteration: 11093; Percent complete: 74.0%; Average loss: 1.1601\n",
            "Iteration: 11094; Percent complete: 74.0%; Average loss: 1.0532\n",
            "Iteration: 11095; Percent complete: 74.0%; Average loss: 1.1586\n",
            "Iteration: 11096; Percent complete: 74.0%; Average loss: 1.0397\n",
            "Iteration: 11097; Percent complete: 74.0%; Average loss: 1.0365\n",
            "Iteration: 11098; Percent complete: 74.0%; Average loss: 1.1676\n",
            "Iteration: 11099; Percent complete: 74.0%; Average loss: 1.1505\n",
            "Iteration: 11100; Percent complete: 74.0%; Average loss: 1.2911\n",
            "Iteration: 11101; Percent complete: 74.0%; Average loss: 1.2039\n",
            "Iteration: 11102; Percent complete: 74.0%; Average loss: 1.0917\n",
            "Iteration: 11103; Percent complete: 74.0%; Average loss: 1.1248\n",
            "Iteration: 11104; Percent complete: 74.0%; Average loss: 1.0525\n",
            "Iteration: 11105; Percent complete: 74.0%; Average loss: 0.9973\n",
            "Iteration: 11106; Percent complete: 74.0%; Average loss: 1.1123\n",
            "Iteration: 11107; Percent complete: 74.0%; Average loss: 1.2440\n",
            "Iteration: 11108; Percent complete: 74.1%; Average loss: 0.9978\n",
            "Iteration: 11109; Percent complete: 74.1%; Average loss: 1.0198\n",
            "Iteration: 11110; Percent complete: 74.1%; Average loss: 1.0664\n",
            "Iteration: 11111; Percent complete: 74.1%; Average loss: 1.1322\n",
            "Iteration: 11112; Percent complete: 74.1%; Average loss: 1.2699\n",
            "Iteration: 11113; Percent complete: 74.1%; Average loss: 1.3546\n",
            "Iteration: 11114; Percent complete: 74.1%; Average loss: 1.2868\n",
            "Iteration: 11115; Percent complete: 74.1%; Average loss: 1.0855\n",
            "Iteration: 11116; Percent complete: 74.1%; Average loss: 1.0323\n",
            "Iteration: 11117; Percent complete: 74.1%; Average loss: 1.0391\n",
            "Iteration: 11118; Percent complete: 74.1%; Average loss: 1.1977\n",
            "Iteration: 11119; Percent complete: 74.1%; Average loss: 1.2326\n",
            "Iteration: 11120; Percent complete: 74.1%; Average loss: 1.1885\n",
            "Iteration: 11121; Percent complete: 74.1%; Average loss: 1.2566\n",
            "Iteration: 11122; Percent complete: 74.1%; Average loss: 1.0929\n",
            "Iteration: 11123; Percent complete: 74.2%; Average loss: 1.0901\n",
            "Iteration: 11124; Percent complete: 74.2%; Average loss: 1.1124\n",
            "Iteration: 11125; Percent complete: 74.2%; Average loss: 1.2809\n",
            "Iteration: 11126; Percent complete: 74.2%; Average loss: 1.0278\n",
            "Iteration: 11127; Percent complete: 74.2%; Average loss: 1.2087\n",
            "Iteration: 11128; Percent complete: 74.2%; Average loss: 1.1772\n",
            "Iteration: 11129; Percent complete: 74.2%; Average loss: 1.3237\n",
            "Iteration: 11130; Percent complete: 74.2%; Average loss: 1.1160\n",
            "Iteration: 11131; Percent complete: 74.2%; Average loss: 1.0808\n",
            "Iteration: 11132; Percent complete: 74.2%; Average loss: 1.0580\n",
            "Iteration: 11133; Percent complete: 74.2%; Average loss: 1.1896\n",
            "Iteration: 11134; Percent complete: 74.2%; Average loss: 1.1425\n",
            "Iteration: 11135; Percent complete: 74.2%; Average loss: 1.0477\n",
            "Iteration: 11136; Percent complete: 74.2%; Average loss: 1.0658\n",
            "Iteration: 11137; Percent complete: 74.2%; Average loss: 1.0209\n",
            "Iteration: 11138; Percent complete: 74.3%; Average loss: 1.0572\n",
            "Iteration: 11139; Percent complete: 74.3%; Average loss: 1.3217\n",
            "Iteration: 11140; Percent complete: 74.3%; Average loss: 1.3537\n",
            "Iteration: 11141; Percent complete: 74.3%; Average loss: 1.0119\n",
            "Iteration: 11142; Percent complete: 74.3%; Average loss: 1.1076\n",
            "Iteration: 11143; Percent complete: 74.3%; Average loss: 1.0740\n",
            "Iteration: 11144; Percent complete: 74.3%; Average loss: 1.0219\n",
            "Iteration: 11145; Percent complete: 74.3%; Average loss: 1.2114\n",
            "Iteration: 11146; Percent complete: 74.3%; Average loss: 1.2862\n",
            "Iteration: 11147; Percent complete: 74.3%; Average loss: 1.1421\n",
            "Iteration: 11148; Percent complete: 74.3%; Average loss: 1.2577\n",
            "Iteration: 11149; Percent complete: 74.3%; Average loss: 1.1726\n",
            "Iteration: 11150; Percent complete: 74.3%; Average loss: 1.2337\n",
            "Iteration: 11151; Percent complete: 74.3%; Average loss: 1.1679\n",
            "Iteration: 11152; Percent complete: 74.3%; Average loss: 0.9947\n",
            "Iteration: 11153; Percent complete: 74.4%; Average loss: 1.1103\n",
            "Iteration: 11154; Percent complete: 74.4%; Average loss: 0.9457\n",
            "Iteration: 11155; Percent complete: 74.4%; Average loss: 1.1470\n",
            "Iteration: 11156; Percent complete: 74.4%; Average loss: 1.0803\n",
            "Iteration: 11157; Percent complete: 74.4%; Average loss: 1.2680\n",
            "Iteration: 11158; Percent complete: 74.4%; Average loss: 1.2276\n",
            "Iteration: 11159; Percent complete: 74.4%; Average loss: 1.0647\n",
            "Iteration: 11160; Percent complete: 74.4%; Average loss: 1.1917\n",
            "Iteration: 11161; Percent complete: 74.4%; Average loss: 1.2945\n",
            "Iteration: 11162; Percent complete: 74.4%; Average loss: 1.1085\n",
            "Iteration: 11163; Percent complete: 74.4%; Average loss: 1.3454\n",
            "Iteration: 11164; Percent complete: 74.4%; Average loss: 1.1366\n",
            "Iteration: 11165; Percent complete: 74.4%; Average loss: 1.1035\n",
            "Iteration: 11166; Percent complete: 74.4%; Average loss: 1.2035\n",
            "Iteration: 11167; Percent complete: 74.4%; Average loss: 1.1607\n",
            "Iteration: 11168; Percent complete: 74.5%; Average loss: 1.1557\n",
            "Iteration: 11169; Percent complete: 74.5%; Average loss: 1.1190\n",
            "Iteration: 11170; Percent complete: 74.5%; Average loss: 1.1600\n",
            "Iteration: 11171; Percent complete: 74.5%; Average loss: 1.0701\n",
            "Iteration: 11172; Percent complete: 74.5%; Average loss: 1.2610\n",
            "Iteration: 11173; Percent complete: 74.5%; Average loss: 1.1699\n",
            "Iteration: 11174; Percent complete: 74.5%; Average loss: 1.0529\n",
            "Iteration: 11175; Percent complete: 74.5%; Average loss: 1.1645\n",
            "Iteration: 11176; Percent complete: 74.5%; Average loss: 1.0927\n",
            "Iteration: 11177; Percent complete: 74.5%; Average loss: 1.1855\n",
            "Iteration: 11178; Percent complete: 74.5%; Average loss: 0.9151\n",
            "Iteration: 11179; Percent complete: 74.5%; Average loss: 0.9837\n",
            "Iteration: 11180; Percent complete: 74.5%; Average loss: 1.0807\n",
            "Iteration: 11181; Percent complete: 74.5%; Average loss: 1.1802\n",
            "Iteration: 11182; Percent complete: 74.5%; Average loss: 1.0672\n",
            "Iteration: 11183; Percent complete: 74.6%; Average loss: 1.1548\n",
            "Iteration: 11184; Percent complete: 74.6%; Average loss: 1.1036\n",
            "Iteration: 11185; Percent complete: 74.6%; Average loss: 1.1391\n",
            "Iteration: 11186; Percent complete: 74.6%; Average loss: 1.1844\n",
            "Iteration: 11187; Percent complete: 74.6%; Average loss: 1.2935\n",
            "Iteration: 11188; Percent complete: 74.6%; Average loss: 1.1079\n",
            "Iteration: 11189; Percent complete: 74.6%; Average loss: 1.2719\n",
            "Iteration: 11190; Percent complete: 74.6%; Average loss: 1.1787\n",
            "Iteration: 11191; Percent complete: 74.6%; Average loss: 1.1229\n",
            "Iteration: 11192; Percent complete: 74.6%; Average loss: 1.0764\n",
            "Iteration: 11193; Percent complete: 74.6%; Average loss: 1.0115\n",
            "Iteration: 11194; Percent complete: 74.6%; Average loss: 1.1105\n",
            "Iteration: 11195; Percent complete: 74.6%; Average loss: 1.2024\n",
            "Iteration: 11196; Percent complete: 74.6%; Average loss: 1.1921\n",
            "Iteration: 11197; Percent complete: 74.6%; Average loss: 1.1682\n",
            "Iteration: 11198; Percent complete: 74.7%; Average loss: 1.3278\n",
            "Iteration: 11199; Percent complete: 74.7%; Average loss: 1.2231\n",
            "Iteration: 11200; Percent complete: 74.7%; Average loss: 1.2500\n",
            "Iteration: 11201; Percent complete: 74.7%; Average loss: 1.1007\n",
            "Iteration: 11202; Percent complete: 74.7%; Average loss: 1.1608\n",
            "Iteration: 11203; Percent complete: 74.7%; Average loss: 1.1381\n",
            "Iteration: 11204; Percent complete: 74.7%; Average loss: 1.0597\n",
            "Iteration: 11205; Percent complete: 74.7%; Average loss: 1.3061\n",
            "Iteration: 11206; Percent complete: 74.7%; Average loss: 1.1622\n",
            "Iteration: 11207; Percent complete: 74.7%; Average loss: 1.1753\n",
            "Iteration: 11208; Percent complete: 74.7%; Average loss: 1.0035\n",
            "Iteration: 11209; Percent complete: 74.7%; Average loss: 1.2634\n",
            "Iteration: 11210; Percent complete: 74.7%; Average loss: 1.1678\n",
            "Iteration: 11211; Percent complete: 74.7%; Average loss: 1.1438\n",
            "Iteration: 11212; Percent complete: 74.7%; Average loss: 1.0535\n",
            "Iteration: 11213; Percent complete: 74.8%; Average loss: 1.0492\n",
            "Iteration: 11214; Percent complete: 74.8%; Average loss: 1.1924\n",
            "Iteration: 11215; Percent complete: 74.8%; Average loss: 1.1031\n",
            "Iteration: 11216; Percent complete: 74.8%; Average loss: 1.2376\n",
            "Iteration: 11217; Percent complete: 74.8%; Average loss: 1.0204\n",
            "Iteration: 11218; Percent complete: 74.8%; Average loss: 1.0593\n",
            "Iteration: 11219; Percent complete: 74.8%; Average loss: 1.0775\n",
            "Iteration: 11220; Percent complete: 74.8%; Average loss: 1.1774\n",
            "Iteration: 11221; Percent complete: 74.8%; Average loss: 0.9261\n",
            "Iteration: 11222; Percent complete: 74.8%; Average loss: 1.1992\n",
            "Iteration: 11223; Percent complete: 74.8%; Average loss: 1.3085\n",
            "Iteration: 11224; Percent complete: 74.8%; Average loss: 1.3024\n",
            "Iteration: 11225; Percent complete: 74.8%; Average loss: 1.3037\n",
            "Iteration: 11226; Percent complete: 74.8%; Average loss: 1.1980\n",
            "Iteration: 11227; Percent complete: 74.8%; Average loss: 1.2120\n",
            "Iteration: 11228; Percent complete: 74.9%; Average loss: 0.9354\n",
            "Iteration: 11229; Percent complete: 74.9%; Average loss: 1.1931\n",
            "Iteration: 11230; Percent complete: 74.9%; Average loss: 1.0666\n",
            "Iteration: 11231; Percent complete: 74.9%; Average loss: 1.2584\n",
            "Iteration: 11232; Percent complete: 74.9%; Average loss: 1.1739\n",
            "Iteration: 11233; Percent complete: 74.9%; Average loss: 1.0929\n",
            "Iteration: 11234; Percent complete: 74.9%; Average loss: 1.2300\n",
            "Iteration: 11235; Percent complete: 74.9%; Average loss: 1.3577\n",
            "Iteration: 11236; Percent complete: 74.9%; Average loss: 1.1296\n",
            "Iteration: 11237; Percent complete: 74.9%; Average loss: 0.9514\n",
            "Iteration: 11238; Percent complete: 74.9%; Average loss: 1.1095\n",
            "Iteration: 11239; Percent complete: 74.9%; Average loss: 1.2441\n",
            "Iteration: 11240; Percent complete: 74.9%; Average loss: 1.1628\n",
            "Iteration: 11241; Percent complete: 74.9%; Average loss: 1.1156\n",
            "Iteration: 11242; Percent complete: 74.9%; Average loss: 1.2783\n",
            "Iteration: 11243; Percent complete: 75.0%; Average loss: 1.2909\n",
            "Iteration: 11244; Percent complete: 75.0%; Average loss: 1.2585\n",
            "Iteration: 11245; Percent complete: 75.0%; Average loss: 1.2944\n",
            "Iteration: 11246; Percent complete: 75.0%; Average loss: 1.1672\n",
            "Iteration: 11247; Percent complete: 75.0%; Average loss: 0.9808\n",
            "Iteration: 11248; Percent complete: 75.0%; Average loss: 1.0435\n",
            "Iteration: 11249; Percent complete: 75.0%; Average loss: 1.1614\n",
            "Iteration: 11250; Percent complete: 75.0%; Average loss: 1.1641\n",
            "Iteration: 11251; Percent complete: 75.0%; Average loss: 1.1675\n",
            "Iteration: 11252; Percent complete: 75.0%; Average loss: 1.1654\n",
            "Iteration: 11253; Percent complete: 75.0%; Average loss: 1.1221\n",
            "Iteration: 11254; Percent complete: 75.0%; Average loss: 0.9435\n",
            "Iteration: 11255; Percent complete: 75.0%; Average loss: 1.3016\n",
            "Iteration: 11256; Percent complete: 75.0%; Average loss: 1.1399\n",
            "Iteration: 11257; Percent complete: 75.0%; Average loss: 1.0964\n",
            "Iteration: 11258; Percent complete: 75.1%; Average loss: 1.1644\n",
            "Iteration: 11259; Percent complete: 75.1%; Average loss: 1.0508\n",
            "Iteration: 11260; Percent complete: 75.1%; Average loss: 1.2496\n",
            "Iteration: 11261; Percent complete: 75.1%; Average loss: 0.9928\n",
            "Iteration: 11262; Percent complete: 75.1%; Average loss: 1.0125\n",
            "Iteration: 11263; Percent complete: 75.1%; Average loss: 1.2110\n",
            "Iteration: 11264; Percent complete: 75.1%; Average loss: 1.2253\n",
            "Iteration: 11265; Percent complete: 75.1%; Average loss: 1.1533\n",
            "Iteration: 11266; Percent complete: 75.1%; Average loss: 1.1040\n",
            "Iteration: 11267; Percent complete: 75.1%; Average loss: 1.0241\n",
            "Iteration: 11268; Percent complete: 75.1%; Average loss: 1.0978\n",
            "Iteration: 11269; Percent complete: 75.1%; Average loss: 1.2007\n",
            "Iteration: 11270; Percent complete: 75.1%; Average loss: 1.1584\n",
            "Iteration: 11271; Percent complete: 75.1%; Average loss: 1.3232\n",
            "Iteration: 11272; Percent complete: 75.1%; Average loss: 0.9674\n",
            "Iteration: 11273; Percent complete: 75.2%; Average loss: 1.2594\n",
            "Iteration: 11274; Percent complete: 75.2%; Average loss: 1.1045\n",
            "Iteration: 11275; Percent complete: 75.2%; Average loss: 1.1062\n",
            "Iteration: 11276; Percent complete: 75.2%; Average loss: 1.1287\n",
            "Iteration: 11277; Percent complete: 75.2%; Average loss: 1.1028\n",
            "Iteration: 11278; Percent complete: 75.2%; Average loss: 1.3316\n",
            "Iteration: 11279; Percent complete: 75.2%; Average loss: 1.1623\n",
            "Iteration: 11280; Percent complete: 75.2%; Average loss: 1.0665\n",
            "Iteration: 11281; Percent complete: 75.2%; Average loss: 1.1661\n",
            "Iteration: 11282; Percent complete: 75.2%; Average loss: 1.1685\n",
            "Iteration: 11283; Percent complete: 75.2%; Average loss: 1.1311\n",
            "Iteration: 11284; Percent complete: 75.2%; Average loss: 1.1677\n",
            "Iteration: 11285; Percent complete: 75.2%; Average loss: 1.3160\n",
            "Iteration: 11286; Percent complete: 75.2%; Average loss: 1.0543\n",
            "Iteration: 11287; Percent complete: 75.2%; Average loss: 1.2592\n",
            "Iteration: 11288; Percent complete: 75.3%; Average loss: 1.1856\n",
            "Iteration: 11289; Percent complete: 75.3%; Average loss: 1.1769\n",
            "Iteration: 11290; Percent complete: 75.3%; Average loss: 1.0257\n",
            "Iteration: 11291; Percent complete: 75.3%; Average loss: 1.0302\n",
            "Iteration: 11292; Percent complete: 75.3%; Average loss: 1.1133\n",
            "Iteration: 11293; Percent complete: 75.3%; Average loss: 1.1950\n",
            "Iteration: 11294; Percent complete: 75.3%; Average loss: 1.1233\n",
            "Iteration: 11295; Percent complete: 75.3%; Average loss: 1.1297\n",
            "Iteration: 11296; Percent complete: 75.3%; Average loss: 1.1599\n",
            "Iteration: 11297; Percent complete: 75.3%; Average loss: 1.0649\n",
            "Iteration: 11298; Percent complete: 75.3%; Average loss: 1.0914\n",
            "Iteration: 11299; Percent complete: 75.3%; Average loss: 1.1743\n",
            "Iteration: 11300; Percent complete: 75.3%; Average loss: 1.1653\n",
            "Iteration: 11301; Percent complete: 75.3%; Average loss: 1.1393\n",
            "Iteration: 11302; Percent complete: 75.3%; Average loss: 1.0700\n",
            "Iteration: 11303; Percent complete: 75.4%; Average loss: 1.0974\n",
            "Iteration: 11304; Percent complete: 75.4%; Average loss: 1.0620\n",
            "Iteration: 11305; Percent complete: 75.4%; Average loss: 1.1393\n",
            "Iteration: 11306; Percent complete: 75.4%; Average loss: 1.1601\n",
            "Iteration: 11307; Percent complete: 75.4%; Average loss: 1.0658\n",
            "Iteration: 11308; Percent complete: 75.4%; Average loss: 1.0512\n",
            "Iteration: 11309; Percent complete: 75.4%; Average loss: 1.1262\n",
            "Iteration: 11310; Percent complete: 75.4%; Average loss: 0.9704\n",
            "Iteration: 11311; Percent complete: 75.4%; Average loss: 1.2490\n",
            "Iteration: 11312; Percent complete: 75.4%; Average loss: 0.9193\n",
            "Iteration: 11313; Percent complete: 75.4%; Average loss: 1.1057\n",
            "Iteration: 11314; Percent complete: 75.4%; Average loss: 1.1551\n",
            "Iteration: 11315; Percent complete: 75.4%; Average loss: 1.0680\n",
            "Iteration: 11316; Percent complete: 75.4%; Average loss: 1.2300\n",
            "Iteration: 11317; Percent complete: 75.4%; Average loss: 1.0249\n",
            "Iteration: 11318; Percent complete: 75.5%; Average loss: 1.1932\n",
            "Iteration: 11319; Percent complete: 75.5%; Average loss: 1.0812\n",
            "Iteration: 11320; Percent complete: 75.5%; Average loss: 0.9832\n",
            "Iteration: 11321; Percent complete: 75.5%; Average loss: 1.1881\n",
            "Iteration: 11322; Percent complete: 75.5%; Average loss: 0.9438\n",
            "Iteration: 11323; Percent complete: 75.5%; Average loss: 1.2229\n",
            "Iteration: 11324; Percent complete: 75.5%; Average loss: 1.3179\n",
            "Iteration: 11325; Percent complete: 75.5%; Average loss: 1.0506\n",
            "Iteration: 11326; Percent complete: 75.5%; Average loss: 0.9905\n",
            "Iteration: 11327; Percent complete: 75.5%; Average loss: 1.0862\n",
            "Iteration: 11328; Percent complete: 75.5%; Average loss: 1.0691\n",
            "Iteration: 11329; Percent complete: 75.5%; Average loss: 1.1495\n",
            "Iteration: 11330; Percent complete: 75.5%; Average loss: 1.0860\n",
            "Iteration: 11331; Percent complete: 75.5%; Average loss: 1.1946\n",
            "Iteration: 11332; Percent complete: 75.5%; Average loss: 1.1155\n",
            "Iteration: 11333; Percent complete: 75.6%; Average loss: 1.1493\n",
            "Iteration: 11334; Percent complete: 75.6%; Average loss: 1.0186\n",
            "Iteration: 11335; Percent complete: 75.6%; Average loss: 1.1367\n",
            "Iteration: 11336; Percent complete: 75.6%; Average loss: 1.2336\n",
            "Iteration: 11337; Percent complete: 75.6%; Average loss: 1.0901\n",
            "Iteration: 11338; Percent complete: 75.6%; Average loss: 1.0118\n",
            "Iteration: 11339; Percent complete: 75.6%; Average loss: 1.2229\n",
            "Iteration: 11340; Percent complete: 75.6%; Average loss: 1.2156\n",
            "Iteration: 11341; Percent complete: 75.6%; Average loss: 1.2337\n",
            "Iteration: 11342; Percent complete: 75.6%; Average loss: 1.3576\n",
            "Iteration: 11343; Percent complete: 75.6%; Average loss: 1.0715\n",
            "Iteration: 11344; Percent complete: 75.6%; Average loss: 1.2349\n",
            "Iteration: 11345; Percent complete: 75.6%; Average loss: 1.2909\n",
            "Iteration: 11346; Percent complete: 75.6%; Average loss: 1.2894\n",
            "Iteration: 11347; Percent complete: 75.6%; Average loss: 1.1444\n",
            "Iteration: 11348; Percent complete: 75.7%; Average loss: 1.1843\n",
            "Iteration: 11349; Percent complete: 75.7%; Average loss: 1.2715\n",
            "Iteration: 11350; Percent complete: 75.7%; Average loss: 1.0005\n",
            "Iteration: 11351; Percent complete: 75.7%; Average loss: 0.8669\n",
            "Iteration: 11352; Percent complete: 75.7%; Average loss: 0.9734\n",
            "Iteration: 11353; Percent complete: 75.7%; Average loss: 0.9703\n",
            "Iteration: 11354; Percent complete: 75.7%; Average loss: 1.0756\n",
            "Iteration: 11355; Percent complete: 75.7%; Average loss: 1.1947\n",
            "Iteration: 11356; Percent complete: 75.7%; Average loss: 1.0248\n",
            "Iteration: 11357; Percent complete: 75.7%; Average loss: 1.2396\n",
            "Iteration: 11358; Percent complete: 75.7%; Average loss: 1.1590\n",
            "Iteration: 11359; Percent complete: 75.7%; Average loss: 1.0789\n",
            "Iteration: 11360; Percent complete: 75.7%; Average loss: 1.3741\n",
            "Iteration: 11361; Percent complete: 75.7%; Average loss: 1.1215\n",
            "Iteration: 11362; Percent complete: 75.7%; Average loss: 1.0058\n",
            "Iteration: 11363; Percent complete: 75.8%; Average loss: 1.1043\n",
            "Iteration: 11364; Percent complete: 75.8%; Average loss: 1.0053\n",
            "Iteration: 11365; Percent complete: 75.8%; Average loss: 1.0120\n",
            "Iteration: 11366; Percent complete: 75.8%; Average loss: 1.1093\n",
            "Iteration: 11367; Percent complete: 75.8%; Average loss: 1.0001\n",
            "Iteration: 11368; Percent complete: 75.8%; Average loss: 0.9585\n",
            "Iteration: 11369; Percent complete: 75.8%; Average loss: 1.0087\n",
            "Iteration: 11370; Percent complete: 75.8%; Average loss: 0.9393\n",
            "Iteration: 11371; Percent complete: 75.8%; Average loss: 1.1593\n",
            "Iteration: 11372; Percent complete: 75.8%; Average loss: 1.2206\n",
            "Iteration: 11373; Percent complete: 75.8%; Average loss: 0.9982\n",
            "Iteration: 11374; Percent complete: 75.8%; Average loss: 1.2379\n",
            "Iteration: 11375; Percent complete: 75.8%; Average loss: 1.0214\n",
            "Iteration: 11376; Percent complete: 75.8%; Average loss: 1.2446\n",
            "Iteration: 11377; Percent complete: 75.8%; Average loss: 1.1808\n",
            "Iteration: 11378; Percent complete: 75.9%; Average loss: 1.1688\n",
            "Iteration: 11379; Percent complete: 75.9%; Average loss: 1.2352\n",
            "Iteration: 11380; Percent complete: 75.9%; Average loss: 1.1538\n",
            "Iteration: 11381; Percent complete: 75.9%; Average loss: 1.1176\n",
            "Iteration: 11382; Percent complete: 75.9%; Average loss: 1.2312\n",
            "Iteration: 11383; Percent complete: 75.9%; Average loss: 1.1229\n",
            "Iteration: 11384; Percent complete: 75.9%; Average loss: 1.0205\n",
            "Iteration: 11385; Percent complete: 75.9%; Average loss: 1.0249\n",
            "Iteration: 11386; Percent complete: 75.9%; Average loss: 0.9837\n",
            "Iteration: 11387; Percent complete: 75.9%; Average loss: 0.9809\n",
            "Iteration: 11388; Percent complete: 75.9%; Average loss: 1.2745\n",
            "Iteration: 11389; Percent complete: 75.9%; Average loss: 1.1832\n",
            "Iteration: 11390; Percent complete: 75.9%; Average loss: 1.1846\n",
            "Iteration: 11391; Percent complete: 75.9%; Average loss: 1.2104\n",
            "Iteration: 11392; Percent complete: 75.9%; Average loss: 1.0655\n",
            "Iteration: 11393; Percent complete: 76.0%; Average loss: 1.2204\n",
            "Iteration: 11394; Percent complete: 76.0%; Average loss: 1.2175\n",
            "Iteration: 11395; Percent complete: 76.0%; Average loss: 1.1719\n",
            "Iteration: 11396; Percent complete: 76.0%; Average loss: 1.2064\n",
            "Iteration: 11397; Percent complete: 76.0%; Average loss: 1.0037\n",
            "Iteration: 11398; Percent complete: 76.0%; Average loss: 1.2908\n",
            "Iteration: 11399; Percent complete: 76.0%; Average loss: 1.0765\n",
            "Iteration: 11400; Percent complete: 76.0%; Average loss: 1.1915\n",
            "Iteration: 11401; Percent complete: 76.0%; Average loss: 1.2027\n",
            "Iteration: 11402; Percent complete: 76.0%; Average loss: 1.1306\n",
            "Iteration: 11403; Percent complete: 76.0%; Average loss: 1.3926\n",
            "Iteration: 11404; Percent complete: 76.0%; Average loss: 0.9831\n",
            "Iteration: 11405; Percent complete: 76.0%; Average loss: 1.0855\n",
            "Iteration: 11406; Percent complete: 76.0%; Average loss: 0.9510\n",
            "Iteration: 11407; Percent complete: 76.0%; Average loss: 1.0377\n",
            "Iteration: 11408; Percent complete: 76.1%; Average loss: 1.0699\n",
            "Iteration: 11409; Percent complete: 76.1%; Average loss: 1.0050\n",
            "Iteration: 11410; Percent complete: 76.1%; Average loss: 1.2076\n",
            "Iteration: 11411; Percent complete: 76.1%; Average loss: 1.0991\n",
            "Iteration: 11412; Percent complete: 76.1%; Average loss: 1.1737\n",
            "Iteration: 11413; Percent complete: 76.1%; Average loss: 1.1489\n",
            "Iteration: 11414; Percent complete: 76.1%; Average loss: 1.1066\n",
            "Iteration: 11415; Percent complete: 76.1%; Average loss: 1.1541\n",
            "Iteration: 11416; Percent complete: 76.1%; Average loss: 1.1923\n",
            "Iteration: 11417; Percent complete: 76.1%; Average loss: 1.2921\n",
            "Iteration: 11418; Percent complete: 76.1%; Average loss: 1.0876\n",
            "Iteration: 11419; Percent complete: 76.1%; Average loss: 1.1099\n",
            "Iteration: 11420; Percent complete: 76.1%; Average loss: 0.9908\n",
            "Iteration: 11421; Percent complete: 76.1%; Average loss: 1.0004\n",
            "Iteration: 11422; Percent complete: 76.1%; Average loss: 1.4378\n",
            "Iteration: 11423; Percent complete: 76.2%; Average loss: 1.1103\n",
            "Iteration: 11424; Percent complete: 76.2%; Average loss: 1.2491\n",
            "Iteration: 11425; Percent complete: 76.2%; Average loss: 1.3418\n",
            "Iteration: 11426; Percent complete: 76.2%; Average loss: 1.0847\n",
            "Iteration: 11427; Percent complete: 76.2%; Average loss: 1.2052\n",
            "Iteration: 11428; Percent complete: 76.2%; Average loss: 1.1274\n",
            "Iteration: 11429; Percent complete: 76.2%; Average loss: 1.0412\n",
            "Iteration: 11430; Percent complete: 76.2%; Average loss: 1.1154\n",
            "Iteration: 11431; Percent complete: 76.2%; Average loss: 1.0988\n",
            "Iteration: 11432; Percent complete: 76.2%; Average loss: 1.1598\n",
            "Iteration: 11433; Percent complete: 76.2%; Average loss: 0.9786\n",
            "Iteration: 11434; Percent complete: 76.2%; Average loss: 1.1762\n",
            "Iteration: 11435; Percent complete: 76.2%; Average loss: 1.0379\n",
            "Iteration: 11436; Percent complete: 76.2%; Average loss: 1.1517\n",
            "Iteration: 11437; Percent complete: 76.2%; Average loss: 1.1371\n",
            "Iteration: 11438; Percent complete: 76.3%; Average loss: 1.0811\n",
            "Iteration: 11439; Percent complete: 76.3%; Average loss: 0.9680\n",
            "Iteration: 11440; Percent complete: 76.3%; Average loss: 1.0472\n",
            "Iteration: 11441; Percent complete: 76.3%; Average loss: 1.0349\n",
            "Iteration: 11442; Percent complete: 76.3%; Average loss: 1.1054\n",
            "Iteration: 11443; Percent complete: 76.3%; Average loss: 1.2091\n",
            "Iteration: 11444; Percent complete: 76.3%; Average loss: 1.1094\n",
            "Iteration: 11445; Percent complete: 76.3%; Average loss: 1.1090\n",
            "Iteration: 11446; Percent complete: 76.3%; Average loss: 1.0625\n",
            "Iteration: 11447; Percent complete: 76.3%; Average loss: 1.1521\n",
            "Iteration: 11448; Percent complete: 76.3%; Average loss: 1.1093\n",
            "Iteration: 11449; Percent complete: 76.3%; Average loss: 1.0547\n",
            "Iteration: 11450; Percent complete: 76.3%; Average loss: 1.1355\n",
            "Iteration: 11451; Percent complete: 76.3%; Average loss: 1.0766\n",
            "Iteration: 11452; Percent complete: 76.3%; Average loss: 0.9364\n",
            "Iteration: 11453; Percent complete: 76.4%; Average loss: 1.1667\n",
            "Iteration: 11454; Percent complete: 76.4%; Average loss: 1.0700\n",
            "Iteration: 11455; Percent complete: 76.4%; Average loss: 1.0505\n",
            "Iteration: 11456; Percent complete: 76.4%; Average loss: 1.2996\n",
            "Iteration: 11457; Percent complete: 76.4%; Average loss: 0.9879\n",
            "Iteration: 11458; Percent complete: 76.4%; Average loss: 1.0355\n",
            "Iteration: 11459; Percent complete: 76.4%; Average loss: 1.0457\n",
            "Iteration: 11460; Percent complete: 76.4%; Average loss: 1.2905\n",
            "Iteration: 11461; Percent complete: 76.4%; Average loss: 0.9064\n",
            "Iteration: 11462; Percent complete: 76.4%; Average loss: 1.1287\n",
            "Iteration: 11463; Percent complete: 76.4%; Average loss: 1.0174\n",
            "Iteration: 11464; Percent complete: 76.4%; Average loss: 1.0481\n",
            "Iteration: 11465; Percent complete: 76.4%; Average loss: 1.2358\n",
            "Iteration: 11466; Percent complete: 76.4%; Average loss: 1.0775\n",
            "Iteration: 11467; Percent complete: 76.4%; Average loss: 1.0570\n",
            "Iteration: 11468; Percent complete: 76.5%; Average loss: 1.1128\n",
            "Iteration: 11469; Percent complete: 76.5%; Average loss: 1.0694\n",
            "Iteration: 11470; Percent complete: 76.5%; Average loss: 1.1054\n",
            "Iteration: 11471; Percent complete: 76.5%; Average loss: 1.0293\n",
            "Iteration: 11472; Percent complete: 76.5%; Average loss: 0.9136\n",
            "Iteration: 11473; Percent complete: 76.5%; Average loss: 0.9576\n",
            "Iteration: 11474; Percent complete: 76.5%; Average loss: 1.1080\n",
            "Iteration: 11475; Percent complete: 76.5%; Average loss: 1.1462\n",
            "Iteration: 11476; Percent complete: 76.5%; Average loss: 1.0204\n",
            "Iteration: 11477; Percent complete: 76.5%; Average loss: 1.0830\n",
            "Iteration: 11478; Percent complete: 76.5%; Average loss: 1.0517\n",
            "Iteration: 11479; Percent complete: 76.5%; Average loss: 0.9744\n",
            "Iteration: 11480; Percent complete: 76.5%; Average loss: 1.2262\n",
            "Iteration: 11481; Percent complete: 76.5%; Average loss: 1.1613\n",
            "Iteration: 11482; Percent complete: 76.5%; Average loss: 1.1514\n",
            "Iteration: 11483; Percent complete: 76.6%; Average loss: 1.1137\n",
            "Iteration: 11484; Percent complete: 76.6%; Average loss: 1.0271\n",
            "Iteration: 11485; Percent complete: 76.6%; Average loss: 1.1755\n",
            "Iteration: 11486; Percent complete: 76.6%; Average loss: 0.9949\n",
            "Iteration: 11487; Percent complete: 76.6%; Average loss: 1.1198\n",
            "Iteration: 11488; Percent complete: 76.6%; Average loss: 1.0420\n",
            "Iteration: 11489; Percent complete: 76.6%; Average loss: 1.0546\n",
            "Iteration: 11490; Percent complete: 76.6%; Average loss: 1.0194\n",
            "Iteration: 11491; Percent complete: 76.6%; Average loss: 1.0984\n",
            "Iteration: 11492; Percent complete: 76.6%; Average loss: 1.1086\n",
            "Iteration: 11493; Percent complete: 76.6%; Average loss: 1.0611\n",
            "Iteration: 11494; Percent complete: 76.6%; Average loss: 1.1688\n",
            "Iteration: 11495; Percent complete: 76.6%; Average loss: 0.9963\n",
            "Iteration: 11496; Percent complete: 76.6%; Average loss: 1.2136\n",
            "Iteration: 11497; Percent complete: 76.6%; Average loss: 1.2051\n",
            "Iteration: 11498; Percent complete: 76.7%; Average loss: 1.2180\n",
            "Iteration: 11499; Percent complete: 76.7%; Average loss: 1.1950\n",
            "Iteration: 11500; Percent complete: 76.7%; Average loss: 1.1356\n",
            "Iteration: 11501; Percent complete: 76.7%; Average loss: 1.1788\n",
            "Iteration: 11502; Percent complete: 76.7%; Average loss: 1.0854\n",
            "Iteration: 11503; Percent complete: 76.7%; Average loss: 1.2092\n",
            "Iteration: 11504; Percent complete: 76.7%; Average loss: 0.9887\n",
            "Iteration: 11505; Percent complete: 76.7%; Average loss: 1.0847\n",
            "Iteration: 11506; Percent complete: 76.7%; Average loss: 1.0891\n",
            "Iteration: 11507; Percent complete: 76.7%; Average loss: 1.1952\n",
            "Iteration: 11508; Percent complete: 76.7%; Average loss: 1.0817\n",
            "Iteration: 11509; Percent complete: 76.7%; Average loss: 1.2578\n",
            "Iteration: 11510; Percent complete: 76.7%; Average loss: 1.0382\n",
            "Iteration: 11511; Percent complete: 76.7%; Average loss: 0.9412\n",
            "Iteration: 11512; Percent complete: 76.7%; Average loss: 1.1957\n",
            "Iteration: 11513; Percent complete: 76.8%; Average loss: 1.0467\n",
            "Iteration: 11514; Percent complete: 76.8%; Average loss: 1.0859\n",
            "Iteration: 11515; Percent complete: 76.8%; Average loss: 1.0767\n",
            "Iteration: 11516; Percent complete: 76.8%; Average loss: 0.9997\n",
            "Iteration: 11517; Percent complete: 76.8%; Average loss: 1.0347\n",
            "Iteration: 11518; Percent complete: 76.8%; Average loss: 1.1647\n",
            "Iteration: 11519; Percent complete: 76.8%; Average loss: 0.9036\n",
            "Iteration: 11520; Percent complete: 76.8%; Average loss: 1.1420\n",
            "Iteration: 11521; Percent complete: 76.8%; Average loss: 1.1807\n",
            "Iteration: 11522; Percent complete: 76.8%; Average loss: 1.2150\n",
            "Iteration: 11523; Percent complete: 76.8%; Average loss: 1.0126\n",
            "Iteration: 11524; Percent complete: 76.8%; Average loss: 1.1267\n",
            "Iteration: 11525; Percent complete: 76.8%; Average loss: 1.3555\n",
            "Iteration: 11526; Percent complete: 76.8%; Average loss: 1.1146\n",
            "Iteration: 11527; Percent complete: 76.8%; Average loss: 1.2329\n",
            "Iteration: 11528; Percent complete: 76.9%; Average loss: 1.1983\n",
            "Iteration: 11529; Percent complete: 76.9%; Average loss: 1.0853\n",
            "Iteration: 11530; Percent complete: 76.9%; Average loss: 1.0696\n",
            "Iteration: 11531; Percent complete: 76.9%; Average loss: 1.1416\n",
            "Iteration: 11532; Percent complete: 76.9%; Average loss: 1.1583\n",
            "Iteration: 11533; Percent complete: 76.9%; Average loss: 1.1490\n",
            "Iteration: 11534; Percent complete: 76.9%; Average loss: 1.0701\n",
            "Iteration: 11535; Percent complete: 76.9%; Average loss: 1.0774\n",
            "Iteration: 11536; Percent complete: 76.9%; Average loss: 0.9958\n",
            "Iteration: 11537; Percent complete: 76.9%; Average loss: 0.9262\n",
            "Iteration: 11538; Percent complete: 76.9%; Average loss: 1.0734\n",
            "Iteration: 11539; Percent complete: 76.9%; Average loss: 1.1992\n",
            "Iteration: 11540; Percent complete: 76.9%; Average loss: 1.1427\n",
            "Iteration: 11541; Percent complete: 76.9%; Average loss: 1.0107\n",
            "Iteration: 11542; Percent complete: 76.9%; Average loss: 1.0482\n",
            "Iteration: 11543; Percent complete: 77.0%; Average loss: 1.0393\n",
            "Iteration: 11544; Percent complete: 77.0%; Average loss: 1.1245\n",
            "Iteration: 11545; Percent complete: 77.0%; Average loss: 1.0938\n",
            "Iteration: 11546; Percent complete: 77.0%; Average loss: 1.0144\n",
            "Iteration: 11547; Percent complete: 77.0%; Average loss: 1.3257\n",
            "Iteration: 11548; Percent complete: 77.0%; Average loss: 1.3398\n",
            "Iteration: 11549; Percent complete: 77.0%; Average loss: 1.0638\n",
            "Iteration: 11550; Percent complete: 77.0%; Average loss: 1.1062\n",
            "Iteration: 11551; Percent complete: 77.0%; Average loss: 1.2588\n",
            "Iteration: 11552; Percent complete: 77.0%; Average loss: 0.9583\n",
            "Iteration: 11553; Percent complete: 77.0%; Average loss: 0.9938\n",
            "Iteration: 11554; Percent complete: 77.0%; Average loss: 1.2214\n",
            "Iteration: 11555; Percent complete: 77.0%; Average loss: 1.1085\n",
            "Iteration: 11556; Percent complete: 77.0%; Average loss: 1.0255\n",
            "Iteration: 11557; Percent complete: 77.0%; Average loss: 1.0844\n",
            "Iteration: 11558; Percent complete: 77.1%; Average loss: 1.2743\n",
            "Iteration: 11559; Percent complete: 77.1%; Average loss: 1.0523\n",
            "Iteration: 11560; Percent complete: 77.1%; Average loss: 1.0710\n",
            "Iteration: 11561; Percent complete: 77.1%; Average loss: 1.0436\n",
            "Iteration: 11562; Percent complete: 77.1%; Average loss: 1.0787\n",
            "Iteration: 11563; Percent complete: 77.1%; Average loss: 0.9305\n",
            "Iteration: 11564; Percent complete: 77.1%; Average loss: 1.0363\n",
            "Iteration: 11565; Percent complete: 77.1%; Average loss: 0.9688\n",
            "Iteration: 11566; Percent complete: 77.1%; Average loss: 1.0068\n",
            "Iteration: 11567; Percent complete: 77.1%; Average loss: 1.1718\n",
            "Iteration: 11568; Percent complete: 77.1%; Average loss: 1.0173\n",
            "Iteration: 11569; Percent complete: 77.1%; Average loss: 1.1335\n",
            "Iteration: 11570; Percent complete: 77.1%; Average loss: 1.1816\n",
            "Iteration: 11571; Percent complete: 77.1%; Average loss: 1.1718\n",
            "Iteration: 11572; Percent complete: 77.1%; Average loss: 1.0328\n",
            "Iteration: 11573; Percent complete: 77.2%; Average loss: 1.1969\n",
            "Iteration: 11574; Percent complete: 77.2%; Average loss: 1.0754\n",
            "Iteration: 11575; Percent complete: 77.2%; Average loss: 0.9679\n",
            "Iteration: 11576; Percent complete: 77.2%; Average loss: 0.8822\n",
            "Iteration: 11577; Percent complete: 77.2%; Average loss: 1.0119\n",
            "Iteration: 11578; Percent complete: 77.2%; Average loss: 1.0838\n",
            "Iteration: 11579; Percent complete: 77.2%; Average loss: 1.0946\n",
            "Iteration: 11580; Percent complete: 77.2%; Average loss: 1.1426\n",
            "Iteration: 11581; Percent complete: 77.2%; Average loss: 1.1476\n",
            "Iteration: 11582; Percent complete: 77.2%; Average loss: 1.0275\n",
            "Iteration: 11583; Percent complete: 77.2%; Average loss: 1.1901\n",
            "Iteration: 11584; Percent complete: 77.2%; Average loss: 1.1797\n",
            "Iteration: 11585; Percent complete: 77.2%; Average loss: 1.1550\n",
            "Iteration: 11586; Percent complete: 77.2%; Average loss: 1.2377\n",
            "Iteration: 11587; Percent complete: 77.2%; Average loss: 1.1026\n",
            "Iteration: 11588; Percent complete: 77.3%; Average loss: 1.2246\n",
            "Iteration: 11589; Percent complete: 77.3%; Average loss: 1.1111\n",
            "Iteration: 11590; Percent complete: 77.3%; Average loss: 1.1205\n",
            "Iteration: 11591; Percent complete: 77.3%; Average loss: 1.0644\n",
            "Iteration: 11592; Percent complete: 77.3%; Average loss: 1.3211\n",
            "Iteration: 11593; Percent complete: 77.3%; Average loss: 0.9723\n",
            "Iteration: 11594; Percent complete: 77.3%; Average loss: 1.0815\n",
            "Iteration: 11595; Percent complete: 77.3%; Average loss: 0.9789\n",
            "Iteration: 11596; Percent complete: 77.3%; Average loss: 1.1785\n",
            "Iteration: 11597; Percent complete: 77.3%; Average loss: 1.0016\n",
            "Iteration: 11598; Percent complete: 77.3%; Average loss: 1.2438\n",
            "Iteration: 11599; Percent complete: 77.3%; Average loss: 1.0931\n",
            "Iteration: 11600; Percent complete: 77.3%; Average loss: 0.9905\n",
            "Iteration: 11601; Percent complete: 77.3%; Average loss: 1.0423\n",
            "Iteration: 11602; Percent complete: 77.3%; Average loss: 1.1053\n",
            "Iteration: 11603; Percent complete: 77.4%; Average loss: 0.9422\n",
            "Iteration: 11604; Percent complete: 77.4%; Average loss: 1.0597\n",
            "Iteration: 11605; Percent complete: 77.4%; Average loss: 1.1272\n",
            "Iteration: 11606; Percent complete: 77.4%; Average loss: 0.9150\n",
            "Iteration: 11607; Percent complete: 77.4%; Average loss: 0.9349\n",
            "Iteration: 11608; Percent complete: 77.4%; Average loss: 1.0238\n",
            "Iteration: 11609; Percent complete: 77.4%; Average loss: 1.0522\n",
            "Iteration: 11610; Percent complete: 77.4%; Average loss: 1.0927\n",
            "Iteration: 11611; Percent complete: 77.4%; Average loss: 1.1355\n",
            "Iteration: 11612; Percent complete: 77.4%; Average loss: 1.2228\n",
            "Iteration: 11613; Percent complete: 77.4%; Average loss: 1.2504\n",
            "Iteration: 11614; Percent complete: 77.4%; Average loss: 1.1130\n",
            "Iteration: 11615; Percent complete: 77.4%; Average loss: 1.1729\n",
            "Iteration: 11616; Percent complete: 77.4%; Average loss: 1.0835\n",
            "Iteration: 11617; Percent complete: 77.4%; Average loss: 1.0922\n",
            "Iteration: 11618; Percent complete: 77.5%; Average loss: 0.9776\n",
            "Iteration: 11619; Percent complete: 77.5%; Average loss: 1.0805\n",
            "Iteration: 11620; Percent complete: 77.5%; Average loss: 1.0691\n",
            "Iteration: 11621; Percent complete: 77.5%; Average loss: 1.1088\n",
            "Iteration: 11622; Percent complete: 77.5%; Average loss: 0.9836\n",
            "Iteration: 11623; Percent complete: 77.5%; Average loss: 1.0074\n",
            "Iteration: 11624; Percent complete: 77.5%; Average loss: 1.0438\n",
            "Iteration: 11625; Percent complete: 77.5%; Average loss: 1.2296\n",
            "Iteration: 11626; Percent complete: 77.5%; Average loss: 1.0017\n",
            "Iteration: 11627; Percent complete: 77.5%; Average loss: 1.0266\n",
            "Iteration: 11628; Percent complete: 77.5%; Average loss: 1.1285\n",
            "Iteration: 11629; Percent complete: 77.5%; Average loss: 1.1923\n",
            "Iteration: 11630; Percent complete: 77.5%; Average loss: 1.0023\n",
            "Iteration: 11631; Percent complete: 77.5%; Average loss: 1.2535\n",
            "Iteration: 11632; Percent complete: 77.5%; Average loss: 1.1503\n",
            "Iteration: 11633; Percent complete: 77.6%; Average loss: 1.0789\n",
            "Iteration: 11634; Percent complete: 77.6%; Average loss: 1.0149\n",
            "Iteration: 11635; Percent complete: 77.6%; Average loss: 0.9079\n",
            "Iteration: 11636; Percent complete: 77.6%; Average loss: 1.0078\n",
            "Iteration: 11637; Percent complete: 77.6%; Average loss: 0.9470\n",
            "Iteration: 11638; Percent complete: 77.6%; Average loss: 1.1665\n",
            "Iteration: 11639; Percent complete: 77.6%; Average loss: 1.0657\n",
            "Iteration: 11640; Percent complete: 77.6%; Average loss: 1.0742\n",
            "Iteration: 11641; Percent complete: 77.6%; Average loss: 1.0875\n",
            "Iteration: 11642; Percent complete: 77.6%; Average loss: 0.8360\n",
            "Iteration: 11643; Percent complete: 77.6%; Average loss: 1.0419\n",
            "Iteration: 11644; Percent complete: 77.6%; Average loss: 1.0793\n",
            "Iteration: 11645; Percent complete: 77.6%; Average loss: 1.0350\n",
            "Iteration: 11646; Percent complete: 77.6%; Average loss: 1.0317\n",
            "Iteration: 11647; Percent complete: 77.6%; Average loss: 1.0776\n",
            "Iteration: 11648; Percent complete: 77.7%; Average loss: 0.9477\n",
            "Iteration: 11649; Percent complete: 77.7%; Average loss: 0.9900\n",
            "Iteration: 11650; Percent complete: 77.7%; Average loss: 1.0501\n",
            "Iteration: 11651; Percent complete: 77.7%; Average loss: 1.0766\n",
            "Iteration: 11652; Percent complete: 77.7%; Average loss: 0.9551\n",
            "Iteration: 11653; Percent complete: 77.7%; Average loss: 1.1008\n",
            "Iteration: 11654; Percent complete: 77.7%; Average loss: 1.1031\n",
            "Iteration: 11655; Percent complete: 77.7%; Average loss: 1.0640\n",
            "Iteration: 11656; Percent complete: 77.7%; Average loss: 1.1255\n",
            "Iteration: 11657; Percent complete: 77.7%; Average loss: 1.0961\n",
            "Iteration: 11658; Percent complete: 77.7%; Average loss: 0.9625\n",
            "Iteration: 11659; Percent complete: 77.7%; Average loss: 1.1649\n",
            "Iteration: 11660; Percent complete: 77.7%; Average loss: 1.1795\n",
            "Iteration: 11661; Percent complete: 77.7%; Average loss: 1.0465\n",
            "Iteration: 11662; Percent complete: 77.7%; Average loss: 1.1298\n",
            "Iteration: 11663; Percent complete: 77.8%; Average loss: 1.1572\n",
            "Iteration: 11664; Percent complete: 77.8%; Average loss: 1.1103\n",
            "Iteration: 11665; Percent complete: 77.8%; Average loss: 0.9565\n",
            "Iteration: 11666; Percent complete: 77.8%; Average loss: 1.1508\n",
            "Iteration: 11667; Percent complete: 77.8%; Average loss: 1.1817\n",
            "Iteration: 11668; Percent complete: 77.8%; Average loss: 1.0615\n",
            "Iteration: 11669; Percent complete: 77.8%; Average loss: 1.1425\n",
            "Iteration: 11670; Percent complete: 77.8%; Average loss: 1.0888\n",
            "Iteration: 11671; Percent complete: 77.8%; Average loss: 1.1113\n",
            "Iteration: 11672; Percent complete: 77.8%; Average loss: 1.1324\n",
            "Iteration: 11673; Percent complete: 77.8%; Average loss: 1.2960\n",
            "Iteration: 11674; Percent complete: 77.8%; Average loss: 1.2674\n",
            "Iteration: 11675; Percent complete: 77.8%; Average loss: 1.1628\n",
            "Iteration: 11676; Percent complete: 77.8%; Average loss: 1.1722\n",
            "Iteration: 11677; Percent complete: 77.8%; Average loss: 1.1305\n",
            "Iteration: 11678; Percent complete: 77.9%; Average loss: 0.9738\n",
            "Iteration: 11679; Percent complete: 77.9%; Average loss: 1.0104\n",
            "Iteration: 11680; Percent complete: 77.9%; Average loss: 0.8886\n",
            "Iteration: 11681; Percent complete: 77.9%; Average loss: 1.0604\n",
            "Iteration: 11682; Percent complete: 77.9%; Average loss: 1.1746\n",
            "Iteration: 11683; Percent complete: 77.9%; Average loss: 1.1717\n",
            "Iteration: 11684; Percent complete: 77.9%; Average loss: 1.0067\n",
            "Iteration: 11685; Percent complete: 77.9%; Average loss: 1.0770\n",
            "Iteration: 11686; Percent complete: 77.9%; Average loss: 1.0188\n",
            "Iteration: 11687; Percent complete: 77.9%; Average loss: 1.1284\n",
            "Iteration: 11688; Percent complete: 77.9%; Average loss: 1.1302\n",
            "Iteration: 11689; Percent complete: 77.9%; Average loss: 1.1845\n",
            "Iteration: 11690; Percent complete: 77.9%; Average loss: 0.9022\n",
            "Iteration: 11691; Percent complete: 77.9%; Average loss: 1.1835\n",
            "Iteration: 11692; Percent complete: 77.9%; Average loss: 1.1318\n",
            "Iteration: 11693; Percent complete: 78.0%; Average loss: 0.9725\n",
            "Iteration: 11694; Percent complete: 78.0%; Average loss: 1.0681\n",
            "Iteration: 11695; Percent complete: 78.0%; Average loss: 1.1415\n",
            "Iteration: 11696; Percent complete: 78.0%; Average loss: 1.2617\n",
            "Iteration: 11697; Percent complete: 78.0%; Average loss: 1.2470\n",
            "Iteration: 11698; Percent complete: 78.0%; Average loss: 1.1023\n",
            "Iteration: 11699; Percent complete: 78.0%; Average loss: 1.1623\n",
            "Iteration: 11700; Percent complete: 78.0%; Average loss: 1.1421\n",
            "Iteration: 11701; Percent complete: 78.0%; Average loss: 1.1038\n",
            "Iteration: 11702; Percent complete: 78.0%; Average loss: 0.9379\n",
            "Iteration: 11703; Percent complete: 78.0%; Average loss: 0.9347\n",
            "Iteration: 11704; Percent complete: 78.0%; Average loss: 1.0174\n",
            "Iteration: 11705; Percent complete: 78.0%; Average loss: 1.0782\n",
            "Iteration: 11706; Percent complete: 78.0%; Average loss: 0.8762\n",
            "Iteration: 11707; Percent complete: 78.0%; Average loss: 1.0403\n",
            "Iteration: 11708; Percent complete: 78.1%; Average loss: 0.9927\n",
            "Iteration: 11709; Percent complete: 78.1%; Average loss: 1.1457\n",
            "Iteration: 11710; Percent complete: 78.1%; Average loss: 0.9471\n",
            "Iteration: 11711; Percent complete: 78.1%; Average loss: 1.1494\n",
            "Iteration: 11712; Percent complete: 78.1%; Average loss: 1.1858\n",
            "Iteration: 11713; Percent complete: 78.1%; Average loss: 1.0711\n",
            "Iteration: 11714; Percent complete: 78.1%; Average loss: 1.0718\n",
            "Iteration: 11715; Percent complete: 78.1%; Average loss: 1.1254\n",
            "Iteration: 11716; Percent complete: 78.1%; Average loss: 0.9709\n",
            "Iteration: 11717; Percent complete: 78.1%; Average loss: 0.9518\n",
            "Iteration: 11718; Percent complete: 78.1%; Average loss: 1.0549\n",
            "Iteration: 11719; Percent complete: 78.1%; Average loss: 1.0691\n",
            "Iteration: 11720; Percent complete: 78.1%; Average loss: 0.9305\n",
            "Iteration: 11721; Percent complete: 78.1%; Average loss: 1.0572\n",
            "Iteration: 11722; Percent complete: 78.1%; Average loss: 0.9633\n",
            "Iteration: 11723; Percent complete: 78.2%; Average loss: 1.1575\n",
            "Iteration: 11724; Percent complete: 78.2%; Average loss: 1.2649\n",
            "Iteration: 11725; Percent complete: 78.2%; Average loss: 1.1704\n",
            "Iteration: 11726; Percent complete: 78.2%; Average loss: 1.0657\n",
            "Iteration: 11727; Percent complete: 78.2%; Average loss: 1.1452\n",
            "Iteration: 11728; Percent complete: 78.2%; Average loss: 1.1005\n",
            "Iteration: 11729; Percent complete: 78.2%; Average loss: 1.2247\n",
            "Iteration: 11730; Percent complete: 78.2%; Average loss: 1.1828\n",
            "Iteration: 11731; Percent complete: 78.2%; Average loss: 1.0988\n",
            "Iteration: 11732; Percent complete: 78.2%; Average loss: 1.0482\n",
            "Iteration: 11733; Percent complete: 78.2%; Average loss: 0.9876\n",
            "Iteration: 11734; Percent complete: 78.2%; Average loss: 0.9985\n",
            "Iteration: 11735; Percent complete: 78.2%; Average loss: 1.2264\n",
            "Iteration: 11736; Percent complete: 78.2%; Average loss: 1.0334\n",
            "Iteration: 11737; Percent complete: 78.2%; Average loss: 1.0299\n",
            "Iteration: 11738; Percent complete: 78.3%; Average loss: 1.0875\n",
            "Iteration: 11739; Percent complete: 78.3%; Average loss: 1.1389\n",
            "Iteration: 11740; Percent complete: 78.3%; Average loss: 1.1253\n",
            "Iteration: 11741; Percent complete: 78.3%; Average loss: 1.0500\n",
            "Iteration: 11742; Percent complete: 78.3%; Average loss: 1.0779\n",
            "Iteration: 11743; Percent complete: 78.3%; Average loss: 0.9985\n",
            "Iteration: 11744; Percent complete: 78.3%; Average loss: 0.9826\n",
            "Iteration: 11745; Percent complete: 78.3%; Average loss: 0.9481\n",
            "Iteration: 11746; Percent complete: 78.3%; Average loss: 1.0888\n",
            "Iteration: 11747; Percent complete: 78.3%; Average loss: 0.9891\n",
            "Iteration: 11748; Percent complete: 78.3%; Average loss: 1.0136\n",
            "Iteration: 11749; Percent complete: 78.3%; Average loss: 1.0472\n",
            "Iteration: 11750; Percent complete: 78.3%; Average loss: 1.0110\n",
            "Iteration: 11751; Percent complete: 78.3%; Average loss: 1.2613\n",
            "Iteration: 11752; Percent complete: 78.3%; Average loss: 1.0597\n",
            "Iteration: 11753; Percent complete: 78.4%; Average loss: 1.0729\n",
            "Iteration: 11754; Percent complete: 78.4%; Average loss: 0.9504\n",
            "Iteration: 11755; Percent complete: 78.4%; Average loss: 1.0637\n",
            "Iteration: 11756; Percent complete: 78.4%; Average loss: 0.9036\n",
            "Iteration: 11757; Percent complete: 78.4%; Average loss: 1.2672\n",
            "Iteration: 11758; Percent complete: 78.4%; Average loss: 0.9517\n",
            "Iteration: 11759; Percent complete: 78.4%; Average loss: 1.2400\n",
            "Iteration: 11760; Percent complete: 78.4%; Average loss: 1.0606\n",
            "Iteration: 11761; Percent complete: 78.4%; Average loss: 1.0120\n",
            "Iteration: 11762; Percent complete: 78.4%; Average loss: 1.0200\n",
            "Iteration: 11763; Percent complete: 78.4%; Average loss: 1.0125\n",
            "Iteration: 11764; Percent complete: 78.4%; Average loss: 1.0193\n",
            "Iteration: 11765; Percent complete: 78.4%; Average loss: 0.8932\n",
            "Iteration: 11766; Percent complete: 78.4%; Average loss: 0.9218\n",
            "Iteration: 11767; Percent complete: 78.4%; Average loss: 0.8815\n",
            "Iteration: 11768; Percent complete: 78.5%; Average loss: 1.2227\n",
            "Iteration: 11769; Percent complete: 78.5%; Average loss: 0.9499\n",
            "Iteration: 11770; Percent complete: 78.5%; Average loss: 1.1336\n",
            "Iteration: 11771; Percent complete: 78.5%; Average loss: 1.0392\n",
            "Iteration: 11772; Percent complete: 78.5%; Average loss: 0.9870\n",
            "Iteration: 11773; Percent complete: 78.5%; Average loss: 1.0755\n",
            "Iteration: 11774; Percent complete: 78.5%; Average loss: 1.1525\n",
            "Iteration: 11775; Percent complete: 78.5%; Average loss: 1.0997\n",
            "Iteration: 11776; Percent complete: 78.5%; Average loss: 0.9515\n",
            "Iteration: 11777; Percent complete: 78.5%; Average loss: 1.0804\n",
            "Iteration: 11778; Percent complete: 78.5%; Average loss: 1.2111\n",
            "Iteration: 11779; Percent complete: 78.5%; Average loss: 1.1248\n",
            "Iteration: 11780; Percent complete: 78.5%; Average loss: 1.0896\n",
            "Iteration: 11781; Percent complete: 78.5%; Average loss: 1.0126\n",
            "Iteration: 11782; Percent complete: 78.5%; Average loss: 0.9018\n",
            "Iteration: 11783; Percent complete: 78.6%; Average loss: 0.9980\n",
            "Iteration: 11784; Percent complete: 78.6%; Average loss: 0.9963\n",
            "Iteration: 11785; Percent complete: 78.6%; Average loss: 1.0304\n",
            "Iteration: 11786; Percent complete: 78.6%; Average loss: 1.0827\n",
            "Iteration: 11787; Percent complete: 78.6%; Average loss: 1.0531\n",
            "Iteration: 11788; Percent complete: 78.6%; Average loss: 1.2487\n",
            "Iteration: 11789; Percent complete: 78.6%; Average loss: 0.9937\n",
            "Iteration: 11790; Percent complete: 78.6%; Average loss: 1.1475\n",
            "Iteration: 11791; Percent complete: 78.6%; Average loss: 1.2065\n",
            "Iteration: 11792; Percent complete: 78.6%; Average loss: 1.0417\n",
            "Iteration: 11793; Percent complete: 78.6%; Average loss: 1.0446\n",
            "Iteration: 11794; Percent complete: 78.6%; Average loss: 1.0806\n",
            "Iteration: 11795; Percent complete: 78.6%; Average loss: 1.1826\n",
            "Iteration: 11796; Percent complete: 78.6%; Average loss: 0.9238\n",
            "Iteration: 11797; Percent complete: 78.6%; Average loss: 1.0270\n",
            "Iteration: 11798; Percent complete: 78.7%; Average loss: 1.1719\n",
            "Iteration: 11799; Percent complete: 78.7%; Average loss: 1.0994\n",
            "Iteration: 11800; Percent complete: 78.7%; Average loss: 1.1157\n",
            "Iteration: 11801; Percent complete: 78.7%; Average loss: 0.9519\n",
            "Iteration: 11802; Percent complete: 78.7%; Average loss: 1.2249\n",
            "Iteration: 11803; Percent complete: 78.7%; Average loss: 1.0878\n",
            "Iteration: 11804; Percent complete: 78.7%; Average loss: 1.1661\n",
            "Iteration: 11805; Percent complete: 78.7%; Average loss: 1.0680\n",
            "Iteration: 11806; Percent complete: 78.7%; Average loss: 0.9498\n",
            "Iteration: 11807; Percent complete: 78.7%; Average loss: 0.9912\n",
            "Iteration: 11808; Percent complete: 78.7%; Average loss: 1.0590\n",
            "Iteration: 11809; Percent complete: 78.7%; Average loss: 0.9966\n",
            "Iteration: 11810; Percent complete: 78.7%; Average loss: 1.0515\n",
            "Iteration: 11811; Percent complete: 78.7%; Average loss: 1.3068\n",
            "Iteration: 11812; Percent complete: 78.7%; Average loss: 1.1810\n",
            "Iteration: 11813; Percent complete: 78.8%; Average loss: 1.2502\n",
            "Iteration: 11814; Percent complete: 78.8%; Average loss: 1.0689\n",
            "Iteration: 11815; Percent complete: 78.8%; Average loss: 1.0951\n",
            "Iteration: 11816; Percent complete: 78.8%; Average loss: 1.0186\n",
            "Iteration: 11817; Percent complete: 78.8%; Average loss: 1.1299\n",
            "Iteration: 11818; Percent complete: 78.8%; Average loss: 1.0890\n",
            "Iteration: 11819; Percent complete: 78.8%; Average loss: 1.0798\n",
            "Iteration: 11820; Percent complete: 78.8%; Average loss: 1.1244\n",
            "Iteration: 11821; Percent complete: 78.8%; Average loss: 1.0566\n",
            "Iteration: 11822; Percent complete: 78.8%; Average loss: 1.0659\n",
            "Iteration: 11823; Percent complete: 78.8%; Average loss: 1.0449\n",
            "Iteration: 11824; Percent complete: 78.8%; Average loss: 0.9382\n",
            "Iteration: 11825; Percent complete: 78.8%; Average loss: 1.1371\n",
            "Iteration: 11826; Percent complete: 78.8%; Average loss: 1.3273\n",
            "Iteration: 11827; Percent complete: 78.8%; Average loss: 0.9829\n",
            "Iteration: 11828; Percent complete: 78.9%; Average loss: 1.1035\n",
            "Iteration: 11829; Percent complete: 78.9%; Average loss: 0.9465\n",
            "Iteration: 11830; Percent complete: 78.9%; Average loss: 1.1276\n",
            "Iteration: 11831; Percent complete: 78.9%; Average loss: 0.9581\n",
            "Iteration: 11832; Percent complete: 78.9%; Average loss: 1.1211\n",
            "Iteration: 11833; Percent complete: 78.9%; Average loss: 1.0847\n",
            "Iteration: 11834; Percent complete: 78.9%; Average loss: 1.0377\n",
            "Iteration: 11835; Percent complete: 78.9%; Average loss: 0.9459\n",
            "Iteration: 11836; Percent complete: 78.9%; Average loss: 1.1651\n",
            "Iteration: 11837; Percent complete: 78.9%; Average loss: 0.9372\n",
            "Iteration: 11838; Percent complete: 78.9%; Average loss: 0.9272\n",
            "Iteration: 11839; Percent complete: 78.9%; Average loss: 1.0700\n",
            "Iteration: 11840; Percent complete: 78.9%; Average loss: 1.1998\n",
            "Iteration: 11841; Percent complete: 78.9%; Average loss: 1.0961\n",
            "Iteration: 11842; Percent complete: 78.9%; Average loss: 0.9997\n",
            "Iteration: 11843; Percent complete: 79.0%; Average loss: 1.0409\n",
            "Iteration: 11844; Percent complete: 79.0%; Average loss: 1.1433\n",
            "Iteration: 11845; Percent complete: 79.0%; Average loss: 0.9478\n",
            "Iteration: 11846; Percent complete: 79.0%; Average loss: 1.0402\n",
            "Iteration: 11847; Percent complete: 79.0%; Average loss: 1.0113\n",
            "Iteration: 11848; Percent complete: 79.0%; Average loss: 1.1885\n",
            "Iteration: 11849; Percent complete: 79.0%; Average loss: 1.0033\n",
            "Iteration: 11850; Percent complete: 79.0%; Average loss: 1.0071\n",
            "Iteration: 11851; Percent complete: 79.0%; Average loss: 1.1635\n",
            "Iteration: 11852; Percent complete: 79.0%; Average loss: 1.1183\n",
            "Iteration: 11853; Percent complete: 79.0%; Average loss: 0.9909\n",
            "Iteration: 11854; Percent complete: 79.0%; Average loss: 1.1429\n",
            "Iteration: 11855; Percent complete: 79.0%; Average loss: 1.0779\n",
            "Iteration: 11856; Percent complete: 79.0%; Average loss: 0.9327\n",
            "Iteration: 11857; Percent complete: 79.0%; Average loss: 0.8957\n",
            "Iteration: 11858; Percent complete: 79.1%; Average loss: 1.1113\n",
            "Iteration: 11859; Percent complete: 79.1%; Average loss: 0.9533\n",
            "Iteration: 11860; Percent complete: 79.1%; Average loss: 1.0787\n",
            "Iteration: 11861; Percent complete: 79.1%; Average loss: 0.9156\n",
            "Iteration: 11862; Percent complete: 79.1%; Average loss: 1.1689\n",
            "Iteration: 11863; Percent complete: 79.1%; Average loss: 0.9817\n",
            "Iteration: 11864; Percent complete: 79.1%; Average loss: 1.2468\n",
            "Iteration: 11865; Percent complete: 79.1%; Average loss: 1.2153\n",
            "Iteration: 11866; Percent complete: 79.1%; Average loss: 0.9848\n",
            "Iteration: 11867; Percent complete: 79.1%; Average loss: 1.0587\n",
            "Iteration: 11868; Percent complete: 79.1%; Average loss: 0.9012\n",
            "Iteration: 11869; Percent complete: 79.1%; Average loss: 1.1029\n",
            "Iteration: 11870; Percent complete: 79.1%; Average loss: 0.8752\n",
            "Iteration: 11871; Percent complete: 79.1%; Average loss: 1.1009\n",
            "Iteration: 11872; Percent complete: 79.1%; Average loss: 0.9857\n",
            "Iteration: 11873; Percent complete: 79.2%; Average loss: 0.9737\n",
            "Iteration: 11874; Percent complete: 79.2%; Average loss: 0.8767\n",
            "Iteration: 11875; Percent complete: 79.2%; Average loss: 1.0279\n",
            "Iteration: 11876; Percent complete: 79.2%; Average loss: 1.1460\n",
            "Iteration: 11877; Percent complete: 79.2%; Average loss: 1.0529\n",
            "Iteration: 11878; Percent complete: 79.2%; Average loss: 0.9548\n",
            "Iteration: 11879; Percent complete: 79.2%; Average loss: 1.0306\n",
            "Iteration: 11880; Percent complete: 79.2%; Average loss: 1.0340\n",
            "Iteration: 11881; Percent complete: 79.2%; Average loss: 1.1316\n",
            "Iteration: 11882; Percent complete: 79.2%; Average loss: 0.9388\n",
            "Iteration: 11883; Percent complete: 79.2%; Average loss: 0.9119\n",
            "Iteration: 11884; Percent complete: 79.2%; Average loss: 1.0651\n",
            "Iteration: 11885; Percent complete: 79.2%; Average loss: 1.0032\n",
            "Iteration: 11886; Percent complete: 79.2%; Average loss: 1.0573\n",
            "Iteration: 11887; Percent complete: 79.2%; Average loss: 1.1899\n",
            "Iteration: 11888; Percent complete: 79.3%; Average loss: 0.9191\n",
            "Iteration: 11889; Percent complete: 79.3%; Average loss: 1.0073\n",
            "Iteration: 11890; Percent complete: 79.3%; Average loss: 0.9061\n",
            "Iteration: 11891; Percent complete: 79.3%; Average loss: 1.0211\n",
            "Iteration: 11892; Percent complete: 79.3%; Average loss: 1.0338\n",
            "Iteration: 11893; Percent complete: 79.3%; Average loss: 1.0131\n",
            "Iteration: 11894; Percent complete: 79.3%; Average loss: 0.9653\n",
            "Iteration: 11895; Percent complete: 79.3%; Average loss: 1.0268\n",
            "Iteration: 11896; Percent complete: 79.3%; Average loss: 0.9829\n",
            "Iteration: 11897; Percent complete: 79.3%; Average loss: 0.9751\n",
            "Iteration: 11898; Percent complete: 79.3%; Average loss: 1.0425\n",
            "Iteration: 11899; Percent complete: 79.3%; Average loss: 1.0445\n",
            "Iteration: 11900; Percent complete: 79.3%; Average loss: 1.0498\n",
            "Iteration: 11901; Percent complete: 79.3%; Average loss: 1.1027\n",
            "Iteration: 11902; Percent complete: 79.3%; Average loss: 0.9726\n",
            "Iteration: 11903; Percent complete: 79.4%; Average loss: 0.9557\n",
            "Iteration: 11904; Percent complete: 79.4%; Average loss: 1.0132\n",
            "Iteration: 11905; Percent complete: 79.4%; Average loss: 1.2370\n",
            "Iteration: 11906; Percent complete: 79.4%; Average loss: 1.1298\n",
            "Iteration: 11907; Percent complete: 79.4%; Average loss: 1.2173\n",
            "Iteration: 11908; Percent complete: 79.4%; Average loss: 1.1645\n",
            "Iteration: 11909; Percent complete: 79.4%; Average loss: 1.0591\n",
            "Iteration: 11910; Percent complete: 79.4%; Average loss: 0.9404\n",
            "Iteration: 11911; Percent complete: 79.4%; Average loss: 0.9263\n",
            "Iteration: 11912; Percent complete: 79.4%; Average loss: 1.0680\n",
            "Iteration: 11913; Percent complete: 79.4%; Average loss: 1.1849\n",
            "Iteration: 11914; Percent complete: 79.4%; Average loss: 1.1807\n",
            "Iteration: 11915; Percent complete: 79.4%; Average loss: 1.1125\n",
            "Iteration: 11916; Percent complete: 79.4%; Average loss: 0.9848\n",
            "Iteration: 11917; Percent complete: 79.4%; Average loss: 1.1956\n",
            "Iteration: 11918; Percent complete: 79.5%; Average loss: 1.0165\n",
            "Iteration: 11919; Percent complete: 79.5%; Average loss: 1.0695\n",
            "Iteration: 11920; Percent complete: 79.5%; Average loss: 1.1740\n",
            "Iteration: 11921; Percent complete: 79.5%; Average loss: 1.0340\n",
            "Iteration: 11922; Percent complete: 79.5%; Average loss: 0.9864\n",
            "Iteration: 11923; Percent complete: 79.5%; Average loss: 0.9376\n",
            "Iteration: 11924; Percent complete: 79.5%; Average loss: 0.9946\n",
            "Iteration: 11925; Percent complete: 79.5%; Average loss: 1.1266\n",
            "Iteration: 11926; Percent complete: 79.5%; Average loss: 1.0024\n",
            "Iteration: 11927; Percent complete: 79.5%; Average loss: 1.1735\n",
            "Iteration: 11928; Percent complete: 79.5%; Average loss: 0.9695\n",
            "Iteration: 11929; Percent complete: 79.5%; Average loss: 1.2238\n",
            "Iteration: 11930; Percent complete: 79.5%; Average loss: 1.0696\n",
            "Iteration: 11931; Percent complete: 79.5%; Average loss: 1.1250\n",
            "Iteration: 11932; Percent complete: 79.5%; Average loss: 1.0614\n",
            "Iteration: 11933; Percent complete: 79.6%; Average loss: 1.1729\n",
            "Iteration: 11934; Percent complete: 79.6%; Average loss: 1.0612\n",
            "Iteration: 11935; Percent complete: 79.6%; Average loss: 0.9062\n",
            "Iteration: 11936; Percent complete: 79.6%; Average loss: 1.0607\n",
            "Iteration: 11937; Percent complete: 79.6%; Average loss: 1.1839\n",
            "Iteration: 11938; Percent complete: 79.6%; Average loss: 1.0373\n",
            "Iteration: 11939; Percent complete: 79.6%; Average loss: 1.0182\n",
            "Iteration: 11940; Percent complete: 79.6%; Average loss: 1.1921\n",
            "Iteration: 11941; Percent complete: 79.6%; Average loss: 1.0843\n",
            "Iteration: 11942; Percent complete: 79.6%; Average loss: 1.1554\n",
            "Iteration: 11943; Percent complete: 79.6%; Average loss: 1.0444\n",
            "Iteration: 11944; Percent complete: 79.6%; Average loss: 1.1331\n",
            "Iteration: 11945; Percent complete: 79.6%; Average loss: 0.9704\n",
            "Iteration: 11946; Percent complete: 79.6%; Average loss: 1.0510\n",
            "Iteration: 11947; Percent complete: 79.6%; Average loss: 0.8469\n",
            "Iteration: 11948; Percent complete: 79.7%; Average loss: 1.0849\n",
            "Iteration: 11949; Percent complete: 79.7%; Average loss: 1.0226\n",
            "Iteration: 11950; Percent complete: 79.7%; Average loss: 0.9289\n",
            "Iteration: 11951; Percent complete: 79.7%; Average loss: 0.9106\n",
            "Iteration: 11952; Percent complete: 79.7%; Average loss: 0.9415\n",
            "Iteration: 11953; Percent complete: 79.7%; Average loss: 1.0146\n",
            "Iteration: 11954; Percent complete: 79.7%; Average loss: 0.9217\n",
            "Iteration: 11955; Percent complete: 79.7%; Average loss: 1.0203\n",
            "Iteration: 11956; Percent complete: 79.7%; Average loss: 1.1766\n",
            "Iteration: 11957; Percent complete: 79.7%; Average loss: 1.0837\n",
            "Iteration: 11958; Percent complete: 79.7%; Average loss: 1.2199\n",
            "Iteration: 11959; Percent complete: 79.7%; Average loss: 1.1062\n",
            "Iteration: 11960; Percent complete: 79.7%; Average loss: 1.0551\n",
            "Iteration: 11961; Percent complete: 79.7%; Average loss: 0.9922\n",
            "Iteration: 11962; Percent complete: 79.7%; Average loss: 0.9882\n",
            "Iteration: 11963; Percent complete: 79.8%; Average loss: 0.9365\n",
            "Iteration: 11964; Percent complete: 79.8%; Average loss: 0.9469\n",
            "Iteration: 11965; Percent complete: 79.8%; Average loss: 0.8995\n",
            "Iteration: 11966; Percent complete: 79.8%; Average loss: 1.0294\n",
            "Iteration: 11967; Percent complete: 79.8%; Average loss: 0.9800\n",
            "Iteration: 11968; Percent complete: 79.8%; Average loss: 1.0479\n",
            "Iteration: 11969; Percent complete: 79.8%; Average loss: 1.0680\n",
            "Iteration: 11970; Percent complete: 79.8%; Average loss: 0.9276\n",
            "Iteration: 11971; Percent complete: 79.8%; Average loss: 1.0314\n",
            "Iteration: 11972; Percent complete: 79.8%; Average loss: 0.8127\n",
            "Iteration: 11973; Percent complete: 79.8%; Average loss: 1.2531\n",
            "Iteration: 11974; Percent complete: 79.8%; Average loss: 1.1935\n",
            "Iteration: 11975; Percent complete: 79.8%; Average loss: 0.8554\n",
            "Iteration: 11976; Percent complete: 79.8%; Average loss: 1.2372\n",
            "Iteration: 11977; Percent complete: 79.8%; Average loss: 0.8692\n",
            "Iteration: 11978; Percent complete: 79.9%; Average loss: 1.1042\n",
            "Iteration: 11979; Percent complete: 79.9%; Average loss: 1.1522\n",
            "Iteration: 11980; Percent complete: 79.9%; Average loss: 0.9364\n",
            "Iteration: 11981; Percent complete: 79.9%; Average loss: 1.1724\n",
            "Iteration: 11982; Percent complete: 79.9%; Average loss: 1.0349\n",
            "Iteration: 11983; Percent complete: 79.9%; Average loss: 1.0581\n",
            "Iteration: 11984; Percent complete: 79.9%; Average loss: 1.0399\n",
            "Iteration: 11985; Percent complete: 79.9%; Average loss: 1.0176\n",
            "Iteration: 11986; Percent complete: 79.9%; Average loss: 1.0824\n",
            "Iteration: 11987; Percent complete: 79.9%; Average loss: 1.0031\n",
            "Iteration: 11988; Percent complete: 79.9%; Average loss: 1.0373\n",
            "Iteration: 11989; Percent complete: 79.9%; Average loss: 0.9186\n",
            "Iteration: 11990; Percent complete: 79.9%; Average loss: 1.1525\n",
            "Iteration: 11991; Percent complete: 79.9%; Average loss: 1.0478\n",
            "Iteration: 11992; Percent complete: 79.9%; Average loss: 0.8908\n",
            "Iteration: 11993; Percent complete: 80.0%; Average loss: 0.9938\n",
            "Iteration: 11994; Percent complete: 80.0%; Average loss: 1.0343\n",
            "Iteration: 11995; Percent complete: 80.0%; Average loss: 1.1636\n",
            "Iteration: 11996; Percent complete: 80.0%; Average loss: 1.1232\n",
            "Iteration: 11997; Percent complete: 80.0%; Average loss: 1.1659\n",
            "Iteration: 11998; Percent complete: 80.0%; Average loss: 1.0545\n",
            "Iteration: 11999; Percent complete: 80.0%; Average loss: 1.0690\n",
            "Iteration: 12000; Percent complete: 80.0%; Average loss: 0.9605\n",
            "Iteration: 12001; Percent complete: 80.0%; Average loss: 1.1038\n",
            "Iteration: 12002; Percent complete: 80.0%; Average loss: 1.1960\n",
            "Iteration: 12003; Percent complete: 80.0%; Average loss: 1.0874\n",
            "Iteration: 12004; Percent complete: 80.0%; Average loss: 0.8584\n",
            "Iteration: 12005; Percent complete: 80.0%; Average loss: 0.9390\n",
            "Iteration: 12006; Percent complete: 80.0%; Average loss: 1.0436\n",
            "Iteration: 12007; Percent complete: 80.0%; Average loss: 1.0605\n",
            "Iteration: 12008; Percent complete: 80.1%; Average loss: 1.3476\n",
            "Iteration: 12009; Percent complete: 80.1%; Average loss: 1.0650\n",
            "Iteration: 12010; Percent complete: 80.1%; Average loss: 1.0271\n",
            "Iteration: 12011; Percent complete: 80.1%; Average loss: 0.8986\n",
            "Iteration: 12012; Percent complete: 80.1%; Average loss: 1.0627\n",
            "Iteration: 12013; Percent complete: 80.1%; Average loss: 1.0579\n",
            "Iteration: 12014; Percent complete: 80.1%; Average loss: 1.1969\n",
            "Iteration: 12015; Percent complete: 80.1%; Average loss: 1.0307\n",
            "Iteration: 12016; Percent complete: 80.1%; Average loss: 1.1115\n",
            "Iteration: 12017; Percent complete: 80.1%; Average loss: 0.9609\n",
            "Iteration: 12018; Percent complete: 80.1%; Average loss: 1.0324\n",
            "Iteration: 12019; Percent complete: 80.1%; Average loss: 0.9936\n",
            "Iteration: 12020; Percent complete: 80.1%; Average loss: 0.9732\n",
            "Iteration: 12021; Percent complete: 80.1%; Average loss: 1.0651\n",
            "Iteration: 12022; Percent complete: 80.1%; Average loss: 1.1222\n",
            "Iteration: 12023; Percent complete: 80.2%; Average loss: 0.9318\n",
            "Iteration: 12024; Percent complete: 80.2%; Average loss: 1.0606\n",
            "Iteration: 12025; Percent complete: 80.2%; Average loss: 0.9921\n",
            "Iteration: 12026; Percent complete: 80.2%; Average loss: 1.1214\n",
            "Iteration: 12027; Percent complete: 80.2%; Average loss: 1.0312\n",
            "Iteration: 12028; Percent complete: 80.2%; Average loss: 1.0519\n",
            "Iteration: 12029; Percent complete: 80.2%; Average loss: 1.0466\n",
            "Iteration: 12030; Percent complete: 80.2%; Average loss: 1.0147\n",
            "Iteration: 12031; Percent complete: 80.2%; Average loss: 1.0153\n",
            "Iteration: 12032; Percent complete: 80.2%; Average loss: 1.0917\n",
            "Iteration: 12033; Percent complete: 80.2%; Average loss: 1.1736\n",
            "Iteration: 12034; Percent complete: 80.2%; Average loss: 0.8879\n",
            "Iteration: 12035; Percent complete: 80.2%; Average loss: 1.1093\n",
            "Iteration: 12036; Percent complete: 80.2%; Average loss: 1.1428\n",
            "Iteration: 12037; Percent complete: 80.2%; Average loss: 1.1638\n",
            "Iteration: 12038; Percent complete: 80.3%; Average loss: 1.0281\n",
            "Iteration: 12039; Percent complete: 80.3%; Average loss: 0.9189\n",
            "Iteration: 12040; Percent complete: 80.3%; Average loss: 1.1797\n",
            "Iteration: 12041; Percent complete: 80.3%; Average loss: 0.9570\n",
            "Iteration: 12042; Percent complete: 80.3%; Average loss: 1.0911\n",
            "Iteration: 12043; Percent complete: 80.3%; Average loss: 1.0192\n",
            "Iteration: 12044; Percent complete: 80.3%; Average loss: 1.1830\n",
            "Iteration: 12045; Percent complete: 80.3%; Average loss: 0.9591\n",
            "Iteration: 12046; Percent complete: 80.3%; Average loss: 0.9045\n",
            "Iteration: 12047; Percent complete: 80.3%; Average loss: 1.0037\n",
            "Iteration: 12048; Percent complete: 80.3%; Average loss: 0.9561\n",
            "Iteration: 12049; Percent complete: 80.3%; Average loss: 1.1273\n",
            "Iteration: 12050; Percent complete: 80.3%; Average loss: 1.0476\n",
            "Iteration: 12051; Percent complete: 80.3%; Average loss: 1.0764\n",
            "Iteration: 12052; Percent complete: 80.3%; Average loss: 1.0823\n",
            "Iteration: 12053; Percent complete: 80.4%; Average loss: 1.0606\n",
            "Iteration: 12054; Percent complete: 80.4%; Average loss: 1.0864\n",
            "Iteration: 12055; Percent complete: 80.4%; Average loss: 1.0155\n",
            "Iteration: 12056; Percent complete: 80.4%; Average loss: 1.0571\n",
            "Iteration: 12057; Percent complete: 80.4%; Average loss: 0.9796\n",
            "Iteration: 12058; Percent complete: 80.4%; Average loss: 0.8872\n",
            "Iteration: 12059; Percent complete: 80.4%; Average loss: 1.1037\n",
            "Iteration: 12060; Percent complete: 80.4%; Average loss: 1.1731\n",
            "Iteration: 12061; Percent complete: 80.4%; Average loss: 1.0645\n",
            "Iteration: 12062; Percent complete: 80.4%; Average loss: 0.8638\n",
            "Iteration: 12063; Percent complete: 80.4%; Average loss: 1.2067\n",
            "Iteration: 12064; Percent complete: 80.4%; Average loss: 1.0392\n",
            "Iteration: 12065; Percent complete: 80.4%; Average loss: 0.9865\n",
            "Iteration: 12066; Percent complete: 80.4%; Average loss: 0.9327\n",
            "Iteration: 12067; Percent complete: 80.4%; Average loss: 0.9907\n",
            "Iteration: 12068; Percent complete: 80.5%; Average loss: 1.0248\n",
            "Iteration: 12069; Percent complete: 80.5%; Average loss: 1.0450\n",
            "Iteration: 12070; Percent complete: 80.5%; Average loss: 1.1213\n",
            "Iteration: 12071; Percent complete: 80.5%; Average loss: 1.0168\n",
            "Iteration: 12072; Percent complete: 80.5%; Average loss: 1.0397\n",
            "Iteration: 12073; Percent complete: 80.5%; Average loss: 0.8880\n",
            "Iteration: 12074; Percent complete: 80.5%; Average loss: 1.0369\n",
            "Iteration: 12075; Percent complete: 80.5%; Average loss: 0.9548\n",
            "Iteration: 12076; Percent complete: 80.5%; Average loss: 0.9459\n",
            "Iteration: 12077; Percent complete: 80.5%; Average loss: 0.9392\n",
            "Iteration: 12078; Percent complete: 80.5%; Average loss: 0.8636\n",
            "Iteration: 12079; Percent complete: 80.5%; Average loss: 0.9750\n",
            "Iteration: 12080; Percent complete: 80.5%; Average loss: 1.0122\n",
            "Iteration: 12081; Percent complete: 80.5%; Average loss: 1.0207\n",
            "Iteration: 12082; Percent complete: 80.5%; Average loss: 0.9598\n",
            "Iteration: 12083; Percent complete: 80.6%; Average loss: 1.0788\n",
            "Iteration: 12084; Percent complete: 80.6%; Average loss: 0.9465\n",
            "Iteration: 12085; Percent complete: 80.6%; Average loss: 0.9399\n",
            "Iteration: 12086; Percent complete: 80.6%; Average loss: 1.2150\n",
            "Iteration: 12087; Percent complete: 80.6%; Average loss: 1.1529\n",
            "Iteration: 12088; Percent complete: 80.6%; Average loss: 0.9426\n",
            "Iteration: 12089; Percent complete: 80.6%; Average loss: 1.0748\n",
            "Iteration: 12090; Percent complete: 80.6%; Average loss: 1.0308\n",
            "Iteration: 12091; Percent complete: 80.6%; Average loss: 1.0031\n",
            "Iteration: 12092; Percent complete: 80.6%; Average loss: 1.0152\n",
            "Iteration: 12093; Percent complete: 80.6%; Average loss: 1.0347\n",
            "Iteration: 12094; Percent complete: 80.6%; Average loss: 1.1454\n",
            "Iteration: 12095; Percent complete: 80.6%; Average loss: 1.0156\n",
            "Iteration: 12096; Percent complete: 80.6%; Average loss: 1.0257\n",
            "Iteration: 12097; Percent complete: 80.6%; Average loss: 0.9056\n",
            "Iteration: 12098; Percent complete: 80.7%; Average loss: 1.1369\n",
            "Iteration: 12099; Percent complete: 80.7%; Average loss: 0.9673\n",
            "Iteration: 12100; Percent complete: 80.7%; Average loss: 0.8558\n",
            "Iteration: 12101; Percent complete: 80.7%; Average loss: 1.0549\n",
            "Iteration: 12102; Percent complete: 80.7%; Average loss: 0.9698\n",
            "Iteration: 12103; Percent complete: 80.7%; Average loss: 1.0987\n",
            "Iteration: 12104; Percent complete: 80.7%; Average loss: 0.9816\n",
            "Iteration: 12105; Percent complete: 80.7%; Average loss: 1.1129\n",
            "Iteration: 12106; Percent complete: 80.7%; Average loss: 1.0731\n",
            "Iteration: 12107; Percent complete: 80.7%; Average loss: 1.0638\n",
            "Iteration: 12108; Percent complete: 80.7%; Average loss: 0.9563\n",
            "Iteration: 12109; Percent complete: 80.7%; Average loss: 1.0443\n",
            "Iteration: 12110; Percent complete: 80.7%; Average loss: 0.9493\n",
            "Iteration: 12111; Percent complete: 80.7%; Average loss: 1.0416\n",
            "Iteration: 12112; Percent complete: 80.7%; Average loss: 1.0148\n",
            "Iteration: 12113; Percent complete: 80.8%; Average loss: 0.9127\n",
            "Iteration: 12114; Percent complete: 80.8%; Average loss: 0.9929\n",
            "Iteration: 12115; Percent complete: 80.8%; Average loss: 1.1152\n",
            "Iteration: 12116; Percent complete: 80.8%; Average loss: 1.1335\n",
            "Iteration: 12117; Percent complete: 80.8%; Average loss: 1.1597\n",
            "Iteration: 12118; Percent complete: 80.8%; Average loss: 0.9354\n",
            "Iteration: 12119; Percent complete: 80.8%; Average loss: 1.1193\n",
            "Iteration: 12120; Percent complete: 80.8%; Average loss: 1.0770\n",
            "Iteration: 12121; Percent complete: 80.8%; Average loss: 0.9206\n",
            "Iteration: 12122; Percent complete: 80.8%; Average loss: 0.8861\n",
            "Iteration: 12123; Percent complete: 80.8%; Average loss: 0.9561\n",
            "Iteration: 12124; Percent complete: 80.8%; Average loss: 1.0309\n",
            "Iteration: 12125; Percent complete: 80.8%; Average loss: 0.9757\n",
            "Iteration: 12126; Percent complete: 80.8%; Average loss: 0.8510\n",
            "Iteration: 12127; Percent complete: 80.8%; Average loss: 1.0183\n",
            "Iteration: 12128; Percent complete: 80.9%; Average loss: 1.1774\n",
            "Iteration: 12129; Percent complete: 80.9%; Average loss: 0.8410\n",
            "Iteration: 12130; Percent complete: 80.9%; Average loss: 1.0517\n",
            "Iteration: 12131; Percent complete: 80.9%; Average loss: 1.0866\n",
            "Iteration: 12132; Percent complete: 80.9%; Average loss: 1.0072\n",
            "Iteration: 12133; Percent complete: 80.9%; Average loss: 1.0297\n",
            "Iteration: 12134; Percent complete: 80.9%; Average loss: 1.1796\n",
            "Iteration: 12135; Percent complete: 80.9%; Average loss: 0.9621\n",
            "Iteration: 12136; Percent complete: 80.9%; Average loss: 1.0706\n",
            "Iteration: 12137; Percent complete: 80.9%; Average loss: 0.8749\n",
            "Iteration: 12138; Percent complete: 80.9%; Average loss: 0.9624\n",
            "Iteration: 12139; Percent complete: 80.9%; Average loss: 0.9908\n",
            "Iteration: 12140; Percent complete: 80.9%; Average loss: 1.0255\n",
            "Iteration: 12141; Percent complete: 80.9%; Average loss: 0.8052\n",
            "Iteration: 12142; Percent complete: 80.9%; Average loss: 0.7730\n",
            "Iteration: 12143; Percent complete: 81.0%; Average loss: 1.0252\n",
            "Iteration: 12144; Percent complete: 81.0%; Average loss: 1.0849\n",
            "Iteration: 12145; Percent complete: 81.0%; Average loss: 0.9703\n",
            "Iteration: 12146; Percent complete: 81.0%; Average loss: 1.2448\n",
            "Iteration: 12147; Percent complete: 81.0%; Average loss: 1.0324\n",
            "Iteration: 12148; Percent complete: 81.0%; Average loss: 0.9546\n",
            "Iteration: 12149; Percent complete: 81.0%; Average loss: 0.9240\n",
            "Iteration: 12150; Percent complete: 81.0%; Average loss: 0.9518\n",
            "Iteration: 12151; Percent complete: 81.0%; Average loss: 1.0868\n",
            "Iteration: 12152; Percent complete: 81.0%; Average loss: 1.0655\n",
            "Iteration: 12153; Percent complete: 81.0%; Average loss: 1.0671\n",
            "Iteration: 12154; Percent complete: 81.0%; Average loss: 0.9108\n",
            "Iteration: 12155; Percent complete: 81.0%; Average loss: 0.9629\n",
            "Iteration: 12156; Percent complete: 81.0%; Average loss: 1.1104\n",
            "Iteration: 12157; Percent complete: 81.0%; Average loss: 1.2261\n",
            "Iteration: 12158; Percent complete: 81.1%; Average loss: 0.9652\n",
            "Iteration: 12159; Percent complete: 81.1%; Average loss: 0.9848\n",
            "Iteration: 12160; Percent complete: 81.1%; Average loss: 1.0630\n",
            "Iteration: 12161; Percent complete: 81.1%; Average loss: 0.9820\n",
            "Iteration: 12162; Percent complete: 81.1%; Average loss: 1.1176\n",
            "Iteration: 12163; Percent complete: 81.1%; Average loss: 1.0155\n",
            "Iteration: 12164; Percent complete: 81.1%; Average loss: 1.1593\n",
            "Iteration: 12165; Percent complete: 81.1%; Average loss: 1.0789\n",
            "Iteration: 12166; Percent complete: 81.1%; Average loss: 1.1785\n",
            "Iteration: 12167; Percent complete: 81.1%; Average loss: 1.0308\n",
            "Iteration: 12168; Percent complete: 81.1%; Average loss: 1.1547\n",
            "Iteration: 12169; Percent complete: 81.1%; Average loss: 0.8625\n",
            "Iteration: 12170; Percent complete: 81.1%; Average loss: 1.0024\n",
            "Iteration: 12171; Percent complete: 81.1%; Average loss: 1.0951\n",
            "Iteration: 12172; Percent complete: 81.1%; Average loss: 0.9387\n",
            "Iteration: 12173; Percent complete: 81.2%; Average loss: 0.9647\n",
            "Iteration: 12174; Percent complete: 81.2%; Average loss: 1.1017\n",
            "Iteration: 12175; Percent complete: 81.2%; Average loss: 1.1158\n",
            "Iteration: 12176; Percent complete: 81.2%; Average loss: 0.8654\n",
            "Iteration: 12177; Percent complete: 81.2%; Average loss: 0.8862\n",
            "Iteration: 12178; Percent complete: 81.2%; Average loss: 0.9480\n",
            "Iteration: 12179; Percent complete: 81.2%; Average loss: 1.1947\n",
            "Iteration: 12180; Percent complete: 81.2%; Average loss: 0.9209\n",
            "Iteration: 12181; Percent complete: 81.2%; Average loss: 1.0377\n",
            "Iteration: 12182; Percent complete: 81.2%; Average loss: 1.1152\n",
            "Iteration: 12183; Percent complete: 81.2%; Average loss: 0.8065\n",
            "Iteration: 12184; Percent complete: 81.2%; Average loss: 0.8401\n",
            "Iteration: 12185; Percent complete: 81.2%; Average loss: 0.9766\n",
            "Iteration: 12186; Percent complete: 81.2%; Average loss: 0.9540\n",
            "Iteration: 12187; Percent complete: 81.2%; Average loss: 0.9952\n",
            "Iteration: 12188; Percent complete: 81.3%; Average loss: 1.0109\n",
            "Iteration: 12189; Percent complete: 81.3%; Average loss: 1.0209\n",
            "Iteration: 12190; Percent complete: 81.3%; Average loss: 1.0178\n",
            "Iteration: 12191; Percent complete: 81.3%; Average loss: 0.9637\n",
            "Iteration: 12192; Percent complete: 81.3%; Average loss: 1.0098\n",
            "Iteration: 12193; Percent complete: 81.3%; Average loss: 0.8401\n",
            "Iteration: 12194; Percent complete: 81.3%; Average loss: 0.9570\n",
            "Iteration: 12195; Percent complete: 81.3%; Average loss: 1.0524\n",
            "Iteration: 12196; Percent complete: 81.3%; Average loss: 0.9975\n",
            "Iteration: 12197; Percent complete: 81.3%; Average loss: 0.9699\n",
            "Iteration: 12198; Percent complete: 81.3%; Average loss: 1.0040\n",
            "Iteration: 12199; Percent complete: 81.3%; Average loss: 0.9705\n",
            "Iteration: 12200; Percent complete: 81.3%; Average loss: 1.1439\n",
            "Iteration: 12201; Percent complete: 81.3%; Average loss: 1.0570\n",
            "Iteration: 12202; Percent complete: 81.3%; Average loss: 1.0679\n",
            "Iteration: 12203; Percent complete: 81.4%; Average loss: 1.0237\n",
            "Iteration: 12204; Percent complete: 81.4%; Average loss: 1.1292\n",
            "Iteration: 12205; Percent complete: 81.4%; Average loss: 1.0412\n",
            "Iteration: 12206; Percent complete: 81.4%; Average loss: 0.9549\n",
            "Iteration: 12207; Percent complete: 81.4%; Average loss: 0.9663\n",
            "Iteration: 12208; Percent complete: 81.4%; Average loss: 1.0408\n",
            "Iteration: 12209; Percent complete: 81.4%; Average loss: 0.9182\n",
            "Iteration: 12210; Percent complete: 81.4%; Average loss: 1.0357\n",
            "Iteration: 12211; Percent complete: 81.4%; Average loss: 1.0300\n",
            "Iteration: 12212; Percent complete: 81.4%; Average loss: 0.9382\n",
            "Iteration: 12213; Percent complete: 81.4%; Average loss: 1.0378\n",
            "Iteration: 12214; Percent complete: 81.4%; Average loss: 1.0674\n",
            "Iteration: 12215; Percent complete: 81.4%; Average loss: 0.9999\n",
            "Iteration: 12216; Percent complete: 81.4%; Average loss: 0.8611\n",
            "Iteration: 12217; Percent complete: 81.4%; Average loss: 0.9664\n",
            "Iteration: 12218; Percent complete: 81.5%; Average loss: 0.9169\n",
            "Iteration: 12219; Percent complete: 81.5%; Average loss: 1.0405\n",
            "Iteration: 12220; Percent complete: 81.5%; Average loss: 1.0417\n",
            "Iteration: 12221; Percent complete: 81.5%; Average loss: 0.9689\n",
            "Iteration: 12222; Percent complete: 81.5%; Average loss: 0.9786\n",
            "Iteration: 12223; Percent complete: 81.5%; Average loss: 1.0591\n",
            "Iteration: 12224; Percent complete: 81.5%; Average loss: 0.9264\n",
            "Iteration: 12225; Percent complete: 81.5%; Average loss: 0.9948\n",
            "Iteration: 12226; Percent complete: 81.5%; Average loss: 0.9977\n",
            "Iteration: 12227; Percent complete: 81.5%; Average loss: 1.0656\n",
            "Iteration: 12228; Percent complete: 81.5%; Average loss: 0.9831\n",
            "Iteration: 12229; Percent complete: 81.5%; Average loss: 0.9827\n",
            "Iteration: 12230; Percent complete: 81.5%; Average loss: 0.9804\n",
            "Iteration: 12231; Percent complete: 81.5%; Average loss: 1.0901\n",
            "Iteration: 12232; Percent complete: 81.5%; Average loss: 1.2654\n",
            "Iteration: 12233; Percent complete: 81.6%; Average loss: 1.0228\n",
            "Iteration: 12234; Percent complete: 81.6%; Average loss: 1.0578\n",
            "Iteration: 12235; Percent complete: 81.6%; Average loss: 1.1273\n",
            "Iteration: 12236; Percent complete: 81.6%; Average loss: 1.1789\n",
            "Iteration: 12237; Percent complete: 81.6%; Average loss: 0.9723\n",
            "Iteration: 12238; Percent complete: 81.6%; Average loss: 0.9847\n",
            "Iteration: 12239; Percent complete: 81.6%; Average loss: 1.0351\n",
            "Iteration: 12240; Percent complete: 81.6%; Average loss: 1.0542\n",
            "Iteration: 12241; Percent complete: 81.6%; Average loss: 1.0900\n",
            "Iteration: 12242; Percent complete: 81.6%; Average loss: 0.9402\n",
            "Iteration: 12243; Percent complete: 81.6%; Average loss: 0.9772\n",
            "Iteration: 12244; Percent complete: 81.6%; Average loss: 1.0716\n",
            "Iteration: 12245; Percent complete: 81.6%; Average loss: 1.0250\n",
            "Iteration: 12246; Percent complete: 81.6%; Average loss: 0.9480\n",
            "Iteration: 12247; Percent complete: 81.6%; Average loss: 1.0281\n",
            "Iteration: 12248; Percent complete: 81.7%; Average loss: 1.1792\n",
            "Iteration: 12249; Percent complete: 81.7%; Average loss: 0.9527\n",
            "Iteration: 12250; Percent complete: 81.7%; Average loss: 0.9945\n",
            "Iteration: 12251; Percent complete: 81.7%; Average loss: 1.0907\n",
            "Iteration: 12252; Percent complete: 81.7%; Average loss: 0.9068\n",
            "Iteration: 12253; Percent complete: 81.7%; Average loss: 0.9088\n",
            "Iteration: 12254; Percent complete: 81.7%; Average loss: 1.0297\n",
            "Iteration: 12255; Percent complete: 81.7%; Average loss: 0.9018\n",
            "Iteration: 12256; Percent complete: 81.7%; Average loss: 0.9804\n",
            "Iteration: 12257; Percent complete: 81.7%; Average loss: 1.1722\n",
            "Iteration: 12258; Percent complete: 81.7%; Average loss: 1.0257\n",
            "Iteration: 12259; Percent complete: 81.7%; Average loss: 0.9201\n",
            "Iteration: 12260; Percent complete: 81.7%; Average loss: 1.0588\n",
            "Iteration: 12261; Percent complete: 81.7%; Average loss: 0.9683\n",
            "Iteration: 12262; Percent complete: 81.7%; Average loss: 0.8381\n",
            "Iteration: 12263; Percent complete: 81.8%; Average loss: 0.9681\n",
            "Iteration: 12264; Percent complete: 81.8%; Average loss: 1.1597\n",
            "Iteration: 12265; Percent complete: 81.8%; Average loss: 1.0591\n",
            "Iteration: 12266; Percent complete: 81.8%; Average loss: 1.0427\n",
            "Iteration: 12267; Percent complete: 81.8%; Average loss: 1.0068\n",
            "Iteration: 12268; Percent complete: 81.8%; Average loss: 1.0786\n",
            "Iteration: 12269; Percent complete: 81.8%; Average loss: 1.0787\n",
            "Iteration: 12270; Percent complete: 81.8%; Average loss: 0.9381\n",
            "Iteration: 12271; Percent complete: 81.8%; Average loss: 1.0372\n",
            "Iteration: 12272; Percent complete: 81.8%; Average loss: 1.1938\n",
            "Iteration: 12273; Percent complete: 81.8%; Average loss: 1.0046\n",
            "Iteration: 12274; Percent complete: 81.8%; Average loss: 1.0747\n",
            "Iteration: 12275; Percent complete: 81.8%; Average loss: 0.9237\n",
            "Iteration: 12276; Percent complete: 81.8%; Average loss: 0.9587\n",
            "Iteration: 12277; Percent complete: 81.8%; Average loss: 0.9599\n",
            "Iteration: 12278; Percent complete: 81.9%; Average loss: 0.9312\n",
            "Iteration: 12279; Percent complete: 81.9%; Average loss: 0.8352\n",
            "Iteration: 12280; Percent complete: 81.9%; Average loss: 0.8473\n",
            "Iteration: 12281; Percent complete: 81.9%; Average loss: 1.0613\n",
            "Iteration: 12282; Percent complete: 81.9%; Average loss: 1.1147\n",
            "Iteration: 12283; Percent complete: 81.9%; Average loss: 0.8172\n",
            "Iteration: 12284; Percent complete: 81.9%; Average loss: 1.1614\n",
            "Iteration: 12285; Percent complete: 81.9%; Average loss: 0.9331\n",
            "Iteration: 12286; Percent complete: 81.9%; Average loss: 1.1146\n",
            "Iteration: 12287; Percent complete: 81.9%; Average loss: 1.1129\n",
            "Iteration: 12288; Percent complete: 81.9%; Average loss: 0.9433\n",
            "Iteration: 12289; Percent complete: 81.9%; Average loss: 1.0073\n",
            "Iteration: 12290; Percent complete: 81.9%; Average loss: 0.9634\n",
            "Iteration: 12291; Percent complete: 81.9%; Average loss: 1.0270\n",
            "Iteration: 12292; Percent complete: 81.9%; Average loss: 1.0419\n",
            "Iteration: 12293; Percent complete: 82.0%; Average loss: 1.0575\n",
            "Iteration: 12294; Percent complete: 82.0%; Average loss: 0.8483\n",
            "Iteration: 12295; Percent complete: 82.0%; Average loss: 0.9687\n",
            "Iteration: 12296; Percent complete: 82.0%; Average loss: 1.0477\n",
            "Iteration: 12297; Percent complete: 82.0%; Average loss: 0.7845\n",
            "Iteration: 12298; Percent complete: 82.0%; Average loss: 1.0185\n",
            "Iteration: 12299; Percent complete: 82.0%; Average loss: 1.0098\n",
            "Iteration: 12300; Percent complete: 82.0%; Average loss: 0.9243\n",
            "Iteration: 12301; Percent complete: 82.0%; Average loss: 1.0726\n",
            "Iteration: 12302; Percent complete: 82.0%; Average loss: 0.9844\n",
            "Iteration: 12303; Percent complete: 82.0%; Average loss: 0.8902\n",
            "Iteration: 12304; Percent complete: 82.0%; Average loss: 0.9256\n",
            "Iteration: 12305; Percent complete: 82.0%; Average loss: 0.9392\n",
            "Iteration: 12306; Percent complete: 82.0%; Average loss: 1.0974\n",
            "Iteration: 12307; Percent complete: 82.0%; Average loss: 0.9367\n",
            "Iteration: 12308; Percent complete: 82.1%; Average loss: 1.0634\n",
            "Iteration: 12309; Percent complete: 82.1%; Average loss: 0.8507\n",
            "Iteration: 12310; Percent complete: 82.1%; Average loss: 1.0050\n",
            "Iteration: 12311; Percent complete: 82.1%; Average loss: 1.1356\n",
            "Iteration: 12312; Percent complete: 82.1%; Average loss: 1.0155\n",
            "Iteration: 12313; Percent complete: 82.1%; Average loss: 1.0025\n",
            "Iteration: 12314; Percent complete: 82.1%; Average loss: 0.9845\n",
            "Iteration: 12315; Percent complete: 82.1%; Average loss: 0.9022\n",
            "Iteration: 12316; Percent complete: 82.1%; Average loss: 0.8792\n",
            "Iteration: 12317; Percent complete: 82.1%; Average loss: 1.0944\n",
            "Iteration: 12318; Percent complete: 82.1%; Average loss: 0.9553\n",
            "Iteration: 12319; Percent complete: 82.1%; Average loss: 1.1437\n",
            "Iteration: 12320; Percent complete: 82.1%; Average loss: 1.0096\n",
            "Iteration: 12321; Percent complete: 82.1%; Average loss: 1.0519\n",
            "Iteration: 12322; Percent complete: 82.1%; Average loss: 0.9643\n",
            "Iteration: 12323; Percent complete: 82.2%; Average loss: 0.8954\n",
            "Iteration: 12324; Percent complete: 82.2%; Average loss: 1.1519\n",
            "Iteration: 12325; Percent complete: 82.2%; Average loss: 0.9734\n",
            "Iteration: 12326; Percent complete: 82.2%; Average loss: 0.9784\n",
            "Iteration: 12327; Percent complete: 82.2%; Average loss: 0.9442\n",
            "Iteration: 12328; Percent complete: 82.2%; Average loss: 0.9269\n",
            "Iteration: 12329; Percent complete: 82.2%; Average loss: 1.0129\n",
            "Iteration: 12330; Percent complete: 82.2%; Average loss: 1.1119\n",
            "Iteration: 12331; Percent complete: 82.2%; Average loss: 0.8883\n",
            "Iteration: 12332; Percent complete: 82.2%; Average loss: 1.0239\n",
            "Iteration: 12333; Percent complete: 82.2%; Average loss: 0.8998\n",
            "Iteration: 12334; Percent complete: 82.2%; Average loss: 0.9322\n",
            "Iteration: 12335; Percent complete: 82.2%; Average loss: 0.9202\n",
            "Iteration: 12336; Percent complete: 82.2%; Average loss: 1.0256\n",
            "Iteration: 12337; Percent complete: 82.2%; Average loss: 0.9970\n",
            "Iteration: 12338; Percent complete: 82.3%; Average loss: 0.9929\n",
            "Iteration: 12339; Percent complete: 82.3%; Average loss: 0.9697\n",
            "Iteration: 12340; Percent complete: 82.3%; Average loss: 0.9509\n",
            "Iteration: 12341; Percent complete: 82.3%; Average loss: 0.9412\n",
            "Iteration: 12342; Percent complete: 82.3%; Average loss: 0.9088\n",
            "Iteration: 12343; Percent complete: 82.3%; Average loss: 0.9102\n",
            "Iteration: 12344; Percent complete: 82.3%; Average loss: 1.0594\n",
            "Iteration: 12345; Percent complete: 82.3%; Average loss: 1.0100\n",
            "Iteration: 12346; Percent complete: 82.3%; Average loss: 0.8788\n",
            "Iteration: 12347; Percent complete: 82.3%; Average loss: 0.9359\n",
            "Iteration: 12348; Percent complete: 82.3%; Average loss: 0.9226\n",
            "Iteration: 12349; Percent complete: 82.3%; Average loss: 1.0168\n",
            "Iteration: 12350; Percent complete: 82.3%; Average loss: 1.0181\n",
            "Iteration: 12351; Percent complete: 82.3%; Average loss: 0.9595\n",
            "Iteration: 12352; Percent complete: 82.3%; Average loss: 1.0141\n",
            "Iteration: 12353; Percent complete: 82.4%; Average loss: 0.8901\n",
            "Iteration: 12354; Percent complete: 82.4%; Average loss: 1.1260\n",
            "Iteration: 12355; Percent complete: 82.4%; Average loss: 1.0324\n",
            "Iteration: 12356; Percent complete: 82.4%; Average loss: 1.0836\n",
            "Iteration: 12357; Percent complete: 82.4%; Average loss: 1.0650\n",
            "Iteration: 12358; Percent complete: 82.4%; Average loss: 0.8592\n",
            "Iteration: 12359; Percent complete: 82.4%; Average loss: 0.9961\n",
            "Iteration: 12360; Percent complete: 82.4%; Average loss: 1.0194\n",
            "Iteration: 12361; Percent complete: 82.4%; Average loss: 0.9705\n",
            "Iteration: 12362; Percent complete: 82.4%; Average loss: 0.9875\n",
            "Iteration: 12363; Percent complete: 82.4%; Average loss: 1.0224\n",
            "Iteration: 12364; Percent complete: 82.4%; Average loss: 0.9058\n",
            "Iteration: 12365; Percent complete: 82.4%; Average loss: 1.0459\n",
            "Iteration: 12366; Percent complete: 82.4%; Average loss: 0.9587\n",
            "Iteration: 12367; Percent complete: 82.4%; Average loss: 1.0012\n",
            "Iteration: 12368; Percent complete: 82.5%; Average loss: 0.9407\n",
            "Iteration: 12369; Percent complete: 82.5%; Average loss: 0.9323\n",
            "Iteration: 12370; Percent complete: 82.5%; Average loss: 0.9450\n",
            "Iteration: 12371; Percent complete: 82.5%; Average loss: 0.9933\n",
            "Iteration: 12372; Percent complete: 82.5%; Average loss: 0.9839\n",
            "Iteration: 12373; Percent complete: 82.5%; Average loss: 0.8822\n",
            "Iteration: 12374; Percent complete: 82.5%; Average loss: 1.0444\n",
            "Iteration: 12375; Percent complete: 82.5%; Average loss: 0.8726\n",
            "Iteration: 12376; Percent complete: 82.5%; Average loss: 0.9868\n",
            "Iteration: 12377; Percent complete: 82.5%; Average loss: 1.0190\n",
            "Iteration: 12378; Percent complete: 82.5%; Average loss: 0.9327\n",
            "Iteration: 12379; Percent complete: 82.5%; Average loss: 1.1133\n",
            "Iteration: 12380; Percent complete: 82.5%; Average loss: 0.9822\n",
            "Iteration: 12381; Percent complete: 82.5%; Average loss: 0.9693\n",
            "Iteration: 12382; Percent complete: 82.5%; Average loss: 1.0895\n",
            "Iteration: 12383; Percent complete: 82.6%; Average loss: 0.9502\n",
            "Iteration: 12384; Percent complete: 82.6%; Average loss: 1.0234\n",
            "Iteration: 12385; Percent complete: 82.6%; Average loss: 0.9438\n",
            "Iteration: 12386; Percent complete: 82.6%; Average loss: 0.7811\n",
            "Iteration: 12387; Percent complete: 82.6%; Average loss: 1.0244\n",
            "Iteration: 12388; Percent complete: 82.6%; Average loss: 0.8160\n",
            "Iteration: 12389; Percent complete: 82.6%; Average loss: 1.1569\n",
            "Iteration: 12390; Percent complete: 82.6%; Average loss: 1.1041\n",
            "Iteration: 12391; Percent complete: 82.6%; Average loss: 0.9378\n",
            "Iteration: 12392; Percent complete: 82.6%; Average loss: 0.8413\n",
            "Iteration: 12393; Percent complete: 82.6%; Average loss: 0.9888\n",
            "Iteration: 12394; Percent complete: 82.6%; Average loss: 1.0841\n",
            "Iteration: 12395; Percent complete: 82.6%; Average loss: 1.0039\n",
            "Iteration: 12396; Percent complete: 82.6%; Average loss: 1.0408\n",
            "Iteration: 12397; Percent complete: 82.6%; Average loss: 0.9114\n",
            "Iteration: 12398; Percent complete: 82.7%; Average loss: 1.0067\n",
            "Iteration: 12399; Percent complete: 82.7%; Average loss: 1.0328\n",
            "Iteration: 12400; Percent complete: 82.7%; Average loss: 1.0838\n",
            "Iteration: 12401; Percent complete: 82.7%; Average loss: 1.0008\n",
            "Iteration: 12402; Percent complete: 82.7%; Average loss: 1.0735\n",
            "Iteration: 12403; Percent complete: 82.7%; Average loss: 0.8682\n",
            "Iteration: 12404; Percent complete: 82.7%; Average loss: 0.8811\n",
            "Iteration: 12405; Percent complete: 82.7%; Average loss: 0.8649\n",
            "Iteration: 12406; Percent complete: 82.7%; Average loss: 1.0522\n",
            "Iteration: 12407; Percent complete: 82.7%; Average loss: 1.0204\n",
            "Iteration: 12408; Percent complete: 82.7%; Average loss: 1.1058\n",
            "Iteration: 12409; Percent complete: 82.7%; Average loss: 1.0102\n",
            "Iteration: 12410; Percent complete: 82.7%; Average loss: 1.1292\n",
            "Iteration: 12411; Percent complete: 82.7%; Average loss: 1.0496\n",
            "Iteration: 12412; Percent complete: 82.7%; Average loss: 0.9414\n",
            "Iteration: 12413; Percent complete: 82.8%; Average loss: 0.8732\n",
            "Iteration: 12414; Percent complete: 82.8%; Average loss: 1.0197\n",
            "Iteration: 12415; Percent complete: 82.8%; Average loss: 0.9182\n",
            "Iteration: 12416; Percent complete: 82.8%; Average loss: 1.0783\n",
            "Iteration: 12417; Percent complete: 82.8%; Average loss: 1.0421\n",
            "Iteration: 12418; Percent complete: 82.8%; Average loss: 1.0081\n",
            "Iteration: 12419; Percent complete: 82.8%; Average loss: 1.1532\n",
            "Iteration: 12420; Percent complete: 82.8%; Average loss: 1.0096\n",
            "Iteration: 12421; Percent complete: 82.8%; Average loss: 0.9035\n",
            "Iteration: 12422; Percent complete: 82.8%; Average loss: 0.8722\n",
            "Iteration: 12423; Percent complete: 82.8%; Average loss: 0.9289\n",
            "Iteration: 12424; Percent complete: 82.8%; Average loss: 1.0362\n",
            "Iteration: 12425; Percent complete: 82.8%; Average loss: 1.0889\n",
            "Iteration: 12426; Percent complete: 82.8%; Average loss: 0.9493\n",
            "Iteration: 12427; Percent complete: 82.8%; Average loss: 0.9310\n",
            "Iteration: 12428; Percent complete: 82.9%; Average loss: 0.9943\n",
            "Iteration: 12429; Percent complete: 82.9%; Average loss: 1.0793\n",
            "Iteration: 12430; Percent complete: 82.9%; Average loss: 1.1535\n",
            "Iteration: 12431; Percent complete: 82.9%; Average loss: 0.9391\n",
            "Iteration: 12432; Percent complete: 82.9%; Average loss: 0.9440\n",
            "Iteration: 12433; Percent complete: 82.9%; Average loss: 0.7705\n",
            "Iteration: 12434; Percent complete: 82.9%; Average loss: 0.9339\n",
            "Iteration: 12435; Percent complete: 82.9%; Average loss: 0.8981\n",
            "Iteration: 12436; Percent complete: 82.9%; Average loss: 1.0102\n",
            "Iteration: 12437; Percent complete: 82.9%; Average loss: 0.9380\n",
            "Iteration: 12438; Percent complete: 82.9%; Average loss: 0.9078\n",
            "Iteration: 12439; Percent complete: 82.9%; Average loss: 0.8628\n",
            "Iteration: 12440; Percent complete: 82.9%; Average loss: 0.8537\n",
            "Iteration: 12441; Percent complete: 82.9%; Average loss: 0.8639\n",
            "Iteration: 12442; Percent complete: 82.9%; Average loss: 0.8781\n",
            "Iteration: 12443; Percent complete: 83.0%; Average loss: 0.9449\n",
            "Iteration: 12444; Percent complete: 83.0%; Average loss: 0.9011\n",
            "Iteration: 12445; Percent complete: 83.0%; Average loss: 1.1552\n",
            "Iteration: 12446; Percent complete: 83.0%; Average loss: 1.0386\n",
            "Iteration: 12447; Percent complete: 83.0%; Average loss: 1.1313\n",
            "Iteration: 12448; Percent complete: 83.0%; Average loss: 0.8541\n",
            "Iteration: 12449; Percent complete: 83.0%; Average loss: 0.8708\n",
            "Iteration: 12450; Percent complete: 83.0%; Average loss: 0.8586\n",
            "Iteration: 12451; Percent complete: 83.0%; Average loss: 1.0592\n",
            "Iteration: 12452; Percent complete: 83.0%; Average loss: 1.0980\n",
            "Iteration: 12453; Percent complete: 83.0%; Average loss: 0.8554\n",
            "Iteration: 12454; Percent complete: 83.0%; Average loss: 0.7986\n",
            "Iteration: 12455; Percent complete: 83.0%; Average loss: 1.0620\n",
            "Iteration: 12456; Percent complete: 83.0%; Average loss: 0.9079\n",
            "Iteration: 12457; Percent complete: 83.0%; Average loss: 0.9271\n",
            "Iteration: 12458; Percent complete: 83.1%; Average loss: 1.0057\n",
            "Iteration: 12459; Percent complete: 83.1%; Average loss: 1.1247\n",
            "Iteration: 12460; Percent complete: 83.1%; Average loss: 1.0198\n",
            "Iteration: 12461; Percent complete: 83.1%; Average loss: 1.0037\n",
            "Iteration: 12462; Percent complete: 83.1%; Average loss: 1.0110\n",
            "Iteration: 12463; Percent complete: 83.1%; Average loss: 0.8929\n",
            "Iteration: 12464; Percent complete: 83.1%; Average loss: 1.0781\n",
            "Iteration: 12465; Percent complete: 83.1%; Average loss: 1.0238\n",
            "Iteration: 12466; Percent complete: 83.1%; Average loss: 1.1039\n",
            "Iteration: 12467; Percent complete: 83.1%; Average loss: 0.9892\n",
            "Iteration: 12468; Percent complete: 83.1%; Average loss: 1.0978\n",
            "Iteration: 12469; Percent complete: 83.1%; Average loss: 1.0125\n",
            "Iteration: 12470; Percent complete: 83.1%; Average loss: 1.0090\n",
            "Iteration: 12471; Percent complete: 83.1%; Average loss: 0.9087\n",
            "Iteration: 12472; Percent complete: 83.1%; Average loss: 0.9672\n",
            "Iteration: 12473; Percent complete: 83.2%; Average loss: 0.9619\n",
            "Iteration: 12474; Percent complete: 83.2%; Average loss: 1.0011\n",
            "Iteration: 12475; Percent complete: 83.2%; Average loss: 0.9228\n",
            "Iteration: 12476; Percent complete: 83.2%; Average loss: 0.9758\n",
            "Iteration: 12477; Percent complete: 83.2%; Average loss: 1.0025\n",
            "Iteration: 12478; Percent complete: 83.2%; Average loss: 0.9300\n",
            "Iteration: 12479; Percent complete: 83.2%; Average loss: 0.9234\n",
            "Iteration: 12480; Percent complete: 83.2%; Average loss: 1.0790\n",
            "Iteration: 12481; Percent complete: 83.2%; Average loss: 0.8718\n",
            "Iteration: 12482; Percent complete: 83.2%; Average loss: 0.8671\n",
            "Iteration: 12483; Percent complete: 83.2%; Average loss: 1.1360\n",
            "Iteration: 12484; Percent complete: 83.2%; Average loss: 0.9885\n",
            "Iteration: 12485; Percent complete: 83.2%; Average loss: 1.0920\n",
            "Iteration: 12486; Percent complete: 83.2%; Average loss: 1.0119\n",
            "Iteration: 12487; Percent complete: 83.2%; Average loss: 1.2228\n",
            "Iteration: 12488; Percent complete: 83.3%; Average loss: 1.0453\n",
            "Iteration: 12489; Percent complete: 83.3%; Average loss: 0.8529\n",
            "Iteration: 12490; Percent complete: 83.3%; Average loss: 1.1057\n",
            "Iteration: 12491; Percent complete: 83.3%; Average loss: 0.9589\n",
            "Iteration: 12492; Percent complete: 83.3%; Average loss: 1.0161\n",
            "Iteration: 12493; Percent complete: 83.3%; Average loss: 0.8375\n",
            "Iteration: 12494; Percent complete: 83.3%; Average loss: 0.9428\n",
            "Iteration: 12495; Percent complete: 83.3%; Average loss: 1.1194\n",
            "Iteration: 12496; Percent complete: 83.3%; Average loss: 1.0269\n",
            "Iteration: 12497; Percent complete: 83.3%; Average loss: 0.9926\n",
            "Iteration: 12498; Percent complete: 83.3%; Average loss: 0.9413\n",
            "Iteration: 12499; Percent complete: 83.3%; Average loss: 1.0543\n",
            "Iteration: 12500; Percent complete: 83.3%; Average loss: 0.8250\n",
            "Iteration: 12501; Percent complete: 83.3%; Average loss: 0.9629\n",
            "Iteration: 12502; Percent complete: 83.3%; Average loss: 1.0452\n",
            "Iteration: 12503; Percent complete: 83.4%; Average loss: 0.8968\n",
            "Iteration: 12504; Percent complete: 83.4%; Average loss: 0.8811\n",
            "Iteration: 12505; Percent complete: 83.4%; Average loss: 0.9212\n",
            "Iteration: 12506; Percent complete: 83.4%; Average loss: 0.9605\n",
            "Iteration: 12507; Percent complete: 83.4%; Average loss: 0.8770\n",
            "Iteration: 12508; Percent complete: 83.4%; Average loss: 0.9904\n",
            "Iteration: 12509; Percent complete: 83.4%; Average loss: 0.9577\n",
            "Iteration: 12510; Percent complete: 83.4%; Average loss: 0.9217\n",
            "Iteration: 12511; Percent complete: 83.4%; Average loss: 0.9873\n",
            "Iteration: 12512; Percent complete: 83.4%; Average loss: 1.1067\n",
            "Iteration: 12513; Percent complete: 83.4%; Average loss: 0.8897\n",
            "Iteration: 12514; Percent complete: 83.4%; Average loss: 1.0118\n",
            "Iteration: 12515; Percent complete: 83.4%; Average loss: 0.9793\n",
            "Iteration: 12516; Percent complete: 83.4%; Average loss: 0.9222\n",
            "Iteration: 12517; Percent complete: 83.4%; Average loss: 0.9704\n",
            "Iteration: 12518; Percent complete: 83.5%; Average loss: 0.9387\n",
            "Iteration: 12519; Percent complete: 83.5%; Average loss: 0.9722\n",
            "Iteration: 12520; Percent complete: 83.5%; Average loss: 0.9315\n",
            "Iteration: 12521; Percent complete: 83.5%; Average loss: 1.0280\n",
            "Iteration: 12522; Percent complete: 83.5%; Average loss: 0.8502\n",
            "Iteration: 12523; Percent complete: 83.5%; Average loss: 0.9158\n",
            "Iteration: 12524; Percent complete: 83.5%; Average loss: 0.9952\n",
            "Iteration: 12525; Percent complete: 83.5%; Average loss: 1.0405\n",
            "Iteration: 12526; Percent complete: 83.5%; Average loss: 0.7593\n",
            "Iteration: 12527; Percent complete: 83.5%; Average loss: 0.7936\n",
            "Iteration: 12528; Percent complete: 83.5%; Average loss: 0.9648\n",
            "Iteration: 12529; Percent complete: 83.5%; Average loss: 1.1217\n",
            "Iteration: 12530; Percent complete: 83.5%; Average loss: 0.9247\n",
            "Iteration: 12531; Percent complete: 83.5%; Average loss: 0.9226\n",
            "Iteration: 12532; Percent complete: 83.5%; Average loss: 0.9170\n",
            "Iteration: 12533; Percent complete: 83.6%; Average loss: 1.0502\n",
            "Iteration: 12534; Percent complete: 83.6%; Average loss: 0.9448\n",
            "Iteration: 12535; Percent complete: 83.6%; Average loss: 0.9304\n",
            "Iteration: 12536; Percent complete: 83.6%; Average loss: 1.1779\n",
            "Iteration: 12537; Percent complete: 83.6%; Average loss: 0.9229\n",
            "Iteration: 12538; Percent complete: 83.6%; Average loss: 1.0366\n",
            "Iteration: 12539; Percent complete: 83.6%; Average loss: 0.8502\n",
            "Iteration: 12540; Percent complete: 83.6%; Average loss: 0.8748\n",
            "Iteration: 12541; Percent complete: 83.6%; Average loss: 0.8778\n",
            "Iteration: 12542; Percent complete: 83.6%; Average loss: 0.8870\n",
            "Iteration: 12543; Percent complete: 83.6%; Average loss: 0.9062\n",
            "Iteration: 12544; Percent complete: 83.6%; Average loss: 0.8717\n",
            "Iteration: 12545; Percent complete: 83.6%; Average loss: 1.0099\n",
            "Iteration: 12546; Percent complete: 83.6%; Average loss: 1.1337\n",
            "Iteration: 12547; Percent complete: 83.6%; Average loss: 1.1284\n",
            "Iteration: 12548; Percent complete: 83.7%; Average loss: 0.8747\n",
            "Iteration: 12549; Percent complete: 83.7%; Average loss: 0.9112\n",
            "Iteration: 12550; Percent complete: 83.7%; Average loss: 1.0213\n",
            "Iteration: 12551; Percent complete: 83.7%; Average loss: 0.9837\n",
            "Iteration: 12552; Percent complete: 83.7%; Average loss: 0.9792\n",
            "Iteration: 12553; Percent complete: 83.7%; Average loss: 0.9415\n",
            "Iteration: 12554; Percent complete: 83.7%; Average loss: 0.9506\n",
            "Iteration: 12555; Percent complete: 83.7%; Average loss: 0.8642\n",
            "Iteration: 12556; Percent complete: 83.7%; Average loss: 0.9478\n",
            "Iteration: 12557; Percent complete: 83.7%; Average loss: 0.9500\n",
            "Iteration: 12558; Percent complete: 83.7%; Average loss: 0.9545\n",
            "Iteration: 12559; Percent complete: 83.7%; Average loss: 0.9566\n",
            "Iteration: 12560; Percent complete: 83.7%; Average loss: 0.9553\n",
            "Iteration: 12561; Percent complete: 83.7%; Average loss: 0.8826\n",
            "Iteration: 12562; Percent complete: 83.7%; Average loss: 0.9178\n",
            "Iteration: 12563; Percent complete: 83.8%; Average loss: 0.9622\n",
            "Iteration: 12564; Percent complete: 83.8%; Average loss: 0.9385\n",
            "Iteration: 12565; Percent complete: 83.8%; Average loss: 1.0335\n",
            "Iteration: 12566; Percent complete: 83.8%; Average loss: 1.0905\n",
            "Iteration: 12567; Percent complete: 83.8%; Average loss: 0.9131\n",
            "Iteration: 12568; Percent complete: 83.8%; Average loss: 0.8258\n",
            "Iteration: 12569; Percent complete: 83.8%; Average loss: 0.9641\n",
            "Iteration: 12570; Percent complete: 83.8%; Average loss: 1.0384\n",
            "Iteration: 12571; Percent complete: 83.8%; Average loss: 0.9886\n",
            "Iteration: 12572; Percent complete: 83.8%; Average loss: 1.0689\n",
            "Iteration: 12573; Percent complete: 83.8%; Average loss: 1.0464\n",
            "Iteration: 12574; Percent complete: 83.8%; Average loss: 0.8855\n",
            "Iteration: 12575; Percent complete: 83.8%; Average loss: 0.9848\n",
            "Iteration: 12576; Percent complete: 83.8%; Average loss: 0.8825\n",
            "Iteration: 12577; Percent complete: 83.8%; Average loss: 1.1273\n",
            "Iteration: 12578; Percent complete: 83.9%; Average loss: 0.8467\n",
            "Iteration: 12579; Percent complete: 83.9%; Average loss: 0.9212\n",
            "Iteration: 12580; Percent complete: 83.9%; Average loss: 0.9092\n",
            "Iteration: 12581; Percent complete: 83.9%; Average loss: 1.0910\n",
            "Iteration: 12582; Percent complete: 83.9%; Average loss: 1.0106\n",
            "Iteration: 12583; Percent complete: 83.9%; Average loss: 0.8685\n",
            "Iteration: 12584; Percent complete: 83.9%; Average loss: 1.1056\n",
            "Iteration: 12585; Percent complete: 83.9%; Average loss: 0.9319\n",
            "Iteration: 12586; Percent complete: 83.9%; Average loss: 1.2829\n",
            "Iteration: 12587; Percent complete: 83.9%; Average loss: 0.9816\n",
            "Iteration: 12588; Percent complete: 83.9%; Average loss: 0.9812\n",
            "Iteration: 12589; Percent complete: 83.9%; Average loss: 1.0113\n",
            "Iteration: 12590; Percent complete: 83.9%; Average loss: 0.8469\n",
            "Iteration: 12591; Percent complete: 83.9%; Average loss: 1.0476\n",
            "Iteration: 12592; Percent complete: 83.9%; Average loss: 1.0203\n",
            "Iteration: 12593; Percent complete: 84.0%; Average loss: 0.8157\n",
            "Iteration: 12594; Percent complete: 84.0%; Average loss: 0.9598\n",
            "Iteration: 12595; Percent complete: 84.0%; Average loss: 1.0284\n",
            "Iteration: 12596; Percent complete: 84.0%; Average loss: 1.1169\n",
            "Iteration: 12597; Percent complete: 84.0%; Average loss: 0.9949\n",
            "Iteration: 12598; Percent complete: 84.0%; Average loss: 1.0288\n",
            "Iteration: 12599; Percent complete: 84.0%; Average loss: 0.9571\n",
            "Iteration: 12600; Percent complete: 84.0%; Average loss: 0.9509\n",
            "Iteration: 12601; Percent complete: 84.0%; Average loss: 0.9691\n",
            "Iteration: 12602; Percent complete: 84.0%; Average loss: 0.9969\n",
            "Iteration: 12603; Percent complete: 84.0%; Average loss: 1.0644\n",
            "Iteration: 12604; Percent complete: 84.0%; Average loss: 0.9969\n",
            "Iteration: 12605; Percent complete: 84.0%; Average loss: 0.8811\n",
            "Iteration: 12606; Percent complete: 84.0%; Average loss: 0.7737\n",
            "Iteration: 12607; Percent complete: 84.0%; Average loss: 0.9874\n",
            "Iteration: 12608; Percent complete: 84.1%; Average loss: 0.8375\n",
            "Iteration: 12609; Percent complete: 84.1%; Average loss: 0.9554\n",
            "Iteration: 12610; Percent complete: 84.1%; Average loss: 0.9528\n",
            "Iteration: 12611; Percent complete: 84.1%; Average loss: 1.0004\n",
            "Iteration: 12612; Percent complete: 84.1%; Average loss: 1.1803\n",
            "Iteration: 12613; Percent complete: 84.1%; Average loss: 0.9497\n",
            "Iteration: 12614; Percent complete: 84.1%; Average loss: 0.8464\n",
            "Iteration: 12615; Percent complete: 84.1%; Average loss: 0.9474\n",
            "Iteration: 12616; Percent complete: 84.1%; Average loss: 0.8549\n",
            "Iteration: 12617; Percent complete: 84.1%; Average loss: 0.8104\n",
            "Iteration: 12618; Percent complete: 84.1%; Average loss: 1.0290\n",
            "Iteration: 12619; Percent complete: 84.1%; Average loss: 0.8713\n",
            "Iteration: 12620; Percent complete: 84.1%; Average loss: 0.9445\n",
            "Iteration: 12621; Percent complete: 84.1%; Average loss: 0.9962\n",
            "Iteration: 12622; Percent complete: 84.1%; Average loss: 0.8875\n",
            "Iteration: 12623; Percent complete: 84.2%; Average loss: 0.8585\n",
            "Iteration: 12624; Percent complete: 84.2%; Average loss: 0.9487\n",
            "Iteration: 12625; Percent complete: 84.2%; Average loss: 1.0213\n",
            "Iteration: 12626; Percent complete: 84.2%; Average loss: 0.9995\n",
            "Iteration: 12627; Percent complete: 84.2%; Average loss: 1.0324\n",
            "Iteration: 12628; Percent complete: 84.2%; Average loss: 0.8962\n",
            "Iteration: 12629; Percent complete: 84.2%; Average loss: 0.9379\n",
            "Iteration: 12630; Percent complete: 84.2%; Average loss: 0.9404\n",
            "Iteration: 12631; Percent complete: 84.2%; Average loss: 0.9787\n",
            "Iteration: 12632; Percent complete: 84.2%; Average loss: 0.9198\n",
            "Iteration: 12633; Percent complete: 84.2%; Average loss: 0.9989\n",
            "Iteration: 12634; Percent complete: 84.2%; Average loss: 0.9422\n",
            "Iteration: 12635; Percent complete: 84.2%; Average loss: 0.9187\n",
            "Iteration: 12636; Percent complete: 84.2%; Average loss: 0.8757\n",
            "Iteration: 12637; Percent complete: 84.2%; Average loss: 0.9366\n",
            "Iteration: 12638; Percent complete: 84.3%; Average loss: 0.8107\n",
            "Iteration: 12639; Percent complete: 84.3%; Average loss: 1.0729\n",
            "Iteration: 12640; Percent complete: 84.3%; Average loss: 1.0534\n",
            "Iteration: 12641; Percent complete: 84.3%; Average loss: 1.0676\n",
            "Iteration: 12642; Percent complete: 84.3%; Average loss: 0.7744\n",
            "Iteration: 12643; Percent complete: 84.3%; Average loss: 0.7475\n",
            "Iteration: 12644; Percent complete: 84.3%; Average loss: 0.8471\n",
            "Iteration: 12645; Percent complete: 84.3%; Average loss: 1.0247\n",
            "Iteration: 12646; Percent complete: 84.3%; Average loss: 1.1148\n",
            "Iteration: 12647; Percent complete: 84.3%; Average loss: 0.8919\n",
            "Iteration: 12648; Percent complete: 84.3%; Average loss: 1.0605\n",
            "Iteration: 12649; Percent complete: 84.3%; Average loss: 0.9061\n",
            "Iteration: 12650; Percent complete: 84.3%; Average loss: 1.0307\n",
            "Iteration: 12651; Percent complete: 84.3%; Average loss: 0.9331\n",
            "Iteration: 12652; Percent complete: 84.3%; Average loss: 1.0066\n",
            "Iteration: 12653; Percent complete: 84.4%; Average loss: 0.8777\n",
            "Iteration: 12654; Percent complete: 84.4%; Average loss: 0.9852\n",
            "Iteration: 12655; Percent complete: 84.4%; Average loss: 0.9103\n",
            "Iteration: 12656; Percent complete: 84.4%; Average loss: 0.9783\n",
            "Iteration: 12657; Percent complete: 84.4%; Average loss: 1.0554\n",
            "Iteration: 12658; Percent complete: 84.4%; Average loss: 1.1620\n",
            "Iteration: 12659; Percent complete: 84.4%; Average loss: 0.9211\n",
            "Iteration: 12660; Percent complete: 84.4%; Average loss: 0.8131\n",
            "Iteration: 12661; Percent complete: 84.4%; Average loss: 0.8529\n",
            "Iteration: 12662; Percent complete: 84.4%; Average loss: 0.9487\n",
            "Iteration: 12663; Percent complete: 84.4%; Average loss: 1.1440\n",
            "Iteration: 12664; Percent complete: 84.4%; Average loss: 1.0301\n",
            "Iteration: 12665; Percent complete: 84.4%; Average loss: 0.8854\n",
            "Iteration: 12666; Percent complete: 84.4%; Average loss: 1.1440\n",
            "Iteration: 12667; Percent complete: 84.4%; Average loss: 1.0471\n",
            "Iteration: 12668; Percent complete: 84.5%; Average loss: 0.9031\n",
            "Iteration: 12669; Percent complete: 84.5%; Average loss: 0.9955\n",
            "Iteration: 12670; Percent complete: 84.5%; Average loss: 1.1442\n",
            "Iteration: 12671; Percent complete: 84.5%; Average loss: 0.8494\n",
            "Iteration: 12672; Percent complete: 84.5%; Average loss: 0.9302\n",
            "Iteration: 12673; Percent complete: 84.5%; Average loss: 0.8998\n",
            "Iteration: 12674; Percent complete: 84.5%; Average loss: 0.8260\n",
            "Iteration: 12675; Percent complete: 84.5%; Average loss: 1.0535\n",
            "Iteration: 12676; Percent complete: 84.5%; Average loss: 0.9819\n",
            "Iteration: 12677; Percent complete: 84.5%; Average loss: 0.8964\n",
            "Iteration: 12678; Percent complete: 84.5%; Average loss: 0.8527\n",
            "Iteration: 12679; Percent complete: 84.5%; Average loss: 0.9844\n",
            "Iteration: 12680; Percent complete: 84.5%; Average loss: 0.9431\n",
            "Iteration: 12681; Percent complete: 84.5%; Average loss: 0.9539\n",
            "Iteration: 12682; Percent complete: 84.5%; Average loss: 1.0889\n",
            "Iteration: 12683; Percent complete: 84.6%; Average loss: 0.9710\n",
            "Iteration: 12684; Percent complete: 84.6%; Average loss: 0.9867\n",
            "Iteration: 12685; Percent complete: 84.6%; Average loss: 0.9020\n",
            "Iteration: 12686; Percent complete: 84.6%; Average loss: 0.9221\n",
            "Iteration: 12687; Percent complete: 84.6%; Average loss: 1.0674\n",
            "Iteration: 12688; Percent complete: 84.6%; Average loss: 0.8543\n",
            "Iteration: 12689; Percent complete: 84.6%; Average loss: 0.9231\n",
            "Iteration: 12690; Percent complete: 84.6%; Average loss: 1.0406\n",
            "Iteration: 12691; Percent complete: 84.6%; Average loss: 0.7462\n",
            "Iteration: 12692; Percent complete: 84.6%; Average loss: 0.8239\n",
            "Iteration: 12693; Percent complete: 84.6%; Average loss: 0.8988\n",
            "Iteration: 12694; Percent complete: 84.6%; Average loss: 0.8681\n",
            "Iteration: 12695; Percent complete: 84.6%; Average loss: 0.9069\n",
            "Iteration: 12696; Percent complete: 84.6%; Average loss: 1.0066\n",
            "Iteration: 12697; Percent complete: 84.6%; Average loss: 0.9066\n",
            "Iteration: 12698; Percent complete: 84.7%; Average loss: 0.9515\n",
            "Iteration: 12699; Percent complete: 84.7%; Average loss: 1.0135\n",
            "Iteration: 12700; Percent complete: 84.7%; Average loss: 0.9020\n",
            "Iteration: 12701; Percent complete: 84.7%; Average loss: 1.1400\n",
            "Iteration: 12702; Percent complete: 84.7%; Average loss: 1.0502\n",
            "Iteration: 12703; Percent complete: 84.7%; Average loss: 1.1002\n",
            "Iteration: 12704; Percent complete: 84.7%; Average loss: 0.8379\n",
            "Iteration: 12705; Percent complete: 84.7%; Average loss: 0.9434\n",
            "Iteration: 12706; Percent complete: 84.7%; Average loss: 1.0074\n",
            "Iteration: 12707; Percent complete: 84.7%; Average loss: 1.0551\n",
            "Iteration: 12708; Percent complete: 84.7%; Average loss: 1.0920\n",
            "Iteration: 12709; Percent complete: 84.7%; Average loss: 0.9525\n",
            "Iteration: 12710; Percent complete: 84.7%; Average loss: 0.9261\n",
            "Iteration: 12711; Percent complete: 84.7%; Average loss: 0.7438\n",
            "Iteration: 12712; Percent complete: 84.7%; Average loss: 0.9603\n",
            "Iteration: 12713; Percent complete: 84.8%; Average loss: 1.1220\n",
            "Iteration: 12714; Percent complete: 84.8%; Average loss: 0.9028\n",
            "Iteration: 12715; Percent complete: 84.8%; Average loss: 0.9379\n",
            "Iteration: 12716; Percent complete: 84.8%; Average loss: 0.8973\n",
            "Iteration: 12717; Percent complete: 84.8%; Average loss: 1.0463\n",
            "Iteration: 12718; Percent complete: 84.8%; Average loss: 0.9009\n",
            "Iteration: 12719; Percent complete: 84.8%; Average loss: 1.0570\n",
            "Iteration: 12720; Percent complete: 84.8%; Average loss: 0.9683\n",
            "Iteration: 12721; Percent complete: 84.8%; Average loss: 0.8715\n",
            "Iteration: 12722; Percent complete: 84.8%; Average loss: 0.8550\n",
            "Iteration: 12723; Percent complete: 84.8%; Average loss: 1.0728\n",
            "Iteration: 12724; Percent complete: 84.8%; Average loss: 0.9109\n",
            "Iteration: 12725; Percent complete: 84.8%; Average loss: 0.9959\n",
            "Iteration: 12726; Percent complete: 84.8%; Average loss: 1.1525\n",
            "Iteration: 12727; Percent complete: 84.8%; Average loss: 0.8685\n",
            "Iteration: 12728; Percent complete: 84.9%; Average loss: 0.9623\n",
            "Iteration: 12729; Percent complete: 84.9%; Average loss: 0.8618\n",
            "Iteration: 12730; Percent complete: 84.9%; Average loss: 0.9131\n",
            "Iteration: 12731; Percent complete: 84.9%; Average loss: 0.7477\n",
            "Iteration: 12732; Percent complete: 84.9%; Average loss: 1.0389\n",
            "Iteration: 12733; Percent complete: 84.9%; Average loss: 0.9593\n",
            "Iteration: 12734; Percent complete: 84.9%; Average loss: 0.9825\n",
            "Iteration: 12735; Percent complete: 84.9%; Average loss: 0.9018\n",
            "Iteration: 12736; Percent complete: 84.9%; Average loss: 1.2030\n",
            "Iteration: 12737; Percent complete: 84.9%; Average loss: 0.8429\n",
            "Iteration: 12738; Percent complete: 84.9%; Average loss: 0.8941\n",
            "Iteration: 12739; Percent complete: 84.9%; Average loss: 1.0232\n",
            "Iteration: 12740; Percent complete: 84.9%; Average loss: 0.9172\n",
            "Iteration: 12741; Percent complete: 84.9%; Average loss: 0.9641\n",
            "Iteration: 12742; Percent complete: 84.9%; Average loss: 0.9171\n",
            "Iteration: 12743; Percent complete: 85.0%; Average loss: 0.9777\n",
            "Iteration: 12744; Percent complete: 85.0%; Average loss: 0.8919\n",
            "Iteration: 12745; Percent complete: 85.0%; Average loss: 0.9129\n",
            "Iteration: 12746; Percent complete: 85.0%; Average loss: 0.8817\n",
            "Iteration: 12747; Percent complete: 85.0%; Average loss: 0.8851\n",
            "Iteration: 12748; Percent complete: 85.0%; Average loss: 0.7277\n",
            "Iteration: 12749; Percent complete: 85.0%; Average loss: 0.8950\n",
            "Iteration: 12750; Percent complete: 85.0%; Average loss: 0.7808\n",
            "Iteration: 12751; Percent complete: 85.0%; Average loss: 0.9727\n",
            "Iteration: 12752; Percent complete: 85.0%; Average loss: 1.0842\n",
            "Iteration: 12753; Percent complete: 85.0%; Average loss: 1.0289\n",
            "Iteration: 12754; Percent complete: 85.0%; Average loss: 1.0546\n",
            "Iteration: 12755; Percent complete: 85.0%; Average loss: 0.8580\n",
            "Iteration: 12756; Percent complete: 85.0%; Average loss: 0.8261\n",
            "Iteration: 12757; Percent complete: 85.0%; Average loss: 1.1581\n",
            "Iteration: 12758; Percent complete: 85.1%; Average loss: 1.0619\n",
            "Iteration: 12759; Percent complete: 85.1%; Average loss: 1.0094\n",
            "Iteration: 12760; Percent complete: 85.1%; Average loss: 1.0350\n",
            "Iteration: 12761; Percent complete: 85.1%; Average loss: 0.8879\n",
            "Iteration: 12762; Percent complete: 85.1%; Average loss: 0.8547\n",
            "Iteration: 12763; Percent complete: 85.1%; Average loss: 0.8839\n",
            "Iteration: 12764; Percent complete: 85.1%; Average loss: 0.9696\n",
            "Iteration: 12765; Percent complete: 85.1%; Average loss: 1.0781\n",
            "Iteration: 12766; Percent complete: 85.1%; Average loss: 0.9534\n",
            "Iteration: 12767; Percent complete: 85.1%; Average loss: 0.9071\n",
            "Iteration: 12768; Percent complete: 85.1%; Average loss: 0.9902\n",
            "Iteration: 12769; Percent complete: 85.1%; Average loss: 1.0334\n",
            "Iteration: 12770; Percent complete: 85.1%; Average loss: 0.8759\n",
            "Iteration: 12771; Percent complete: 85.1%; Average loss: 0.8161\n",
            "Iteration: 12772; Percent complete: 85.1%; Average loss: 0.9926\n",
            "Iteration: 12773; Percent complete: 85.2%; Average loss: 0.9975\n",
            "Iteration: 12774; Percent complete: 85.2%; Average loss: 0.9522\n",
            "Iteration: 12775; Percent complete: 85.2%; Average loss: 0.9091\n",
            "Iteration: 12776; Percent complete: 85.2%; Average loss: 1.0173\n",
            "Iteration: 12777; Percent complete: 85.2%; Average loss: 0.9576\n",
            "Iteration: 12778; Percent complete: 85.2%; Average loss: 0.9722\n",
            "Iteration: 12779; Percent complete: 85.2%; Average loss: 0.8230\n",
            "Iteration: 12780; Percent complete: 85.2%; Average loss: 0.9049\n",
            "Iteration: 12781; Percent complete: 85.2%; Average loss: 1.0596\n",
            "Iteration: 12782; Percent complete: 85.2%; Average loss: 0.7512\n",
            "Iteration: 12783; Percent complete: 85.2%; Average loss: 0.8825\n",
            "Iteration: 12784; Percent complete: 85.2%; Average loss: 0.9951\n",
            "Iteration: 12785; Percent complete: 85.2%; Average loss: 0.9116\n",
            "Iteration: 12786; Percent complete: 85.2%; Average loss: 1.0482\n",
            "Iteration: 12787; Percent complete: 85.2%; Average loss: 0.9716\n",
            "Iteration: 12788; Percent complete: 85.3%; Average loss: 0.9246\n",
            "Iteration: 12789; Percent complete: 85.3%; Average loss: 1.0486\n",
            "Iteration: 12790; Percent complete: 85.3%; Average loss: 1.0615\n",
            "Iteration: 12791; Percent complete: 85.3%; Average loss: 0.8702\n",
            "Iteration: 12792; Percent complete: 85.3%; Average loss: 1.0221\n",
            "Iteration: 12793; Percent complete: 85.3%; Average loss: 0.9700\n",
            "Iteration: 12794; Percent complete: 85.3%; Average loss: 0.8626\n",
            "Iteration: 12795; Percent complete: 85.3%; Average loss: 0.8387\n",
            "Iteration: 12796; Percent complete: 85.3%; Average loss: 0.9166\n",
            "Iteration: 12797; Percent complete: 85.3%; Average loss: 1.0851\n",
            "Iteration: 12798; Percent complete: 85.3%; Average loss: 0.8802\n",
            "Iteration: 12799; Percent complete: 85.3%; Average loss: 1.1642\n",
            "Iteration: 12800; Percent complete: 85.3%; Average loss: 1.1358\n",
            "Iteration: 12801; Percent complete: 85.3%; Average loss: 1.0388\n",
            "Iteration: 12802; Percent complete: 85.3%; Average loss: 1.0492\n",
            "Iteration: 12803; Percent complete: 85.4%; Average loss: 0.8772\n",
            "Iteration: 12804; Percent complete: 85.4%; Average loss: 1.0164\n",
            "Iteration: 12805; Percent complete: 85.4%; Average loss: 0.9119\n",
            "Iteration: 12806; Percent complete: 85.4%; Average loss: 0.9088\n",
            "Iteration: 12807; Percent complete: 85.4%; Average loss: 0.9157\n",
            "Iteration: 12808; Percent complete: 85.4%; Average loss: 1.0558\n",
            "Iteration: 12809; Percent complete: 85.4%; Average loss: 0.9108\n",
            "Iteration: 12810; Percent complete: 85.4%; Average loss: 0.7820\n",
            "Iteration: 12811; Percent complete: 85.4%; Average loss: 1.0281\n",
            "Iteration: 12812; Percent complete: 85.4%; Average loss: 0.8321\n",
            "Iteration: 12813; Percent complete: 85.4%; Average loss: 0.9102\n",
            "Iteration: 12814; Percent complete: 85.4%; Average loss: 1.0462\n",
            "Iteration: 12815; Percent complete: 85.4%; Average loss: 0.9576\n",
            "Iteration: 12816; Percent complete: 85.4%; Average loss: 0.9567\n",
            "Iteration: 12817; Percent complete: 85.4%; Average loss: 0.9887\n",
            "Iteration: 12818; Percent complete: 85.5%; Average loss: 0.9502\n",
            "Iteration: 12819; Percent complete: 85.5%; Average loss: 0.9590\n",
            "Iteration: 12820; Percent complete: 85.5%; Average loss: 0.8935\n",
            "Iteration: 12821; Percent complete: 85.5%; Average loss: 0.9320\n",
            "Iteration: 12822; Percent complete: 85.5%; Average loss: 0.8594\n",
            "Iteration: 12823; Percent complete: 85.5%; Average loss: 0.8999\n",
            "Iteration: 12824; Percent complete: 85.5%; Average loss: 0.9338\n",
            "Iteration: 12825; Percent complete: 85.5%; Average loss: 0.9602\n",
            "Iteration: 12826; Percent complete: 85.5%; Average loss: 0.9900\n",
            "Iteration: 12827; Percent complete: 85.5%; Average loss: 0.9498\n",
            "Iteration: 12828; Percent complete: 85.5%; Average loss: 0.9021\n",
            "Iteration: 12829; Percent complete: 85.5%; Average loss: 0.9198\n",
            "Iteration: 12830; Percent complete: 85.5%; Average loss: 1.0104\n",
            "Iteration: 12831; Percent complete: 85.5%; Average loss: 1.0617\n",
            "Iteration: 12832; Percent complete: 85.5%; Average loss: 1.0060\n",
            "Iteration: 12833; Percent complete: 85.6%; Average loss: 0.9535\n",
            "Iteration: 12834; Percent complete: 85.6%; Average loss: 0.8399\n",
            "Iteration: 12835; Percent complete: 85.6%; Average loss: 0.9516\n",
            "Iteration: 12836; Percent complete: 85.6%; Average loss: 0.7839\n",
            "Iteration: 12837; Percent complete: 85.6%; Average loss: 0.9396\n",
            "Iteration: 12838; Percent complete: 85.6%; Average loss: 1.1683\n",
            "Iteration: 12839; Percent complete: 85.6%; Average loss: 0.9272\n",
            "Iteration: 12840; Percent complete: 85.6%; Average loss: 0.8226\n",
            "Iteration: 12841; Percent complete: 85.6%; Average loss: 0.8790\n",
            "Iteration: 12842; Percent complete: 85.6%; Average loss: 0.8366\n",
            "Iteration: 12843; Percent complete: 85.6%; Average loss: 0.9232\n",
            "Iteration: 12844; Percent complete: 85.6%; Average loss: 0.9713\n",
            "Iteration: 12845; Percent complete: 85.6%; Average loss: 0.9422\n",
            "Iteration: 12846; Percent complete: 85.6%; Average loss: 0.9465\n",
            "Iteration: 12847; Percent complete: 85.6%; Average loss: 0.8757\n",
            "Iteration: 12848; Percent complete: 85.7%; Average loss: 0.8462\n",
            "Iteration: 12849; Percent complete: 85.7%; Average loss: 1.0164\n",
            "Iteration: 12850; Percent complete: 85.7%; Average loss: 0.8876\n",
            "Iteration: 12851; Percent complete: 85.7%; Average loss: 0.9670\n",
            "Iteration: 12852; Percent complete: 85.7%; Average loss: 0.8301\n",
            "Iteration: 12853; Percent complete: 85.7%; Average loss: 0.8078\n",
            "Iteration: 12854; Percent complete: 85.7%; Average loss: 0.9212\n",
            "Iteration: 12855; Percent complete: 85.7%; Average loss: 0.8998\n",
            "Iteration: 12856; Percent complete: 85.7%; Average loss: 0.9918\n",
            "Iteration: 12857; Percent complete: 85.7%; Average loss: 0.8974\n",
            "Iteration: 12858; Percent complete: 85.7%; Average loss: 0.9080\n",
            "Iteration: 12859; Percent complete: 85.7%; Average loss: 0.9519\n",
            "Iteration: 12860; Percent complete: 85.7%; Average loss: 0.9464\n",
            "Iteration: 12861; Percent complete: 85.7%; Average loss: 1.0066\n",
            "Iteration: 12862; Percent complete: 85.7%; Average loss: 1.0317\n",
            "Iteration: 12863; Percent complete: 85.8%; Average loss: 1.0093\n",
            "Iteration: 12864; Percent complete: 85.8%; Average loss: 0.9048\n",
            "Iteration: 12865; Percent complete: 85.8%; Average loss: 0.8085\n",
            "Iteration: 12866; Percent complete: 85.8%; Average loss: 0.9497\n",
            "Iteration: 12867; Percent complete: 85.8%; Average loss: 0.8598\n",
            "Iteration: 12868; Percent complete: 85.8%; Average loss: 0.8801\n",
            "Iteration: 12869; Percent complete: 85.8%; Average loss: 0.8051\n",
            "Iteration: 12870; Percent complete: 85.8%; Average loss: 1.0136\n",
            "Iteration: 12871; Percent complete: 85.8%; Average loss: 0.9277\n",
            "Iteration: 12872; Percent complete: 85.8%; Average loss: 0.8570\n",
            "Iteration: 12873; Percent complete: 85.8%; Average loss: 1.0100\n",
            "Iteration: 12874; Percent complete: 85.8%; Average loss: 0.8555\n",
            "Iteration: 12875; Percent complete: 85.8%; Average loss: 0.8785\n",
            "Iteration: 12876; Percent complete: 85.8%; Average loss: 0.9101\n",
            "Iteration: 12877; Percent complete: 85.8%; Average loss: 0.9498\n",
            "Iteration: 12878; Percent complete: 85.9%; Average loss: 1.0872\n",
            "Iteration: 12879; Percent complete: 85.9%; Average loss: 0.8502\n",
            "Iteration: 12880; Percent complete: 85.9%; Average loss: 0.9732\n",
            "Iteration: 12881; Percent complete: 85.9%; Average loss: 0.8903\n",
            "Iteration: 12882; Percent complete: 85.9%; Average loss: 0.9638\n",
            "Iteration: 12883; Percent complete: 85.9%; Average loss: 0.9460\n",
            "Iteration: 12884; Percent complete: 85.9%; Average loss: 0.8291\n",
            "Iteration: 12885; Percent complete: 85.9%; Average loss: 0.7275\n",
            "Iteration: 12886; Percent complete: 85.9%; Average loss: 0.9469\n",
            "Iteration: 12887; Percent complete: 85.9%; Average loss: 0.9086\n",
            "Iteration: 12888; Percent complete: 85.9%; Average loss: 0.8203\n",
            "Iteration: 12889; Percent complete: 85.9%; Average loss: 0.8368\n",
            "Iteration: 12890; Percent complete: 85.9%; Average loss: 0.9230\n",
            "Iteration: 12891; Percent complete: 85.9%; Average loss: 0.9690\n",
            "Iteration: 12892; Percent complete: 85.9%; Average loss: 0.7165\n",
            "Iteration: 12893; Percent complete: 86.0%; Average loss: 0.8581\n",
            "Iteration: 12894; Percent complete: 86.0%; Average loss: 0.9043\n",
            "Iteration: 12895; Percent complete: 86.0%; Average loss: 0.8178\n",
            "Iteration: 12896; Percent complete: 86.0%; Average loss: 0.9488\n",
            "Iteration: 12897; Percent complete: 86.0%; Average loss: 0.9036\n",
            "Iteration: 12898; Percent complete: 86.0%; Average loss: 0.9308\n",
            "Iteration: 12899; Percent complete: 86.0%; Average loss: 0.7246\n",
            "Iteration: 12900; Percent complete: 86.0%; Average loss: 0.8917\n",
            "Iteration: 12901; Percent complete: 86.0%; Average loss: 1.0602\n",
            "Iteration: 12902; Percent complete: 86.0%; Average loss: 0.8104\n",
            "Iteration: 12903; Percent complete: 86.0%; Average loss: 0.8255\n",
            "Iteration: 12904; Percent complete: 86.0%; Average loss: 0.9446\n",
            "Iteration: 12905; Percent complete: 86.0%; Average loss: 0.7372\n",
            "Iteration: 12906; Percent complete: 86.0%; Average loss: 0.8963\n",
            "Iteration: 12907; Percent complete: 86.0%; Average loss: 1.1271\n",
            "Iteration: 12908; Percent complete: 86.1%; Average loss: 0.8664\n",
            "Iteration: 12909; Percent complete: 86.1%; Average loss: 0.9305\n",
            "Iteration: 12910; Percent complete: 86.1%; Average loss: 1.0075\n",
            "Iteration: 12911; Percent complete: 86.1%; Average loss: 0.7515\n",
            "Iteration: 12912; Percent complete: 86.1%; Average loss: 0.8215\n",
            "Iteration: 12913; Percent complete: 86.1%; Average loss: 0.8977\n",
            "Iteration: 12914; Percent complete: 86.1%; Average loss: 0.8831\n",
            "Iteration: 12915; Percent complete: 86.1%; Average loss: 0.8831\n",
            "Iteration: 12916; Percent complete: 86.1%; Average loss: 1.1422\n",
            "Iteration: 12917; Percent complete: 86.1%; Average loss: 0.8228\n",
            "Iteration: 12918; Percent complete: 86.1%; Average loss: 0.8892\n",
            "Iteration: 12919; Percent complete: 86.1%; Average loss: 0.9291\n",
            "Iteration: 12920; Percent complete: 86.1%; Average loss: 0.9899\n",
            "Iteration: 12921; Percent complete: 86.1%; Average loss: 0.8190\n",
            "Iteration: 12922; Percent complete: 86.1%; Average loss: 0.9676\n",
            "Iteration: 12923; Percent complete: 86.2%; Average loss: 0.7406\n",
            "Iteration: 12924; Percent complete: 86.2%; Average loss: 0.9340\n",
            "Iteration: 12925; Percent complete: 86.2%; Average loss: 1.0134\n",
            "Iteration: 12926; Percent complete: 86.2%; Average loss: 0.8713\n",
            "Iteration: 12927; Percent complete: 86.2%; Average loss: 0.9420\n",
            "Iteration: 12928; Percent complete: 86.2%; Average loss: 1.0400\n",
            "Iteration: 12929; Percent complete: 86.2%; Average loss: 0.9088\n",
            "Iteration: 12930; Percent complete: 86.2%; Average loss: 0.8883\n",
            "Iteration: 12931; Percent complete: 86.2%; Average loss: 0.9769\n",
            "Iteration: 12932; Percent complete: 86.2%; Average loss: 0.8624\n",
            "Iteration: 12933; Percent complete: 86.2%; Average loss: 0.6811\n",
            "Iteration: 12934; Percent complete: 86.2%; Average loss: 0.9964\n",
            "Iteration: 12935; Percent complete: 86.2%; Average loss: 0.9812\n",
            "Iteration: 12936; Percent complete: 86.2%; Average loss: 0.9907\n",
            "Iteration: 12937; Percent complete: 86.2%; Average loss: 0.8314\n",
            "Iteration: 12938; Percent complete: 86.3%; Average loss: 1.0259\n",
            "Iteration: 12939; Percent complete: 86.3%; Average loss: 0.8744\n",
            "Iteration: 12940; Percent complete: 86.3%; Average loss: 0.9280\n",
            "Iteration: 12941; Percent complete: 86.3%; Average loss: 1.1865\n",
            "Iteration: 12942; Percent complete: 86.3%; Average loss: 0.8488\n",
            "Iteration: 12943; Percent complete: 86.3%; Average loss: 1.0580\n",
            "Iteration: 12944; Percent complete: 86.3%; Average loss: 0.9355\n",
            "Iteration: 12945; Percent complete: 86.3%; Average loss: 0.9017\n",
            "Iteration: 12946; Percent complete: 86.3%; Average loss: 0.9293\n",
            "Iteration: 12947; Percent complete: 86.3%; Average loss: 0.8745\n",
            "Iteration: 12948; Percent complete: 86.3%; Average loss: 0.9588\n",
            "Iteration: 12949; Percent complete: 86.3%; Average loss: 1.0876\n",
            "Iteration: 12950; Percent complete: 86.3%; Average loss: 0.9518\n",
            "Iteration: 12951; Percent complete: 86.3%; Average loss: 0.8808\n",
            "Iteration: 12952; Percent complete: 86.3%; Average loss: 0.9712\n",
            "Iteration: 12953; Percent complete: 86.4%; Average loss: 0.8047\n",
            "Iteration: 12954; Percent complete: 86.4%; Average loss: 0.8441\n",
            "Iteration: 12955; Percent complete: 86.4%; Average loss: 0.8172\n",
            "Iteration: 12956; Percent complete: 86.4%; Average loss: 1.0249\n",
            "Iteration: 12957; Percent complete: 86.4%; Average loss: 0.8537\n",
            "Iteration: 12958; Percent complete: 86.4%; Average loss: 0.9093\n",
            "Iteration: 12959; Percent complete: 86.4%; Average loss: 0.8341\n",
            "Iteration: 12960; Percent complete: 86.4%; Average loss: 0.9681\n",
            "Iteration: 12961; Percent complete: 86.4%; Average loss: 0.8644\n",
            "Iteration: 12962; Percent complete: 86.4%; Average loss: 0.8344\n",
            "Iteration: 12963; Percent complete: 86.4%; Average loss: 0.8438\n",
            "Iteration: 12964; Percent complete: 86.4%; Average loss: 0.9741\n",
            "Iteration: 12965; Percent complete: 86.4%; Average loss: 1.0422\n",
            "Iteration: 12966; Percent complete: 86.4%; Average loss: 0.7158\n",
            "Iteration: 12967; Percent complete: 86.4%; Average loss: 0.9927\n",
            "Iteration: 12968; Percent complete: 86.5%; Average loss: 0.8167\n",
            "Iteration: 12969; Percent complete: 86.5%; Average loss: 1.0960\n",
            "Iteration: 12970; Percent complete: 86.5%; Average loss: 0.9023\n",
            "Iteration: 12971; Percent complete: 86.5%; Average loss: 0.9755\n",
            "Iteration: 12972; Percent complete: 86.5%; Average loss: 0.8825\n",
            "Iteration: 12973; Percent complete: 86.5%; Average loss: 0.9371\n",
            "Iteration: 12974; Percent complete: 86.5%; Average loss: 0.7905\n",
            "Iteration: 12975; Percent complete: 86.5%; Average loss: 0.8759\n",
            "Iteration: 12976; Percent complete: 86.5%; Average loss: 0.7875\n",
            "Iteration: 12977; Percent complete: 86.5%; Average loss: 0.9497\n",
            "Iteration: 12978; Percent complete: 86.5%; Average loss: 0.9037\n",
            "Iteration: 12979; Percent complete: 86.5%; Average loss: 0.9114\n",
            "Iteration: 12980; Percent complete: 86.5%; Average loss: 0.8331\n",
            "Iteration: 12981; Percent complete: 86.5%; Average loss: 0.9557\n",
            "Iteration: 12982; Percent complete: 86.5%; Average loss: 0.8244\n",
            "Iteration: 12983; Percent complete: 86.6%; Average loss: 0.9580\n",
            "Iteration: 12984; Percent complete: 86.6%; Average loss: 1.1502\n",
            "Iteration: 12985; Percent complete: 86.6%; Average loss: 0.7517\n",
            "Iteration: 12986; Percent complete: 86.6%; Average loss: 0.9481\n",
            "Iteration: 12987; Percent complete: 86.6%; Average loss: 1.0522\n",
            "Iteration: 12988; Percent complete: 86.6%; Average loss: 0.7379\n",
            "Iteration: 12989; Percent complete: 86.6%; Average loss: 0.8622\n",
            "Iteration: 12990; Percent complete: 86.6%; Average loss: 0.9676\n",
            "Iteration: 12991; Percent complete: 86.6%; Average loss: 1.0826\n",
            "Iteration: 12992; Percent complete: 86.6%; Average loss: 0.9067\n",
            "Iteration: 12993; Percent complete: 86.6%; Average loss: 0.9675\n",
            "Iteration: 12994; Percent complete: 86.6%; Average loss: 0.7713\n",
            "Iteration: 12995; Percent complete: 86.6%; Average loss: 0.7605\n",
            "Iteration: 12996; Percent complete: 86.6%; Average loss: 1.0456\n",
            "Iteration: 12997; Percent complete: 86.6%; Average loss: 1.0436\n",
            "Iteration: 12998; Percent complete: 86.7%; Average loss: 0.9724\n",
            "Iteration: 12999; Percent complete: 86.7%; Average loss: 0.9870\n",
            "Iteration: 13000; Percent complete: 86.7%; Average loss: 0.9761\n",
            "Iteration: 13001; Percent complete: 86.7%; Average loss: 1.0361\n",
            "Iteration: 13002; Percent complete: 86.7%; Average loss: 0.9668\n",
            "Iteration: 13003; Percent complete: 86.7%; Average loss: 0.8729\n",
            "Iteration: 13004; Percent complete: 86.7%; Average loss: 0.8076\n",
            "Iteration: 13005; Percent complete: 86.7%; Average loss: 0.9787\n",
            "Iteration: 13006; Percent complete: 86.7%; Average loss: 1.0711\n",
            "Iteration: 13007; Percent complete: 86.7%; Average loss: 0.7463\n",
            "Iteration: 13008; Percent complete: 86.7%; Average loss: 0.8812\n",
            "Iteration: 13009; Percent complete: 86.7%; Average loss: 0.7582\n",
            "Iteration: 13010; Percent complete: 86.7%; Average loss: 0.9210\n",
            "Iteration: 13011; Percent complete: 86.7%; Average loss: 0.9531\n",
            "Iteration: 13012; Percent complete: 86.7%; Average loss: 0.8598\n",
            "Iteration: 13013; Percent complete: 86.8%; Average loss: 0.7852\n",
            "Iteration: 13014; Percent complete: 86.8%; Average loss: 0.9143\n",
            "Iteration: 13015; Percent complete: 86.8%; Average loss: 1.0599\n",
            "Iteration: 13016; Percent complete: 86.8%; Average loss: 1.0261\n",
            "Iteration: 13017; Percent complete: 86.8%; Average loss: 0.8491\n",
            "Iteration: 13018; Percent complete: 86.8%; Average loss: 1.0067\n",
            "Iteration: 13019; Percent complete: 86.8%; Average loss: 0.9024\n",
            "Iteration: 13020; Percent complete: 86.8%; Average loss: 0.8861\n",
            "Iteration: 13021; Percent complete: 86.8%; Average loss: 0.7338\n",
            "Iteration: 13022; Percent complete: 86.8%; Average loss: 1.0336\n",
            "Iteration: 13023; Percent complete: 86.8%; Average loss: 0.9537\n",
            "Iteration: 13024; Percent complete: 86.8%; Average loss: 0.8693\n",
            "Iteration: 13025; Percent complete: 86.8%; Average loss: 0.8812\n",
            "Iteration: 13026; Percent complete: 86.8%; Average loss: 0.9787\n",
            "Iteration: 13027; Percent complete: 86.8%; Average loss: 0.9501\n",
            "Iteration: 13028; Percent complete: 86.9%; Average loss: 0.8863\n",
            "Iteration: 13029; Percent complete: 86.9%; Average loss: 0.9822\n",
            "Iteration: 13030; Percent complete: 86.9%; Average loss: 0.8715\n",
            "Iteration: 13031; Percent complete: 86.9%; Average loss: 0.9365\n",
            "Iteration: 13032; Percent complete: 86.9%; Average loss: 0.8751\n",
            "Iteration: 13033; Percent complete: 86.9%; Average loss: 0.9463\n",
            "Iteration: 13034; Percent complete: 86.9%; Average loss: 0.8886\n",
            "Iteration: 13035; Percent complete: 86.9%; Average loss: 0.8946\n",
            "Iteration: 13036; Percent complete: 86.9%; Average loss: 0.8466\n",
            "Iteration: 13037; Percent complete: 86.9%; Average loss: 0.8752\n",
            "Iteration: 13038; Percent complete: 86.9%; Average loss: 0.7146\n",
            "Iteration: 13039; Percent complete: 86.9%; Average loss: 0.6763\n",
            "Iteration: 13040; Percent complete: 86.9%; Average loss: 0.9284\n",
            "Iteration: 13041; Percent complete: 86.9%; Average loss: 0.7764\n",
            "Iteration: 13042; Percent complete: 86.9%; Average loss: 0.8638\n",
            "Iteration: 13043; Percent complete: 87.0%; Average loss: 0.8379\n",
            "Iteration: 13044; Percent complete: 87.0%; Average loss: 0.9074\n",
            "Iteration: 13045; Percent complete: 87.0%; Average loss: 0.9684\n",
            "Iteration: 13046; Percent complete: 87.0%; Average loss: 0.8883\n",
            "Iteration: 13047; Percent complete: 87.0%; Average loss: 0.9263\n",
            "Iteration: 13048; Percent complete: 87.0%; Average loss: 1.0195\n",
            "Iteration: 13049; Percent complete: 87.0%; Average loss: 0.8558\n",
            "Iteration: 13050; Percent complete: 87.0%; Average loss: 0.9408\n",
            "Iteration: 13051; Percent complete: 87.0%; Average loss: 0.9326\n",
            "Iteration: 13052; Percent complete: 87.0%; Average loss: 0.7667\n",
            "Iteration: 13053; Percent complete: 87.0%; Average loss: 0.9516\n",
            "Iteration: 13054; Percent complete: 87.0%; Average loss: 0.9131\n",
            "Iteration: 13055; Percent complete: 87.0%; Average loss: 0.9040\n",
            "Iteration: 13056; Percent complete: 87.0%; Average loss: 0.8662\n",
            "Iteration: 13057; Percent complete: 87.0%; Average loss: 0.8454\n",
            "Iteration: 13058; Percent complete: 87.1%; Average loss: 0.9406\n",
            "Iteration: 13059; Percent complete: 87.1%; Average loss: 0.9309\n",
            "Iteration: 13060; Percent complete: 87.1%; Average loss: 0.8031\n",
            "Iteration: 13061; Percent complete: 87.1%; Average loss: 1.0901\n",
            "Iteration: 13062; Percent complete: 87.1%; Average loss: 0.9439\n",
            "Iteration: 13063; Percent complete: 87.1%; Average loss: 0.8812\n",
            "Iteration: 13064; Percent complete: 87.1%; Average loss: 0.8496\n",
            "Iteration: 13065; Percent complete: 87.1%; Average loss: 0.9056\n",
            "Iteration: 13066; Percent complete: 87.1%; Average loss: 0.9612\n",
            "Iteration: 13067; Percent complete: 87.1%; Average loss: 0.8270\n",
            "Iteration: 13068; Percent complete: 87.1%; Average loss: 0.8921\n",
            "Iteration: 13069; Percent complete: 87.1%; Average loss: 0.8266\n",
            "Iteration: 13070; Percent complete: 87.1%; Average loss: 0.8576\n",
            "Iteration: 13071; Percent complete: 87.1%; Average loss: 0.9287\n",
            "Iteration: 13072; Percent complete: 87.1%; Average loss: 0.9535\n",
            "Iteration: 13073; Percent complete: 87.2%; Average loss: 0.9162\n",
            "Iteration: 13074; Percent complete: 87.2%; Average loss: 0.9003\n",
            "Iteration: 13075; Percent complete: 87.2%; Average loss: 0.8440\n",
            "Iteration: 13076; Percent complete: 87.2%; Average loss: 0.9533\n",
            "Iteration: 13077; Percent complete: 87.2%; Average loss: 0.8577\n",
            "Iteration: 13078; Percent complete: 87.2%; Average loss: 0.8513\n",
            "Iteration: 13079; Percent complete: 87.2%; Average loss: 0.9270\n",
            "Iteration: 13080; Percent complete: 87.2%; Average loss: 1.0389\n",
            "Iteration: 13081; Percent complete: 87.2%; Average loss: 0.8769\n",
            "Iteration: 13082; Percent complete: 87.2%; Average loss: 0.8175\n",
            "Iteration: 13083; Percent complete: 87.2%; Average loss: 0.7765\n",
            "Iteration: 13084; Percent complete: 87.2%; Average loss: 0.8302\n",
            "Iteration: 13085; Percent complete: 87.2%; Average loss: 1.0494\n",
            "Iteration: 13086; Percent complete: 87.2%; Average loss: 0.9452\n",
            "Iteration: 13087; Percent complete: 87.2%; Average loss: 1.0999\n",
            "Iteration: 13088; Percent complete: 87.3%; Average loss: 0.9549\n",
            "Iteration: 13089; Percent complete: 87.3%; Average loss: 0.9794\n",
            "Iteration: 13090; Percent complete: 87.3%; Average loss: 0.8217\n",
            "Iteration: 13091; Percent complete: 87.3%; Average loss: 0.8551\n",
            "Iteration: 13092; Percent complete: 87.3%; Average loss: 1.0218\n",
            "Iteration: 13093; Percent complete: 87.3%; Average loss: 1.0944\n",
            "Iteration: 13094; Percent complete: 87.3%; Average loss: 0.7345\n",
            "Iteration: 13095; Percent complete: 87.3%; Average loss: 1.0941\n",
            "Iteration: 13096; Percent complete: 87.3%; Average loss: 0.8514\n",
            "Iteration: 13097; Percent complete: 87.3%; Average loss: 1.0339\n",
            "Iteration: 13098; Percent complete: 87.3%; Average loss: 0.8558\n",
            "Iteration: 13099; Percent complete: 87.3%; Average loss: 0.9002\n",
            "Iteration: 13100; Percent complete: 87.3%; Average loss: 1.0962\n",
            "Iteration: 13101; Percent complete: 87.3%; Average loss: 0.9200\n",
            "Iteration: 13102; Percent complete: 87.3%; Average loss: 0.8668\n",
            "Iteration: 13103; Percent complete: 87.4%; Average loss: 0.9512\n",
            "Iteration: 13104; Percent complete: 87.4%; Average loss: 0.9727\n",
            "Iteration: 13105; Percent complete: 87.4%; Average loss: 0.8654\n",
            "Iteration: 13106; Percent complete: 87.4%; Average loss: 0.9413\n",
            "Iteration: 13107; Percent complete: 87.4%; Average loss: 1.0043\n",
            "Iteration: 13108; Percent complete: 87.4%; Average loss: 0.8685\n",
            "Iteration: 13109; Percent complete: 87.4%; Average loss: 0.8322\n",
            "Iteration: 13110; Percent complete: 87.4%; Average loss: 0.6832\n",
            "Iteration: 13111; Percent complete: 87.4%; Average loss: 0.9594\n",
            "Iteration: 13112; Percent complete: 87.4%; Average loss: 0.8848\n",
            "Iteration: 13113; Percent complete: 87.4%; Average loss: 0.8325\n",
            "Iteration: 13114; Percent complete: 87.4%; Average loss: 0.7599\n",
            "Iteration: 13115; Percent complete: 87.4%; Average loss: 0.9710\n",
            "Iteration: 13116; Percent complete: 87.4%; Average loss: 1.0759\n",
            "Iteration: 13117; Percent complete: 87.4%; Average loss: 1.0449\n",
            "Iteration: 13118; Percent complete: 87.5%; Average loss: 0.7847\n",
            "Iteration: 13119; Percent complete: 87.5%; Average loss: 0.8250\n",
            "Iteration: 13120; Percent complete: 87.5%; Average loss: 0.9416\n",
            "Iteration: 13121; Percent complete: 87.5%; Average loss: 0.9140\n",
            "Iteration: 13122; Percent complete: 87.5%; Average loss: 0.9264\n",
            "Iteration: 13123; Percent complete: 87.5%; Average loss: 0.9036\n",
            "Iteration: 13124; Percent complete: 87.5%; Average loss: 0.9233\n",
            "Iteration: 13125; Percent complete: 87.5%; Average loss: 0.9039\n",
            "Iteration: 13126; Percent complete: 87.5%; Average loss: 0.9898\n",
            "Iteration: 13127; Percent complete: 87.5%; Average loss: 0.9913\n",
            "Iteration: 13128; Percent complete: 87.5%; Average loss: 0.9962\n",
            "Iteration: 13129; Percent complete: 87.5%; Average loss: 0.9082\n",
            "Iteration: 13130; Percent complete: 87.5%; Average loss: 0.8930\n",
            "Iteration: 13131; Percent complete: 87.5%; Average loss: 0.8604\n",
            "Iteration: 13132; Percent complete: 87.5%; Average loss: 0.8578\n",
            "Iteration: 13133; Percent complete: 87.6%; Average loss: 0.7989\n",
            "Iteration: 13134; Percent complete: 87.6%; Average loss: 0.7395\n",
            "Iteration: 13135; Percent complete: 87.6%; Average loss: 0.8705\n",
            "Iteration: 13136; Percent complete: 87.6%; Average loss: 0.8107\n",
            "Iteration: 13137; Percent complete: 87.6%; Average loss: 0.7507\n",
            "Iteration: 13138; Percent complete: 87.6%; Average loss: 0.8838\n",
            "Iteration: 13139; Percent complete: 87.6%; Average loss: 0.8423\n",
            "Iteration: 13140; Percent complete: 87.6%; Average loss: 0.9187\n",
            "Iteration: 13141; Percent complete: 87.6%; Average loss: 0.8984\n",
            "Iteration: 13142; Percent complete: 87.6%; Average loss: 1.0120\n",
            "Iteration: 13143; Percent complete: 87.6%; Average loss: 0.9525\n",
            "Iteration: 13144; Percent complete: 87.6%; Average loss: 1.0481\n",
            "Iteration: 13145; Percent complete: 87.6%; Average loss: 0.9007\n",
            "Iteration: 13146; Percent complete: 87.6%; Average loss: 0.8859\n",
            "Iteration: 13147; Percent complete: 87.6%; Average loss: 0.8977\n",
            "Iteration: 13148; Percent complete: 87.7%; Average loss: 0.8442\n",
            "Iteration: 13149; Percent complete: 87.7%; Average loss: 0.8504\n",
            "Iteration: 13150; Percent complete: 87.7%; Average loss: 0.9194\n",
            "Iteration: 13151; Percent complete: 87.7%; Average loss: 1.0903\n",
            "Iteration: 13152; Percent complete: 87.7%; Average loss: 0.8704\n",
            "Iteration: 13153; Percent complete: 87.7%; Average loss: 0.8868\n",
            "Iteration: 13154; Percent complete: 87.7%; Average loss: 0.8881\n",
            "Iteration: 13155; Percent complete: 87.7%; Average loss: 0.7806\n",
            "Iteration: 13156; Percent complete: 87.7%; Average loss: 1.0274\n",
            "Iteration: 13157; Percent complete: 87.7%; Average loss: 0.8409\n",
            "Iteration: 13158; Percent complete: 87.7%; Average loss: 0.8230\n",
            "Iteration: 13159; Percent complete: 87.7%; Average loss: 0.8105\n",
            "Iteration: 13160; Percent complete: 87.7%; Average loss: 0.8028\n",
            "Iteration: 13161; Percent complete: 87.7%; Average loss: 0.8469\n",
            "Iteration: 13162; Percent complete: 87.7%; Average loss: 0.9325\n",
            "Iteration: 13163; Percent complete: 87.8%; Average loss: 0.9659\n",
            "Iteration: 13164; Percent complete: 87.8%; Average loss: 1.1916\n",
            "Iteration: 13165; Percent complete: 87.8%; Average loss: 0.9675\n",
            "Iteration: 13166; Percent complete: 87.8%; Average loss: 1.0759\n",
            "Iteration: 13167; Percent complete: 87.8%; Average loss: 0.9735\n",
            "Iteration: 13168; Percent complete: 87.8%; Average loss: 0.8210\n",
            "Iteration: 13169; Percent complete: 87.8%; Average loss: 0.8243\n",
            "Iteration: 13170; Percent complete: 87.8%; Average loss: 0.8730\n",
            "Iteration: 13171; Percent complete: 87.8%; Average loss: 1.0619\n",
            "Iteration: 13172; Percent complete: 87.8%; Average loss: 0.8602\n",
            "Iteration: 13173; Percent complete: 87.8%; Average loss: 1.0646\n",
            "Iteration: 13174; Percent complete: 87.8%; Average loss: 0.8403\n",
            "Iteration: 13175; Percent complete: 87.8%; Average loss: 0.9938\n",
            "Iteration: 13176; Percent complete: 87.8%; Average loss: 0.8907\n",
            "Iteration: 13177; Percent complete: 87.8%; Average loss: 0.9780\n",
            "Iteration: 13178; Percent complete: 87.9%; Average loss: 0.8263\n",
            "Iteration: 13179; Percent complete: 87.9%; Average loss: 0.6408\n",
            "Iteration: 13180; Percent complete: 87.9%; Average loss: 0.9471\n",
            "Iteration: 13181; Percent complete: 87.9%; Average loss: 0.7482\n",
            "Iteration: 13182; Percent complete: 87.9%; Average loss: 0.9013\n",
            "Iteration: 13183; Percent complete: 87.9%; Average loss: 0.8232\n",
            "Iteration: 13184; Percent complete: 87.9%; Average loss: 0.8434\n",
            "Iteration: 13185; Percent complete: 87.9%; Average loss: 0.8797\n",
            "Iteration: 13186; Percent complete: 87.9%; Average loss: 0.8163\n",
            "Iteration: 13187; Percent complete: 87.9%; Average loss: 0.7027\n",
            "Iteration: 13188; Percent complete: 87.9%; Average loss: 0.9079\n",
            "Iteration: 13189; Percent complete: 87.9%; Average loss: 0.9551\n",
            "Iteration: 13190; Percent complete: 87.9%; Average loss: 1.1042\n",
            "Iteration: 13191; Percent complete: 87.9%; Average loss: 0.7277\n",
            "Iteration: 13192; Percent complete: 87.9%; Average loss: 1.0790\n",
            "Iteration: 13193; Percent complete: 88.0%; Average loss: 0.8459\n",
            "Iteration: 13194; Percent complete: 88.0%; Average loss: 0.8882\n",
            "Iteration: 13195; Percent complete: 88.0%; Average loss: 0.7570\n",
            "Iteration: 13196; Percent complete: 88.0%; Average loss: 0.8240\n",
            "Iteration: 13197; Percent complete: 88.0%; Average loss: 0.8264\n",
            "Iteration: 13198; Percent complete: 88.0%; Average loss: 0.9081\n",
            "Iteration: 13199; Percent complete: 88.0%; Average loss: 0.8257\n",
            "Iteration: 13200; Percent complete: 88.0%; Average loss: 0.8846\n",
            "Iteration: 13201; Percent complete: 88.0%; Average loss: 0.7214\n",
            "Iteration: 13202; Percent complete: 88.0%; Average loss: 0.7687\n",
            "Iteration: 13203; Percent complete: 88.0%; Average loss: 0.9075\n",
            "Iteration: 13204; Percent complete: 88.0%; Average loss: 0.9145\n",
            "Iteration: 13205; Percent complete: 88.0%; Average loss: 0.9716\n",
            "Iteration: 13206; Percent complete: 88.0%; Average loss: 1.1697\n",
            "Iteration: 13207; Percent complete: 88.0%; Average loss: 0.9108\n",
            "Iteration: 13208; Percent complete: 88.1%; Average loss: 0.9557\n",
            "Iteration: 13209; Percent complete: 88.1%; Average loss: 0.8127\n",
            "Iteration: 13210; Percent complete: 88.1%; Average loss: 0.9540\n",
            "Iteration: 13211; Percent complete: 88.1%; Average loss: 0.9674\n",
            "Iteration: 13212; Percent complete: 88.1%; Average loss: 0.9467\n",
            "Iteration: 13213; Percent complete: 88.1%; Average loss: 1.0821\n",
            "Iteration: 13214; Percent complete: 88.1%; Average loss: 0.7746\n",
            "Iteration: 13215; Percent complete: 88.1%; Average loss: 0.8461\n",
            "Iteration: 13216; Percent complete: 88.1%; Average loss: 0.7532\n",
            "Iteration: 13217; Percent complete: 88.1%; Average loss: 0.8386\n",
            "Iteration: 13218; Percent complete: 88.1%; Average loss: 0.8058\n",
            "Iteration: 13219; Percent complete: 88.1%; Average loss: 0.9118\n",
            "Iteration: 13220; Percent complete: 88.1%; Average loss: 0.8460\n",
            "Iteration: 13221; Percent complete: 88.1%; Average loss: 1.0005\n",
            "Iteration: 13222; Percent complete: 88.1%; Average loss: 0.9540\n",
            "Iteration: 13223; Percent complete: 88.2%; Average loss: 1.0933\n",
            "Iteration: 13224; Percent complete: 88.2%; Average loss: 0.9461\n",
            "Iteration: 13225; Percent complete: 88.2%; Average loss: 0.6867\n",
            "Iteration: 13226; Percent complete: 88.2%; Average loss: 0.8386\n",
            "Iteration: 13227; Percent complete: 88.2%; Average loss: 0.8739\n",
            "Iteration: 13228; Percent complete: 88.2%; Average loss: 0.8106\n",
            "Iteration: 13229; Percent complete: 88.2%; Average loss: 0.9635\n",
            "Iteration: 13230; Percent complete: 88.2%; Average loss: 0.8437\n",
            "Iteration: 13231; Percent complete: 88.2%; Average loss: 0.8952\n",
            "Iteration: 13232; Percent complete: 88.2%; Average loss: 0.8472\n",
            "Iteration: 13233; Percent complete: 88.2%; Average loss: 0.9275\n",
            "Iteration: 13234; Percent complete: 88.2%; Average loss: 0.8823\n",
            "Iteration: 13235; Percent complete: 88.2%; Average loss: 0.9444\n",
            "Iteration: 13236; Percent complete: 88.2%; Average loss: 0.7293\n",
            "Iteration: 13237; Percent complete: 88.2%; Average loss: 0.9380\n",
            "Iteration: 13238; Percent complete: 88.3%; Average loss: 1.1105\n",
            "Iteration: 13239; Percent complete: 88.3%; Average loss: 0.9342\n",
            "Iteration: 13240; Percent complete: 88.3%; Average loss: 0.8149\n",
            "Iteration: 13241; Percent complete: 88.3%; Average loss: 0.8875\n",
            "Iteration: 13242; Percent complete: 88.3%; Average loss: 0.9146\n",
            "Iteration: 13243; Percent complete: 88.3%; Average loss: 0.9704\n",
            "Iteration: 13244; Percent complete: 88.3%; Average loss: 0.8210\n",
            "Iteration: 13245; Percent complete: 88.3%; Average loss: 0.9241\n",
            "Iteration: 13246; Percent complete: 88.3%; Average loss: 0.9629\n",
            "Iteration: 13247; Percent complete: 88.3%; Average loss: 0.8850\n",
            "Iteration: 13248; Percent complete: 88.3%; Average loss: 0.8212\n",
            "Iteration: 13249; Percent complete: 88.3%; Average loss: 0.8692\n",
            "Iteration: 13250; Percent complete: 88.3%; Average loss: 0.8914\n",
            "Iteration: 13251; Percent complete: 88.3%; Average loss: 0.7791\n",
            "Iteration: 13252; Percent complete: 88.3%; Average loss: 0.7524\n",
            "Iteration: 13253; Percent complete: 88.4%; Average loss: 0.9296\n",
            "Iteration: 13254; Percent complete: 88.4%; Average loss: 0.6640\n",
            "Iteration: 13255; Percent complete: 88.4%; Average loss: 0.9687\n",
            "Iteration: 13256; Percent complete: 88.4%; Average loss: 1.0287\n",
            "Iteration: 13257; Percent complete: 88.4%; Average loss: 0.9213\n",
            "Iteration: 13258; Percent complete: 88.4%; Average loss: 0.8595\n",
            "Iteration: 13259; Percent complete: 88.4%; Average loss: 1.0017\n",
            "Iteration: 13260; Percent complete: 88.4%; Average loss: 0.7796\n",
            "Iteration: 13261; Percent complete: 88.4%; Average loss: 0.8343\n",
            "Iteration: 13262; Percent complete: 88.4%; Average loss: 0.5994\n",
            "Iteration: 13263; Percent complete: 88.4%; Average loss: 0.8295\n",
            "Iteration: 13264; Percent complete: 88.4%; Average loss: 0.9061\n",
            "Iteration: 13265; Percent complete: 88.4%; Average loss: 0.9690\n",
            "Iteration: 13266; Percent complete: 88.4%; Average loss: 1.1167\n",
            "Iteration: 13267; Percent complete: 88.4%; Average loss: 0.9544\n",
            "Iteration: 13268; Percent complete: 88.5%; Average loss: 0.9016\n",
            "Iteration: 13269; Percent complete: 88.5%; Average loss: 0.9377\n",
            "Iteration: 13270; Percent complete: 88.5%; Average loss: 0.9707\n",
            "Iteration: 13271; Percent complete: 88.5%; Average loss: 0.7962\n",
            "Iteration: 13272; Percent complete: 88.5%; Average loss: 0.9937\n",
            "Iteration: 13273; Percent complete: 88.5%; Average loss: 0.8972\n",
            "Iteration: 13274; Percent complete: 88.5%; Average loss: 0.8288\n",
            "Iteration: 13275; Percent complete: 88.5%; Average loss: 0.9054\n",
            "Iteration: 13276; Percent complete: 88.5%; Average loss: 0.8969\n",
            "Iteration: 13277; Percent complete: 88.5%; Average loss: 0.6586\n",
            "Iteration: 13278; Percent complete: 88.5%; Average loss: 0.9634\n",
            "Iteration: 13279; Percent complete: 88.5%; Average loss: 0.8621\n",
            "Iteration: 13280; Percent complete: 88.5%; Average loss: 0.7190\n",
            "Iteration: 13281; Percent complete: 88.5%; Average loss: 0.9138\n",
            "Iteration: 13282; Percent complete: 88.5%; Average loss: 0.7090\n",
            "Iteration: 13283; Percent complete: 88.6%; Average loss: 0.8446\n",
            "Iteration: 13284; Percent complete: 88.6%; Average loss: 0.7672\n",
            "Iteration: 13285; Percent complete: 88.6%; Average loss: 0.9162\n",
            "Iteration: 13286; Percent complete: 88.6%; Average loss: 0.7565\n",
            "Iteration: 13287; Percent complete: 88.6%; Average loss: 0.6799\n",
            "Iteration: 13288; Percent complete: 88.6%; Average loss: 0.9119\n",
            "Iteration: 13289; Percent complete: 88.6%; Average loss: 0.8731\n",
            "Iteration: 13290; Percent complete: 88.6%; Average loss: 0.9676\n",
            "Iteration: 13291; Percent complete: 88.6%; Average loss: 0.7268\n",
            "Iteration: 13292; Percent complete: 88.6%; Average loss: 0.8709\n",
            "Iteration: 13293; Percent complete: 88.6%; Average loss: 0.9002\n",
            "Iteration: 13294; Percent complete: 88.6%; Average loss: 0.8292\n",
            "Iteration: 13295; Percent complete: 88.6%; Average loss: 0.9284\n",
            "Iteration: 13296; Percent complete: 88.6%; Average loss: 0.8657\n",
            "Iteration: 13297; Percent complete: 88.6%; Average loss: 0.7024\n",
            "Iteration: 13298; Percent complete: 88.7%; Average loss: 1.0095\n",
            "Iteration: 13299; Percent complete: 88.7%; Average loss: 0.9440\n",
            "Iteration: 13300; Percent complete: 88.7%; Average loss: 0.8667\n",
            "Iteration: 13301; Percent complete: 88.7%; Average loss: 1.0485\n",
            "Iteration: 13302; Percent complete: 88.7%; Average loss: 0.7892\n",
            "Iteration: 13303; Percent complete: 88.7%; Average loss: 0.7367\n",
            "Iteration: 13304; Percent complete: 88.7%; Average loss: 0.8773\n",
            "Iteration: 13305; Percent complete: 88.7%; Average loss: 0.7443\n",
            "Iteration: 13306; Percent complete: 88.7%; Average loss: 0.8306\n",
            "Iteration: 13307; Percent complete: 88.7%; Average loss: 0.7697\n",
            "Iteration: 13308; Percent complete: 88.7%; Average loss: 0.9849\n",
            "Iteration: 13309; Percent complete: 88.7%; Average loss: 0.8218\n",
            "Iteration: 13310; Percent complete: 88.7%; Average loss: 0.8564\n",
            "Iteration: 13311; Percent complete: 88.7%; Average loss: 0.8104\n",
            "Iteration: 13312; Percent complete: 88.7%; Average loss: 0.9858\n",
            "Iteration: 13313; Percent complete: 88.8%; Average loss: 0.8721\n",
            "Iteration: 13314; Percent complete: 88.8%; Average loss: 0.8827\n",
            "Iteration: 13315; Percent complete: 88.8%; Average loss: 0.8612\n",
            "Iteration: 13316; Percent complete: 88.8%; Average loss: 0.8150\n",
            "Iteration: 13317; Percent complete: 88.8%; Average loss: 0.8633\n",
            "Iteration: 13318; Percent complete: 88.8%; Average loss: 0.9070\n",
            "Iteration: 13319; Percent complete: 88.8%; Average loss: 0.8939\n",
            "Iteration: 13320; Percent complete: 88.8%; Average loss: 1.0767\n",
            "Iteration: 13321; Percent complete: 88.8%; Average loss: 0.6684\n",
            "Iteration: 13322; Percent complete: 88.8%; Average loss: 0.8560\n",
            "Iteration: 13323; Percent complete: 88.8%; Average loss: 0.8901\n",
            "Iteration: 13324; Percent complete: 88.8%; Average loss: 0.9694\n",
            "Iteration: 13325; Percent complete: 88.8%; Average loss: 0.8720\n",
            "Iteration: 13326; Percent complete: 88.8%; Average loss: 0.8560\n",
            "Iteration: 13327; Percent complete: 88.8%; Average loss: 0.7295\n",
            "Iteration: 13328; Percent complete: 88.9%; Average loss: 0.7429\n",
            "Iteration: 13329; Percent complete: 88.9%; Average loss: 0.9360\n",
            "Iteration: 13330; Percent complete: 88.9%; Average loss: 0.7315\n",
            "Iteration: 13331; Percent complete: 88.9%; Average loss: 0.7460\n",
            "Iteration: 13332; Percent complete: 88.9%; Average loss: 1.0531\n",
            "Iteration: 13333; Percent complete: 88.9%; Average loss: 0.7867\n",
            "Iteration: 13334; Percent complete: 88.9%; Average loss: 1.0788\n",
            "Iteration: 13335; Percent complete: 88.9%; Average loss: 1.0727\n",
            "Iteration: 13336; Percent complete: 88.9%; Average loss: 0.9627\n",
            "Iteration: 13337; Percent complete: 88.9%; Average loss: 0.8930\n",
            "Iteration: 13338; Percent complete: 88.9%; Average loss: 0.8345\n",
            "Iteration: 13339; Percent complete: 88.9%; Average loss: 0.9345\n",
            "Iteration: 13340; Percent complete: 88.9%; Average loss: 0.8941\n",
            "Iteration: 13341; Percent complete: 88.9%; Average loss: 0.7662\n",
            "Iteration: 13342; Percent complete: 88.9%; Average loss: 0.9165\n",
            "Iteration: 13343; Percent complete: 89.0%; Average loss: 0.7352\n",
            "Iteration: 13344; Percent complete: 89.0%; Average loss: 0.7775\n",
            "Iteration: 13345; Percent complete: 89.0%; Average loss: 0.7491\n",
            "Iteration: 13346; Percent complete: 89.0%; Average loss: 0.8970\n",
            "Iteration: 13347; Percent complete: 89.0%; Average loss: 0.8336\n",
            "Iteration: 13348; Percent complete: 89.0%; Average loss: 0.8434\n",
            "Iteration: 13349; Percent complete: 89.0%; Average loss: 0.7545\n",
            "Iteration: 13350; Percent complete: 89.0%; Average loss: 0.8095\n",
            "Iteration: 13351; Percent complete: 89.0%; Average loss: 0.9921\n",
            "Iteration: 13352; Percent complete: 89.0%; Average loss: 0.8637\n",
            "Iteration: 13353; Percent complete: 89.0%; Average loss: 0.7958\n",
            "Iteration: 13354; Percent complete: 89.0%; Average loss: 0.8360\n",
            "Iteration: 13355; Percent complete: 89.0%; Average loss: 0.8604\n",
            "Iteration: 13356; Percent complete: 89.0%; Average loss: 0.7293\n",
            "Iteration: 13357; Percent complete: 89.0%; Average loss: 0.9724\n",
            "Iteration: 13358; Percent complete: 89.1%; Average loss: 0.8910\n",
            "Iteration: 13359; Percent complete: 89.1%; Average loss: 0.9465\n",
            "Iteration: 13360; Percent complete: 89.1%; Average loss: 0.8471\n",
            "Iteration: 13361; Percent complete: 89.1%; Average loss: 0.8605\n",
            "Iteration: 13362; Percent complete: 89.1%; Average loss: 0.7440\n",
            "Iteration: 13363; Percent complete: 89.1%; Average loss: 0.9035\n",
            "Iteration: 13364; Percent complete: 89.1%; Average loss: 0.9402\n",
            "Iteration: 13365; Percent complete: 89.1%; Average loss: 0.7284\n",
            "Iteration: 13366; Percent complete: 89.1%; Average loss: 0.7312\n",
            "Iteration: 13367; Percent complete: 89.1%; Average loss: 0.7944\n",
            "Iteration: 13368; Percent complete: 89.1%; Average loss: 1.0152\n",
            "Iteration: 13369; Percent complete: 89.1%; Average loss: 0.8957\n",
            "Iteration: 13370; Percent complete: 89.1%; Average loss: 0.8724\n",
            "Iteration: 13371; Percent complete: 89.1%; Average loss: 0.8282\n",
            "Iteration: 13372; Percent complete: 89.1%; Average loss: 0.8139\n",
            "Iteration: 13373; Percent complete: 89.2%; Average loss: 0.9206\n",
            "Iteration: 13374; Percent complete: 89.2%; Average loss: 0.8646\n",
            "Iteration: 13375; Percent complete: 89.2%; Average loss: 0.8477\n",
            "Iteration: 13376; Percent complete: 89.2%; Average loss: 0.8995\n",
            "Iteration: 13377; Percent complete: 89.2%; Average loss: 0.8992\n",
            "Iteration: 13378; Percent complete: 89.2%; Average loss: 0.7663\n",
            "Iteration: 13379; Percent complete: 89.2%; Average loss: 0.9348\n",
            "Iteration: 13380; Percent complete: 89.2%; Average loss: 0.8786\n",
            "Iteration: 13381; Percent complete: 89.2%; Average loss: 0.9064\n",
            "Iteration: 13382; Percent complete: 89.2%; Average loss: 0.9209\n",
            "Iteration: 13383; Percent complete: 89.2%; Average loss: 1.0921\n",
            "Iteration: 13384; Percent complete: 89.2%; Average loss: 0.9022\n",
            "Iteration: 13385; Percent complete: 89.2%; Average loss: 0.8399\n",
            "Iteration: 13386; Percent complete: 89.2%; Average loss: 0.9390\n",
            "Iteration: 13387; Percent complete: 89.2%; Average loss: 0.8355\n",
            "Iteration: 13388; Percent complete: 89.3%; Average loss: 0.9284\n",
            "Iteration: 13389; Percent complete: 89.3%; Average loss: 0.8745\n",
            "Iteration: 13390; Percent complete: 89.3%; Average loss: 0.9076\n",
            "Iteration: 13391; Percent complete: 89.3%; Average loss: 0.7792\n",
            "Iteration: 13392; Percent complete: 89.3%; Average loss: 1.0012\n",
            "Iteration: 13393; Percent complete: 89.3%; Average loss: 0.8214\n",
            "Iteration: 13394; Percent complete: 89.3%; Average loss: 0.8347\n",
            "Iteration: 13395; Percent complete: 89.3%; Average loss: 0.8691\n",
            "Iteration: 13396; Percent complete: 89.3%; Average loss: 0.7718\n",
            "Iteration: 13397; Percent complete: 89.3%; Average loss: 0.7844\n",
            "Iteration: 13398; Percent complete: 89.3%; Average loss: 0.7076\n",
            "Iteration: 13399; Percent complete: 89.3%; Average loss: 0.8829\n",
            "Iteration: 13400; Percent complete: 89.3%; Average loss: 0.9214\n",
            "Iteration: 13401; Percent complete: 89.3%; Average loss: 0.9101\n",
            "Iteration: 13402; Percent complete: 89.3%; Average loss: 1.0705\n",
            "Iteration: 13403; Percent complete: 89.4%; Average loss: 0.7626\n",
            "Iteration: 13404; Percent complete: 89.4%; Average loss: 0.8473\n",
            "Iteration: 13405; Percent complete: 89.4%; Average loss: 0.7332\n",
            "Iteration: 13406; Percent complete: 89.4%; Average loss: 0.9071\n",
            "Iteration: 13407; Percent complete: 89.4%; Average loss: 0.9542\n",
            "Iteration: 13408; Percent complete: 89.4%; Average loss: 0.8189\n",
            "Iteration: 13409; Percent complete: 89.4%; Average loss: 0.7862\n",
            "Iteration: 13410; Percent complete: 89.4%; Average loss: 0.9798\n",
            "Iteration: 13411; Percent complete: 89.4%; Average loss: 0.8915\n",
            "Iteration: 13412; Percent complete: 89.4%; Average loss: 0.8305\n",
            "Iteration: 13413; Percent complete: 89.4%; Average loss: 0.9293\n",
            "Iteration: 13414; Percent complete: 89.4%; Average loss: 0.7209\n",
            "Iteration: 13415; Percent complete: 89.4%; Average loss: 0.8722\n",
            "Iteration: 13416; Percent complete: 89.4%; Average loss: 0.9872\n",
            "Iteration: 13417; Percent complete: 89.4%; Average loss: 0.8371\n",
            "Iteration: 13418; Percent complete: 89.5%; Average loss: 0.9835\n",
            "Iteration: 13419; Percent complete: 89.5%; Average loss: 0.9549\n",
            "Iteration: 13420; Percent complete: 89.5%; Average loss: 0.9050\n",
            "Iteration: 13421; Percent complete: 89.5%; Average loss: 0.7077\n",
            "Iteration: 13422; Percent complete: 89.5%; Average loss: 0.8050\n",
            "Iteration: 13423; Percent complete: 89.5%; Average loss: 0.9899\n",
            "Iteration: 13424; Percent complete: 89.5%; Average loss: 0.8477\n",
            "Iteration: 13425; Percent complete: 89.5%; Average loss: 0.8559\n",
            "Iteration: 13426; Percent complete: 89.5%; Average loss: 0.8133\n",
            "Iteration: 13427; Percent complete: 89.5%; Average loss: 0.6992\n",
            "Iteration: 13428; Percent complete: 89.5%; Average loss: 0.7808\n",
            "Iteration: 13429; Percent complete: 89.5%; Average loss: 0.9242\n",
            "Iteration: 13430; Percent complete: 89.5%; Average loss: 0.7948\n",
            "Iteration: 13431; Percent complete: 89.5%; Average loss: 0.9181\n",
            "Iteration: 13432; Percent complete: 89.5%; Average loss: 0.8758\n",
            "Iteration: 13433; Percent complete: 89.6%; Average loss: 0.9382\n",
            "Iteration: 13434; Percent complete: 89.6%; Average loss: 0.8016\n",
            "Iteration: 13435; Percent complete: 89.6%; Average loss: 0.9194\n",
            "Iteration: 13436; Percent complete: 89.6%; Average loss: 0.7753\n",
            "Iteration: 13437; Percent complete: 89.6%; Average loss: 0.8114\n",
            "Iteration: 13438; Percent complete: 89.6%; Average loss: 0.7820\n",
            "Iteration: 13439; Percent complete: 89.6%; Average loss: 0.8286\n",
            "Iteration: 13440; Percent complete: 89.6%; Average loss: 0.7895\n",
            "Iteration: 13441; Percent complete: 89.6%; Average loss: 0.9100\n",
            "Iteration: 13442; Percent complete: 89.6%; Average loss: 0.9010\n",
            "Iteration: 13443; Percent complete: 89.6%; Average loss: 1.0008\n",
            "Iteration: 13444; Percent complete: 89.6%; Average loss: 1.0052\n",
            "Iteration: 13445; Percent complete: 89.6%; Average loss: 1.0220\n",
            "Iteration: 13446; Percent complete: 89.6%; Average loss: 0.8493\n",
            "Iteration: 13447; Percent complete: 89.6%; Average loss: 0.9650\n",
            "Iteration: 13448; Percent complete: 89.7%; Average loss: 0.9125\n",
            "Iteration: 13449; Percent complete: 89.7%; Average loss: 0.9396\n",
            "Iteration: 13450; Percent complete: 89.7%; Average loss: 0.8872\n",
            "Iteration: 13451; Percent complete: 89.7%; Average loss: 0.6819\n",
            "Iteration: 13452; Percent complete: 89.7%; Average loss: 0.9505\n",
            "Iteration: 13453; Percent complete: 89.7%; Average loss: 0.8025\n",
            "Iteration: 13454; Percent complete: 89.7%; Average loss: 0.9337\n",
            "Iteration: 13455; Percent complete: 89.7%; Average loss: 0.9468\n",
            "Iteration: 13456; Percent complete: 89.7%; Average loss: 0.8498\n",
            "Iteration: 13457; Percent complete: 89.7%; Average loss: 0.9136\n",
            "Iteration: 13458; Percent complete: 89.7%; Average loss: 1.0057\n",
            "Iteration: 13459; Percent complete: 89.7%; Average loss: 0.8403\n",
            "Iteration: 13460; Percent complete: 89.7%; Average loss: 0.8903\n",
            "Iteration: 13461; Percent complete: 89.7%; Average loss: 0.8351\n",
            "Iteration: 13462; Percent complete: 89.7%; Average loss: 0.8629\n",
            "Iteration: 13463; Percent complete: 89.8%; Average loss: 0.8753\n",
            "Iteration: 13464; Percent complete: 89.8%; Average loss: 0.8484\n",
            "Iteration: 13465; Percent complete: 89.8%; Average loss: 0.8566\n",
            "Iteration: 13466; Percent complete: 89.8%; Average loss: 0.8586\n",
            "Iteration: 13467; Percent complete: 89.8%; Average loss: 0.8419\n",
            "Iteration: 13468; Percent complete: 89.8%; Average loss: 0.9603\n",
            "Iteration: 13469; Percent complete: 89.8%; Average loss: 0.8777\n",
            "Iteration: 13470; Percent complete: 89.8%; Average loss: 0.7757\n",
            "Iteration: 13471; Percent complete: 89.8%; Average loss: 0.8691\n",
            "Iteration: 13472; Percent complete: 89.8%; Average loss: 0.7966\n",
            "Iteration: 13473; Percent complete: 89.8%; Average loss: 0.8115\n",
            "Iteration: 13474; Percent complete: 89.8%; Average loss: 0.8678\n",
            "Iteration: 13475; Percent complete: 89.8%; Average loss: 0.8948\n",
            "Iteration: 13476; Percent complete: 89.8%; Average loss: 0.9000\n",
            "Iteration: 13477; Percent complete: 89.8%; Average loss: 0.9145\n",
            "Iteration: 13478; Percent complete: 89.9%; Average loss: 0.9008\n",
            "Iteration: 13479; Percent complete: 89.9%; Average loss: 0.8496\n",
            "Iteration: 13480; Percent complete: 89.9%; Average loss: 0.8891\n",
            "Iteration: 13481; Percent complete: 89.9%; Average loss: 0.8368\n",
            "Iteration: 13482; Percent complete: 89.9%; Average loss: 0.7598\n",
            "Iteration: 13483; Percent complete: 89.9%; Average loss: 0.7873\n",
            "Iteration: 13484; Percent complete: 89.9%; Average loss: 0.7677\n",
            "Iteration: 13485; Percent complete: 89.9%; Average loss: 0.9446\n",
            "Iteration: 13486; Percent complete: 89.9%; Average loss: 0.7305\n",
            "Iteration: 13487; Percent complete: 89.9%; Average loss: 0.9191\n",
            "Iteration: 13488; Percent complete: 89.9%; Average loss: 0.9593\n",
            "Iteration: 13489; Percent complete: 89.9%; Average loss: 0.9411\n",
            "Iteration: 13490; Percent complete: 89.9%; Average loss: 0.8190\n",
            "Iteration: 13491; Percent complete: 89.9%; Average loss: 0.9200\n",
            "Iteration: 13492; Percent complete: 89.9%; Average loss: 0.8928\n",
            "Iteration: 13493; Percent complete: 90.0%; Average loss: 1.0281\n",
            "Iteration: 13494; Percent complete: 90.0%; Average loss: 0.8507\n",
            "Iteration: 13495; Percent complete: 90.0%; Average loss: 0.8115\n",
            "Iteration: 13496; Percent complete: 90.0%; Average loss: 0.7455\n",
            "Iteration: 13497; Percent complete: 90.0%; Average loss: 0.8796\n",
            "Iteration: 13498; Percent complete: 90.0%; Average loss: 0.7324\n",
            "Iteration: 13499; Percent complete: 90.0%; Average loss: 0.8377\n",
            "Iteration: 13500; Percent complete: 90.0%; Average loss: 1.0103\n",
            "Iteration: 13501; Percent complete: 90.0%; Average loss: 0.8905\n",
            "Iteration: 13502; Percent complete: 90.0%; Average loss: 0.8035\n",
            "Iteration: 13503; Percent complete: 90.0%; Average loss: 0.7791\n",
            "Iteration: 13504; Percent complete: 90.0%; Average loss: 0.9183\n",
            "Iteration: 13505; Percent complete: 90.0%; Average loss: 0.8250\n",
            "Iteration: 13506; Percent complete: 90.0%; Average loss: 0.8234\n",
            "Iteration: 13507; Percent complete: 90.0%; Average loss: 0.8918\n",
            "Iteration: 13508; Percent complete: 90.1%; Average loss: 0.9165\n",
            "Iteration: 13509; Percent complete: 90.1%; Average loss: 0.9965\n",
            "Iteration: 13510; Percent complete: 90.1%; Average loss: 0.9334\n",
            "Iteration: 13511; Percent complete: 90.1%; Average loss: 0.9210\n",
            "Iteration: 13512; Percent complete: 90.1%; Average loss: 0.8172\n",
            "Iteration: 13513; Percent complete: 90.1%; Average loss: 0.8868\n",
            "Iteration: 13514; Percent complete: 90.1%; Average loss: 0.7984\n",
            "Iteration: 13515; Percent complete: 90.1%; Average loss: 0.8999\n",
            "Iteration: 13516; Percent complete: 90.1%; Average loss: 0.8617\n",
            "Iteration: 13517; Percent complete: 90.1%; Average loss: 0.8592\n",
            "Iteration: 13518; Percent complete: 90.1%; Average loss: 0.7385\n",
            "Iteration: 13519; Percent complete: 90.1%; Average loss: 0.8883\n",
            "Iteration: 13520; Percent complete: 90.1%; Average loss: 0.8319\n",
            "Iteration: 13521; Percent complete: 90.1%; Average loss: 0.8422\n",
            "Iteration: 13522; Percent complete: 90.1%; Average loss: 0.9519\n",
            "Iteration: 13523; Percent complete: 90.2%; Average loss: 0.7359\n",
            "Iteration: 13524; Percent complete: 90.2%; Average loss: 0.7936\n",
            "Iteration: 13525; Percent complete: 90.2%; Average loss: 0.9316\n",
            "Iteration: 13526; Percent complete: 90.2%; Average loss: 0.9228\n",
            "Iteration: 13527; Percent complete: 90.2%; Average loss: 0.8494\n",
            "Iteration: 13528; Percent complete: 90.2%; Average loss: 0.9535\n",
            "Iteration: 13529; Percent complete: 90.2%; Average loss: 0.8187\n",
            "Iteration: 13530; Percent complete: 90.2%; Average loss: 0.9243\n",
            "Iteration: 13531; Percent complete: 90.2%; Average loss: 0.7581\n",
            "Iteration: 13532; Percent complete: 90.2%; Average loss: 0.9741\n",
            "Iteration: 13533; Percent complete: 90.2%; Average loss: 0.9254\n",
            "Iteration: 13534; Percent complete: 90.2%; Average loss: 0.9114\n",
            "Iteration: 13535; Percent complete: 90.2%; Average loss: 0.7954\n",
            "Iteration: 13536; Percent complete: 90.2%; Average loss: 0.8273\n",
            "Iteration: 13537; Percent complete: 90.2%; Average loss: 0.9022\n",
            "Iteration: 13538; Percent complete: 90.3%; Average loss: 0.8341\n",
            "Iteration: 13539; Percent complete: 90.3%; Average loss: 0.8646\n",
            "Iteration: 13540; Percent complete: 90.3%; Average loss: 1.0848\n",
            "Iteration: 13541; Percent complete: 90.3%; Average loss: 0.8507\n",
            "Iteration: 13542; Percent complete: 90.3%; Average loss: 0.8402\n",
            "Iteration: 13543; Percent complete: 90.3%; Average loss: 0.8233\n",
            "Iteration: 13544; Percent complete: 90.3%; Average loss: 0.8400\n",
            "Iteration: 13545; Percent complete: 90.3%; Average loss: 0.7856\n",
            "Iteration: 13546; Percent complete: 90.3%; Average loss: 0.9144\n",
            "Iteration: 13547; Percent complete: 90.3%; Average loss: 0.8676\n",
            "Iteration: 13548; Percent complete: 90.3%; Average loss: 0.6730\n",
            "Iteration: 13549; Percent complete: 90.3%; Average loss: 0.6159\n",
            "Iteration: 13550; Percent complete: 90.3%; Average loss: 0.8716\n",
            "Iteration: 13551; Percent complete: 90.3%; Average loss: 0.8037\n",
            "Iteration: 13552; Percent complete: 90.3%; Average loss: 0.8690\n",
            "Iteration: 13553; Percent complete: 90.4%; Average loss: 0.7158\n",
            "Iteration: 13554; Percent complete: 90.4%; Average loss: 0.9635\n",
            "Iteration: 13555; Percent complete: 90.4%; Average loss: 0.9598\n",
            "Iteration: 13556; Percent complete: 90.4%; Average loss: 0.9422\n",
            "Iteration: 13557; Percent complete: 90.4%; Average loss: 0.7893\n",
            "Iteration: 13558; Percent complete: 90.4%; Average loss: 0.7987\n",
            "Iteration: 13559; Percent complete: 90.4%; Average loss: 0.9507\n",
            "Iteration: 13560; Percent complete: 90.4%; Average loss: 0.6825\n",
            "Iteration: 13561; Percent complete: 90.4%; Average loss: 0.9072\n",
            "Iteration: 13562; Percent complete: 90.4%; Average loss: 0.8111\n",
            "Iteration: 13563; Percent complete: 90.4%; Average loss: 0.6917\n",
            "Iteration: 13564; Percent complete: 90.4%; Average loss: 0.8287\n",
            "Iteration: 13565; Percent complete: 90.4%; Average loss: 0.9869\n",
            "Iteration: 13566; Percent complete: 90.4%; Average loss: 0.8174\n",
            "Iteration: 13567; Percent complete: 90.4%; Average loss: 0.8258\n",
            "Iteration: 13568; Percent complete: 90.5%; Average loss: 0.7543\n",
            "Iteration: 13569; Percent complete: 90.5%; Average loss: 0.7486\n",
            "Iteration: 13570; Percent complete: 90.5%; Average loss: 0.9361\n",
            "Iteration: 13571; Percent complete: 90.5%; Average loss: 0.9272\n",
            "Iteration: 13572; Percent complete: 90.5%; Average loss: 0.9960\n",
            "Iteration: 13573; Percent complete: 90.5%; Average loss: 0.9036\n",
            "Iteration: 13574; Percent complete: 90.5%; Average loss: 0.7814\n",
            "Iteration: 13575; Percent complete: 90.5%; Average loss: 0.7606\n",
            "Iteration: 13576; Percent complete: 90.5%; Average loss: 0.8541\n",
            "Iteration: 13577; Percent complete: 90.5%; Average loss: 0.8498\n",
            "Iteration: 13578; Percent complete: 90.5%; Average loss: 0.8514\n",
            "Iteration: 13579; Percent complete: 90.5%; Average loss: 0.7358\n",
            "Iteration: 13580; Percent complete: 90.5%; Average loss: 1.0844\n",
            "Iteration: 13581; Percent complete: 90.5%; Average loss: 0.8366\n",
            "Iteration: 13582; Percent complete: 90.5%; Average loss: 0.9423\n",
            "Iteration: 13583; Percent complete: 90.6%; Average loss: 0.9504\n",
            "Iteration: 13584; Percent complete: 90.6%; Average loss: 0.8534\n",
            "Iteration: 13585; Percent complete: 90.6%; Average loss: 0.7385\n",
            "Iteration: 13586; Percent complete: 90.6%; Average loss: 0.8381\n",
            "Iteration: 13587; Percent complete: 90.6%; Average loss: 0.8710\n",
            "Iteration: 13588; Percent complete: 90.6%; Average loss: 0.9773\n",
            "Iteration: 13589; Percent complete: 90.6%; Average loss: 0.7773\n",
            "Iteration: 13590; Percent complete: 90.6%; Average loss: 0.8024\n",
            "Iteration: 13591; Percent complete: 90.6%; Average loss: 0.8318\n",
            "Iteration: 13592; Percent complete: 90.6%; Average loss: 0.9697\n",
            "Iteration: 13593; Percent complete: 90.6%; Average loss: 0.7655\n",
            "Iteration: 13594; Percent complete: 90.6%; Average loss: 0.9253\n",
            "Iteration: 13595; Percent complete: 90.6%; Average loss: 0.8169\n",
            "Iteration: 13596; Percent complete: 90.6%; Average loss: 0.7442\n",
            "Iteration: 13597; Percent complete: 90.6%; Average loss: 1.0300\n",
            "Iteration: 13598; Percent complete: 90.7%; Average loss: 0.8045\n",
            "Iteration: 13599; Percent complete: 90.7%; Average loss: 0.8238\n",
            "Iteration: 13600; Percent complete: 90.7%; Average loss: 0.8429\n",
            "Iteration: 13601; Percent complete: 90.7%; Average loss: 0.9018\n",
            "Iteration: 13602; Percent complete: 90.7%; Average loss: 0.7385\n",
            "Iteration: 13603; Percent complete: 90.7%; Average loss: 0.8341\n",
            "Iteration: 13604; Percent complete: 90.7%; Average loss: 0.8535\n",
            "Iteration: 13605; Percent complete: 90.7%; Average loss: 0.8845\n",
            "Iteration: 13606; Percent complete: 90.7%; Average loss: 0.9349\n",
            "Iteration: 13607; Percent complete: 90.7%; Average loss: 0.8283\n",
            "Iteration: 13608; Percent complete: 90.7%; Average loss: 0.8967\n",
            "Iteration: 13609; Percent complete: 90.7%; Average loss: 0.9153\n",
            "Iteration: 13610; Percent complete: 90.7%; Average loss: 0.8407\n",
            "Iteration: 13611; Percent complete: 90.7%; Average loss: 0.8476\n",
            "Iteration: 13612; Percent complete: 90.7%; Average loss: 0.8196\n",
            "Iteration: 13613; Percent complete: 90.8%; Average loss: 0.8678\n",
            "Iteration: 13614; Percent complete: 90.8%; Average loss: 0.7148\n",
            "Iteration: 13615; Percent complete: 90.8%; Average loss: 0.7118\n",
            "Iteration: 13616; Percent complete: 90.8%; Average loss: 0.8718\n",
            "Iteration: 13617; Percent complete: 90.8%; Average loss: 0.9066\n",
            "Iteration: 13618; Percent complete: 90.8%; Average loss: 0.6957\n",
            "Iteration: 13619; Percent complete: 90.8%; Average loss: 0.8606\n",
            "Iteration: 13620; Percent complete: 90.8%; Average loss: 0.9070\n",
            "Iteration: 13621; Percent complete: 90.8%; Average loss: 0.8311\n",
            "Iteration: 13622; Percent complete: 90.8%; Average loss: 0.9145\n",
            "Iteration: 13623; Percent complete: 90.8%; Average loss: 0.8241\n",
            "Iteration: 13624; Percent complete: 90.8%; Average loss: 0.7983\n",
            "Iteration: 13625; Percent complete: 90.8%; Average loss: 0.8020\n",
            "Iteration: 13626; Percent complete: 90.8%; Average loss: 0.8458\n",
            "Iteration: 13627; Percent complete: 90.8%; Average loss: 0.9965\n",
            "Iteration: 13628; Percent complete: 90.9%; Average loss: 0.8388\n",
            "Iteration: 13629; Percent complete: 90.9%; Average loss: 0.7793\n",
            "Iteration: 13630; Percent complete: 90.9%; Average loss: 0.8063\n",
            "Iteration: 13631; Percent complete: 90.9%; Average loss: 0.8561\n",
            "Iteration: 13632; Percent complete: 90.9%; Average loss: 0.7377\n",
            "Iteration: 13633; Percent complete: 90.9%; Average loss: 0.8156\n",
            "Iteration: 13634; Percent complete: 90.9%; Average loss: 0.8197\n",
            "Iteration: 13635; Percent complete: 90.9%; Average loss: 0.8191\n",
            "Iteration: 13636; Percent complete: 90.9%; Average loss: 0.7871\n",
            "Iteration: 13637; Percent complete: 90.9%; Average loss: 0.9492\n",
            "Iteration: 13638; Percent complete: 90.9%; Average loss: 0.8979\n",
            "Iteration: 13639; Percent complete: 90.9%; Average loss: 0.7902\n",
            "Iteration: 13640; Percent complete: 90.9%; Average loss: 0.8238\n",
            "Iteration: 13641; Percent complete: 90.9%; Average loss: 0.8137\n",
            "Iteration: 13642; Percent complete: 90.9%; Average loss: 0.8558\n",
            "Iteration: 13643; Percent complete: 91.0%; Average loss: 0.9202\n",
            "Iteration: 13644; Percent complete: 91.0%; Average loss: 0.9178\n",
            "Iteration: 13645; Percent complete: 91.0%; Average loss: 0.8557\n",
            "Iteration: 13646; Percent complete: 91.0%; Average loss: 0.8647\n",
            "Iteration: 13647; Percent complete: 91.0%; Average loss: 0.8550\n",
            "Iteration: 13648; Percent complete: 91.0%; Average loss: 0.7798\n",
            "Iteration: 13649; Percent complete: 91.0%; Average loss: 0.7311\n",
            "Iteration: 13650; Percent complete: 91.0%; Average loss: 0.8481\n",
            "Iteration: 13651; Percent complete: 91.0%; Average loss: 0.8302\n",
            "Iteration: 13652; Percent complete: 91.0%; Average loss: 0.8783\n",
            "Iteration: 13653; Percent complete: 91.0%; Average loss: 0.8539\n",
            "Iteration: 13654; Percent complete: 91.0%; Average loss: 0.8280\n",
            "Iteration: 13655; Percent complete: 91.0%; Average loss: 0.7936\n",
            "Iteration: 13656; Percent complete: 91.0%; Average loss: 1.0604\n",
            "Iteration: 13657; Percent complete: 91.0%; Average loss: 0.9146\n",
            "Iteration: 13658; Percent complete: 91.1%; Average loss: 0.7843\n",
            "Iteration: 13659; Percent complete: 91.1%; Average loss: 0.9330\n",
            "Iteration: 13660; Percent complete: 91.1%; Average loss: 0.9536\n",
            "Iteration: 13661; Percent complete: 91.1%; Average loss: 0.8690\n",
            "Iteration: 13662; Percent complete: 91.1%; Average loss: 0.7938\n",
            "Iteration: 13663; Percent complete: 91.1%; Average loss: 0.9308\n",
            "Iteration: 13664; Percent complete: 91.1%; Average loss: 0.8156\n",
            "Iteration: 13665; Percent complete: 91.1%; Average loss: 0.7812\n",
            "Iteration: 13666; Percent complete: 91.1%; Average loss: 0.8301\n",
            "Iteration: 13667; Percent complete: 91.1%; Average loss: 0.8014\n",
            "Iteration: 13668; Percent complete: 91.1%; Average loss: 0.9390\n",
            "Iteration: 13669; Percent complete: 91.1%; Average loss: 0.8941\n",
            "Iteration: 13670; Percent complete: 91.1%; Average loss: 0.8758\n",
            "Iteration: 13671; Percent complete: 91.1%; Average loss: 0.8996\n",
            "Iteration: 13672; Percent complete: 91.1%; Average loss: 0.9662\n",
            "Iteration: 13673; Percent complete: 91.2%; Average loss: 0.7933\n",
            "Iteration: 13674; Percent complete: 91.2%; Average loss: 1.0464\n",
            "Iteration: 13675; Percent complete: 91.2%; Average loss: 0.7994\n",
            "Iteration: 13676; Percent complete: 91.2%; Average loss: 0.6914\n",
            "Iteration: 13677; Percent complete: 91.2%; Average loss: 1.1045\n",
            "Iteration: 13678; Percent complete: 91.2%; Average loss: 0.7729\n",
            "Iteration: 13679; Percent complete: 91.2%; Average loss: 0.8451\n",
            "Iteration: 13680; Percent complete: 91.2%; Average loss: 0.7283\n",
            "Iteration: 13681; Percent complete: 91.2%; Average loss: 0.9312\n",
            "Iteration: 13682; Percent complete: 91.2%; Average loss: 0.8571\n",
            "Iteration: 13683; Percent complete: 91.2%; Average loss: 0.8713\n",
            "Iteration: 13684; Percent complete: 91.2%; Average loss: 0.8231\n",
            "Iteration: 13685; Percent complete: 91.2%; Average loss: 0.9135\n",
            "Iteration: 13686; Percent complete: 91.2%; Average loss: 0.7759\n",
            "Iteration: 13687; Percent complete: 91.2%; Average loss: 0.8717\n",
            "Iteration: 13688; Percent complete: 91.3%; Average loss: 0.9290\n",
            "Iteration: 13689; Percent complete: 91.3%; Average loss: 0.8082\n",
            "Iteration: 13690; Percent complete: 91.3%; Average loss: 0.9034\n",
            "Iteration: 13691; Percent complete: 91.3%; Average loss: 0.8094\n",
            "Iteration: 13692; Percent complete: 91.3%; Average loss: 0.9368\n",
            "Iteration: 13693; Percent complete: 91.3%; Average loss: 0.8901\n",
            "Iteration: 13694; Percent complete: 91.3%; Average loss: 0.9206\n",
            "Iteration: 13695; Percent complete: 91.3%; Average loss: 0.7798\n",
            "Iteration: 13696; Percent complete: 91.3%; Average loss: 0.9547\n",
            "Iteration: 13697; Percent complete: 91.3%; Average loss: 0.8505\n",
            "Iteration: 13698; Percent complete: 91.3%; Average loss: 0.9156\n",
            "Iteration: 13699; Percent complete: 91.3%; Average loss: 0.8341\n",
            "Iteration: 13700; Percent complete: 91.3%; Average loss: 0.9181\n",
            "Iteration: 13701; Percent complete: 91.3%; Average loss: 0.8222\n",
            "Iteration: 13702; Percent complete: 91.3%; Average loss: 0.9767\n",
            "Iteration: 13703; Percent complete: 91.4%; Average loss: 0.9673\n",
            "Iteration: 13704; Percent complete: 91.4%; Average loss: 0.9510\n",
            "Iteration: 13705; Percent complete: 91.4%; Average loss: 0.9111\n",
            "Iteration: 13706; Percent complete: 91.4%; Average loss: 0.7806\n",
            "Iteration: 13707; Percent complete: 91.4%; Average loss: 0.9238\n",
            "Iteration: 13708; Percent complete: 91.4%; Average loss: 0.8398\n",
            "Iteration: 13709; Percent complete: 91.4%; Average loss: 0.9559\n",
            "Iteration: 13710; Percent complete: 91.4%; Average loss: 0.7350\n",
            "Iteration: 13711; Percent complete: 91.4%; Average loss: 0.8251\n",
            "Iteration: 13712; Percent complete: 91.4%; Average loss: 0.7846\n",
            "Iteration: 13713; Percent complete: 91.4%; Average loss: 0.8952\n",
            "Iteration: 13714; Percent complete: 91.4%; Average loss: 0.8543\n",
            "Iteration: 13715; Percent complete: 91.4%; Average loss: 0.7196\n",
            "Iteration: 13716; Percent complete: 91.4%; Average loss: 0.8070\n",
            "Iteration: 13717; Percent complete: 91.4%; Average loss: 0.9195\n",
            "Iteration: 13718; Percent complete: 91.5%; Average loss: 0.6934\n",
            "Iteration: 13719; Percent complete: 91.5%; Average loss: 0.8305\n",
            "Iteration: 13720; Percent complete: 91.5%; Average loss: 0.7603\n",
            "Iteration: 13721; Percent complete: 91.5%; Average loss: 0.8856\n",
            "Iteration: 13722; Percent complete: 91.5%; Average loss: 0.6793\n",
            "Iteration: 13723; Percent complete: 91.5%; Average loss: 0.6729\n",
            "Iteration: 13724; Percent complete: 91.5%; Average loss: 0.6878\n",
            "Iteration: 13725; Percent complete: 91.5%; Average loss: 0.9164\n",
            "Iteration: 13726; Percent complete: 91.5%; Average loss: 0.9137\n",
            "Iteration: 13727; Percent complete: 91.5%; Average loss: 0.9443\n",
            "Iteration: 13728; Percent complete: 91.5%; Average loss: 0.8573\n",
            "Iteration: 13729; Percent complete: 91.5%; Average loss: 0.8936\n",
            "Iteration: 13730; Percent complete: 91.5%; Average loss: 0.9766\n",
            "Iteration: 13731; Percent complete: 91.5%; Average loss: 0.7460\n",
            "Iteration: 13732; Percent complete: 91.5%; Average loss: 0.7969\n",
            "Iteration: 13733; Percent complete: 91.6%; Average loss: 0.8693\n",
            "Iteration: 13734; Percent complete: 91.6%; Average loss: 0.8843\n",
            "Iteration: 13735; Percent complete: 91.6%; Average loss: 1.0060\n",
            "Iteration: 13736; Percent complete: 91.6%; Average loss: 0.8983\n",
            "Iteration: 13737; Percent complete: 91.6%; Average loss: 0.9860\n",
            "Iteration: 13738; Percent complete: 91.6%; Average loss: 0.8518\n",
            "Iteration: 13739; Percent complete: 91.6%; Average loss: 0.7846\n",
            "Iteration: 13740; Percent complete: 91.6%; Average loss: 0.7542\n",
            "Iteration: 13741; Percent complete: 91.6%; Average loss: 0.6543\n",
            "Iteration: 13742; Percent complete: 91.6%; Average loss: 0.8285\n",
            "Iteration: 13743; Percent complete: 91.6%; Average loss: 0.9079\n",
            "Iteration: 13744; Percent complete: 91.6%; Average loss: 0.8970\n",
            "Iteration: 13745; Percent complete: 91.6%; Average loss: 0.8464\n",
            "Iteration: 13746; Percent complete: 91.6%; Average loss: 0.9777\n",
            "Iteration: 13747; Percent complete: 91.6%; Average loss: 0.8629\n",
            "Iteration: 13748; Percent complete: 91.7%; Average loss: 0.7744\n",
            "Iteration: 13749; Percent complete: 91.7%; Average loss: 0.8814\n",
            "Iteration: 13750; Percent complete: 91.7%; Average loss: 0.9022\n",
            "Iteration: 13751; Percent complete: 91.7%; Average loss: 0.8521\n",
            "Iteration: 13752; Percent complete: 91.7%; Average loss: 0.8721\n",
            "Iteration: 13753; Percent complete: 91.7%; Average loss: 0.8577\n",
            "Iteration: 13754; Percent complete: 91.7%; Average loss: 0.9352\n",
            "Iteration: 13755; Percent complete: 91.7%; Average loss: 0.8772\n",
            "Iteration: 13756; Percent complete: 91.7%; Average loss: 0.9374\n",
            "Iteration: 13757; Percent complete: 91.7%; Average loss: 0.9064\n",
            "Iteration: 13758; Percent complete: 91.7%; Average loss: 0.8541\n",
            "Iteration: 13759; Percent complete: 91.7%; Average loss: 0.7733\n",
            "Iteration: 13760; Percent complete: 91.7%; Average loss: 0.7300\n",
            "Iteration: 13761; Percent complete: 91.7%; Average loss: 0.7244\n",
            "Iteration: 13762; Percent complete: 91.7%; Average loss: 0.6794\n",
            "Iteration: 13763; Percent complete: 91.8%; Average loss: 0.8126\n",
            "Iteration: 13764; Percent complete: 91.8%; Average loss: 0.8669\n",
            "Iteration: 13765; Percent complete: 91.8%; Average loss: 0.7855\n",
            "Iteration: 13766; Percent complete: 91.8%; Average loss: 0.8045\n",
            "Iteration: 13767; Percent complete: 91.8%; Average loss: 0.8620\n",
            "Iteration: 13768; Percent complete: 91.8%; Average loss: 0.7287\n",
            "Iteration: 13769; Percent complete: 91.8%; Average loss: 0.8692\n",
            "Iteration: 13770; Percent complete: 91.8%; Average loss: 0.9057\n",
            "Iteration: 13771; Percent complete: 91.8%; Average loss: 0.9570\n",
            "Iteration: 13772; Percent complete: 91.8%; Average loss: 0.8397\n",
            "Iteration: 13773; Percent complete: 91.8%; Average loss: 0.9993\n",
            "Iteration: 13774; Percent complete: 91.8%; Average loss: 0.8898\n",
            "Iteration: 13775; Percent complete: 91.8%; Average loss: 0.6978\n",
            "Iteration: 13776; Percent complete: 91.8%; Average loss: 0.8225\n",
            "Iteration: 13777; Percent complete: 91.8%; Average loss: 0.6285\n",
            "Iteration: 13778; Percent complete: 91.9%; Average loss: 0.8183\n",
            "Iteration: 13779; Percent complete: 91.9%; Average loss: 0.8786\n",
            "Iteration: 13780; Percent complete: 91.9%; Average loss: 0.8359\n",
            "Iteration: 13781; Percent complete: 91.9%; Average loss: 0.8836\n",
            "Iteration: 13782; Percent complete: 91.9%; Average loss: 0.7107\n",
            "Iteration: 13783; Percent complete: 91.9%; Average loss: 0.7122\n",
            "Iteration: 13784; Percent complete: 91.9%; Average loss: 0.8333\n",
            "Iteration: 13785; Percent complete: 91.9%; Average loss: 0.8310\n",
            "Iteration: 13786; Percent complete: 91.9%; Average loss: 0.7426\n",
            "Iteration: 13787; Percent complete: 91.9%; Average loss: 0.7545\n",
            "Iteration: 13788; Percent complete: 91.9%; Average loss: 0.7750\n",
            "Iteration: 13789; Percent complete: 91.9%; Average loss: 0.8016\n",
            "Iteration: 13790; Percent complete: 91.9%; Average loss: 1.0162\n",
            "Iteration: 13791; Percent complete: 91.9%; Average loss: 0.9286\n",
            "Iteration: 13792; Percent complete: 91.9%; Average loss: 0.8674\n",
            "Iteration: 13793; Percent complete: 92.0%; Average loss: 0.8511\n",
            "Iteration: 13794; Percent complete: 92.0%; Average loss: 0.8763\n",
            "Iteration: 13795; Percent complete: 92.0%; Average loss: 0.7040\n",
            "Iteration: 13796; Percent complete: 92.0%; Average loss: 0.8003\n",
            "Iteration: 13797; Percent complete: 92.0%; Average loss: 0.8453\n",
            "Iteration: 13798; Percent complete: 92.0%; Average loss: 0.7338\n",
            "Iteration: 13799; Percent complete: 92.0%; Average loss: 0.7320\n",
            "Iteration: 13800; Percent complete: 92.0%; Average loss: 0.7546\n",
            "Iteration: 13801; Percent complete: 92.0%; Average loss: 0.7709\n",
            "Iteration: 13802; Percent complete: 92.0%; Average loss: 0.8677\n",
            "Iteration: 13803; Percent complete: 92.0%; Average loss: 0.9133\n",
            "Iteration: 13804; Percent complete: 92.0%; Average loss: 0.8104\n",
            "Iteration: 13805; Percent complete: 92.0%; Average loss: 1.0277\n",
            "Iteration: 13806; Percent complete: 92.0%; Average loss: 0.7106\n",
            "Iteration: 13807; Percent complete: 92.0%; Average loss: 0.8675\n",
            "Iteration: 13808; Percent complete: 92.1%; Average loss: 0.7233\n",
            "Iteration: 13809; Percent complete: 92.1%; Average loss: 0.7787\n",
            "Iteration: 13810; Percent complete: 92.1%; Average loss: 0.8360\n",
            "Iteration: 13811; Percent complete: 92.1%; Average loss: 0.7659\n",
            "Iteration: 13812; Percent complete: 92.1%; Average loss: 0.9147\n",
            "Iteration: 13813; Percent complete: 92.1%; Average loss: 0.7756\n",
            "Iteration: 13814; Percent complete: 92.1%; Average loss: 0.7469\n",
            "Iteration: 13815; Percent complete: 92.1%; Average loss: 0.9935\n",
            "Iteration: 13816; Percent complete: 92.1%; Average loss: 0.7899\n",
            "Iteration: 13817; Percent complete: 92.1%; Average loss: 0.7482\n",
            "Iteration: 13818; Percent complete: 92.1%; Average loss: 0.7941\n",
            "Iteration: 13819; Percent complete: 92.1%; Average loss: 0.7362\n",
            "Iteration: 13820; Percent complete: 92.1%; Average loss: 1.0203\n",
            "Iteration: 13821; Percent complete: 92.1%; Average loss: 0.7903\n",
            "Iteration: 13822; Percent complete: 92.1%; Average loss: 0.8324\n",
            "Iteration: 13823; Percent complete: 92.2%; Average loss: 0.7183\n",
            "Iteration: 13824; Percent complete: 92.2%; Average loss: 0.9892\n",
            "Iteration: 13825; Percent complete: 92.2%; Average loss: 0.7804\n",
            "Iteration: 13826; Percent complete: 92.2%; Average loss: 0.6501\n",
            "Iteration: 13827; Percent complete: 92.2%; Average loss: 0.7219\n",
            "Iteration: 13828; Percent complete: 92.2%; Average loss: 0.7409\n",
            "Iteration: 13829; Percent complete: 92.2%; Average loss: 0.6890\n",
            "Iteration: 13830; Percent complete: 92.2%; Average loss: 0.9987\n",
            "Iteration: 13831; Percent complete: 92.2%; Average loss: 1.0154\n",
            "Iteration: 13832; Percent complete: 92.2%; Average loss: 0.9008\n",
            "Iteration: 13833; Percent complete: 92.2%; Average loss: 0.7163\n",
            "Iteration: 13834; Percent complete: 92.2%; Average loss: 0.7597\n",
            "Iteration: 13835; Percent complete: 92.2%; Average loss: 0.8470\n",
            "Iteration: 13836; Percent complete: 92.2%; Average loss: 0.8768\n",
            "Iteration: 13837; Percent complete: 92.2%; Average loss: 0.7525\n",
            "Iteration: 13838; Percent complete: 92.3%; Average loss: 0.7171\n",
            "Iteration: 13839; Percent complete: 92.3%; Average loss: 0.6832\n",
            "Iteration: 13840; Percent complete: 92.3%; Average loss: 0.8189\n",
            "Iteration: 13841; Percent complete: 92.3%; Average loss: 0.7919\n",
            "Iteration: 13842; Percent complete: 92.3%; Average loss: 0.7532\n",
            "Iteration: 13843; Percent complete: 92.3%; Average loss: 0.8498\n",
            "Iteration: 13844; Percent complete: 92.3%; Average loss: 0.7399\n",
            "Iteration: 13845; Percent complete: 92.3%; Average loss: 0.7526\n",
            "Iteration: 13846; Percent complete: 92.3%; Average loss: 0.8735\n",
            "Iteration: 13847; Percent complete: 92.3%; Average loss: 0.7833\n",
            "Iteration: 13848; Percent complete: 92.3%; Average loss: 0.9059\n",
            "Iteration: 13849; Percent complete: 92.3%; Average loss: 0.7053\n",
            "Iteration: 13850; Percent complete: 92.3%; Average loss: 0.7895\n",
            "Iteration: 13851; Percent complete: 92.3%; Average loss: 0.9015\n",
            "Iteration: 13852; Percent complete: 92.3%; Average loss: 0.7611\n",
            "Iteration: 13853; Percent complete: 92.4%; Average loss: 0.9387\n",
            "Iteration: 13854; Percent complete: 92.4%; Average loss: 0.8778\n",
            "Iteration: 13855; Percent complete: 92.4%; Average loss: 0.7891\n",
            "Iteration: 13856; Percent complete: 92.4%; Average loss: 0.7962\n",
            "Iteration: 13857; Percent complete: 92.4%; Average loss: 0.8714\n",
            "Iteration: 13858; Percent complete: 92.4%; Average loss: 0.7684\n",
            "Iteration: 13859; Percent complete: 92.4%; Average loss: 0.7459\n",
            "Iteration: 13860; Percent complete: 92.4%; Average loss: 0.8404\n",
            "Iteration: 13861; Percent complete: 92.4%; Average loss: 0.8362\n",
            "Iteration: 13862; Percent complete: 92.4%; Average loss: 0.7641\n",
            "Iteration: 13863; Percent complete: 92.4%; Average loss: 0.8962\n",
            "Iteration: 13864; Percent complete: 92.4%; Average loss: 0.8232\n",
            "Iteration: 13865; Percent complete: 92.4%; Average loss: 0.6819\n",
            "Iteration: 13866; Percent complete: 92.4%; Average loss: 0.7914\n",
            "Iteration: 13867; Percent complete: 92.4%; Average loss: 0.8519\n",
            "Iteration: 13868; Percent complete: 92.5%; Average loss: 0.9101\n",
            "Iteration: 13869; Percent complete: 92.5%; Average loss: 0.7897\n",
            "Iteration: 13870; Percent complete: 92.5%; Average loss: 0.7214\n",
            "Iteration: 13871; Percent complete: 92.5%; Average loss: 0.7469\n",
            "Iteration: 13872; Percent complete: 92.5%; Average loss: 0.7740\n",
            "Iteration: 13873; Percent complete: 92.5%; Average loss: 0.7522\n",
            "Iteration: 13874; Percent complete: 92.5%; Average loss: 0.7769\n",
            "Iteration: 13875; Percent complete: 92.5%; Average loss: 0.7729\n",
            "Iteration: 13876; Percent complete: 92.5%; Average loss: 0.8805\n",
            "Iteration: 13877; Percent complete: 92.5%; Average loss: 1.0506\n",
            "Iteration: 13878; Percent complete: 92.5%; Average loss: 0.8115\n",
            "Iteration: 13879; Percent complete: 92.5%; Average loss: 0.7397\n",
            "Iteration: 13880; Percent complete: 92.5%; Average loss: 0.7619\n",
            "Iteration: 13881; Percent complete: 92.5%; Average loss: 0.8281\n",
            "Iteration: 13882; Percent complete: 92.5%; Average loss: 0.7810\n",
            "Iteration: 13883; Percent complete: 92.6%; Average loss: 0.7695\n",
            "Iteration: 13884; Percent complete: 92.6%; Average loss: 0.8951\n",
            "Iteration: 13885; Percent complete: 92.6%; Average loss: 0.8049\n",
            "Iteration: 13886; Percent complete: 92.6%; Average loss: 0.7927\n",
            "Iteration: 13887; Percent complete: 92.6%; Average loss: 0.9956\n",
            "Iteration: 13888; Percent complete: 92.6%; Average loss: 0.7690\n",
            "Iteration: 13889; Percent complete: 92.6%; Average loss: 0.9158\n",
            "Iteration: 13890; Percent complete: 92.6%; Average loss: 0.8121\n",
            "Iteration: 13891; Percent complete: 92.6%; Average loss: 0.7042\n",
            "Iteration: 13892; Percent complete: 92.6%; Average loss: 0.7060\n",
            "Iteration: 13893; Percent complete: 92.6%; Average loss: 0.7115\n",
            "Iteration: 13894; Percent complete: 92.6%; Average loss: 0.8079\n",
            "Iteration: 13895; Percent complete: 92.6%; Average loss: 0.8526\n",
            "Iteration: 13896; Percent complete: 92.6%; Average loss: 0.7035\n",
            "Iteration: 13897; Percent complete: 92.6%; Average loss: 0.8752\n",
            "Iteration: 13898; Percent complete: 92.7%; Average loss: 0.6198\n",
            "Iteration: 13899; Percent complete: 92.7%; Average loss: 0.7650\n",
            "Iteration: 13900; Percent complete: 92.7%; Average loss: 0.8793\n",
            "Iteration: 13901; Percent complete: 92.7%; Average loss: 0.7715\n",
            "Iteration: 13902; Percent complete: 92.7%; Average loss: 0.6741\n",
            "Iteration: 13903; Percent complete: 92.7%; Average loss: 0.6891\n",
            "Iteration: 13904; Percent complete: 92.7%; Average loss: 1.0115\n",
            "Iteration: 13905; Percent complete: 92.7%; Average loss: 0.7262\n",
            "Iteration: 13906; Percent complete: 92.7%; Average loss: 0.9464\n",
            "Iteration: 13907; Percent complete: 92.7%; Average loss: 0.6962\n",
            "Iteration: 13908; Percent complete: 92.7%; Average loss: 0.8009\n",
            "Iteration: 13909; Percent complete: 92.7%; Average loss: 0.8304\n",
            "Iteration: 13910; Percent complete: 92.7%; Average loss: 0.8610\n",
            "Iteration: 13911; Percent complete: 92.7%; Average loss: 0.8290\n",
            "Iteration: 13912; Percent complete: 92.7%; Average loss: 0.8855\n",
            "Iteration: 13913; Percent complete: 92.8%; Average loss: 0.8389\n",
            "Iteration: 13914; Percent complete: 92.8%; Average loss: 0.7346\n",
            "Iteration: 13915; Percent complete: 92.8%; Average loss: 0.9348\n",
            "Iteration: 13916; Percent complete: 92.8%; Average loss: 0.5862\n",
            "Iteration: 13917; Percent complete: 92.8%; Average loss: 0.8852\n",
            "Iteration: 13918; Percent complete: 92.8%; Average loss: 0.8345\n",
            "Iteration: 13919; Percent complete: 92.8%; Average loss: 0.7784\n",
            "Iteration: 13920; Percent complete: 92.8%; Average loss: 0.8505\n",
            "Iteration: 13921; Percent complete: 92.8%; Average loss: 0.7565\n",
            "Iteration: 13922; Percent complete: 92.8%; Average loss: 1.0137\n",
            "Iteration: 13923; Percent complete: 92.8%; Average loss: 0.7420\n",
            "Iteration: 13924; Percent complete: 92.8%; Average loss: 0.8980\n",
            "Iteration: 13925; Percent complete: 92.8%; Average loss: 0.8880\n",
            "Iteration: 13926; Percent complete: 92.8%; Average loss: 0.5780\n",
            "Iteration: 13927; Percent complete: 92.8%; Average loss: 0.6156\n",
            "Iteration: 13928; Percent complete: 92.9%; Average loss: 0.6688\n",
            "Iteration: 13929; Percent complete: 92.9%; Average loss: 0.6588\n",
            "Iteration: 13930; Percent complete: 92.9%; Average loss: 0.6842\n",
            "Iteration: 13931; Percent complete: 92.9%; Average loss: 0.7856\n",
            "Iteration: 13932; Percent complete: 92.9%; Average loss: 0.6420\n",
            "Iteration: 13933; Percent complete: 92.9%; Average loss: 0.7329\n",
            "Iteration: 13934; Percent complete: 92.9%; Average loss: 0.6318\n",
            "Iteration: 13935; Percent complete: 92.9%; Average loss: 0.8224\n",
            "Iteration: 13936; Percent complete: 92.9%; Average loss: 0.7854\n",
            "Iteration: 13937; Percent complete: 92.9%; Average loss: 0.8518\n",
            "Iteration: 13938; Percent complete: 92.9%; Average loss: 0.8756\n",
            "Iteration: 13939; Percent complete: 92.9%; Average loss: 0.6679\n",
            "Iteration: 13940; Percent complete: 92.9%; Average loss: 0.8280\n",
            "Iteration: 13941; Percent complete: 92.9%; Average loss: 0.9130\n",
            "Iteration: 13942; Percent complete: 92.9%; Average loss: 0.7417\n",
            "Iteration: 13943; Percent complete: 93.0%; Average loss: 0.8003\n",
            "Iteration: 13944; Percent complete: 93.0%; Average loss: 0.8211\n",
            "Iteration: 13945; Percent complete: 93.0%; Average loss: 0.6578\n",
            "Iteration: 13946; Percent complete: 93.0%; Average loss: 0.8257\n",
            "Iteration: 13947; Percent complete: 93.0%; Average loss: 0.7765\n",
            "Iteration: 13948; Percent complete: 93.0%; Average loss: 0.7862\n",
            "Iteration: 13949; Percent complete: 93.0%; Average loss: 0.8621\n",
            "Iteration: 13950; Percent complete: 93.0%; Average loss: 0.8582\n",
            "Iteration: 13951; Percent complete: 93.0%; Average loss: 1.0100\n",
            "Iteration: 13952; Percent complete: 93.0%; Average loss: 0.7053\n",
            "Iteration: 13953; Percent complete: 93.0%; Average loss: 0.7618\n",
            "Iteration: 13954; Percent complete: 93.0%; Average loss: 0.8905\n",
            "Iteration: 13955; Percent complete: 93.0%; Average loss: 0.7417\n",
            "Iteration: 13956; Percent complete: 93.0%; Average loss: 0.7954\n",
            "Iteration: 13957; Percent complete: 93.0%; Average loss: 0.8369\n",
            "Iteration: 13958; Percent complete: 93.1%; Average loss: 0.9285\n",
            "Iteration: 13959; Percent complete: 93.1%; Average loss: 0.7732\n",
            "Iteration: 13960; Percent complete: 93.1%; Average loss: 0.6439\n",
            "Iteration: 13961; Percent complete: 93.1%; Average loss: 0.6792\n",
            "Iteration: 13962; Percent complete: 93.1%; Average loss: 0.7079\n",
            "Iteration: 13963; Percent complete: 93.1%; Average loss: 0.7166\n",
            "Iteration: 13964; Percent complete: 93.1%; Average loss: 0.8579\n",
            "Iteration: 13965; Percent complete: 93.1%; Average loss: 0.7250\n",
            "Iteration: 13966; Percent complete: 93.1%; Average loss: 0.6458\n",
            "Iteration: 13967; Percent complete: 93.1%; Average loss: 0.7535\n",
            "Iteration: 13968; Percent complete: 93.1%; Average loss: 0.8235\n",
            "Iteration: 13969; Percent complete: 93.1%; Average loss: 0.7090\n",
            "Iteration: 13970; Percent complete: 93.1%; Average loss: 0.8236\n",
            "Iteration: 13971; Percent complete: 93.1%; Average loss: 0.7927\n",
            "Iteration: 13972; Percent complete: 93.1%; Average loss: 0.8134\n",
            "Iteration: 13973; Percent complete: 93.2%; Average loss: 0.6173\n",
            "Iteration: 13974; Percent complete: 93.2%; Average loss: 0.7447\n",
            "Iteration: 13975; Percent complete: 93.2%; Average loss: 0.9945\n",
            "Iteration: 13976; Percent complete: 93.2%; Average loss: 0.9044\n",
            "Iteration: 13977; Percent complete: 93.2%; Average loss: 0.6737\n",
            "Iteration: 13978; Percent complete: 93.2%; Average loss: 0.7904\n",
            "Iteration: 13979; Percent complete: 93.2%; Average loss: 0.7045\n",
            "Iteration: 13980; Percent complete: 93.2%; Average loss: 0.8092\n",
            "Iteration: 13981; Percent complete: 93.2%; Average loss: 0.7857\n",
            "Iteration: 13982; Percent complete: 93.2%; Average loss: 0.8404\n",
            "Iteration: 13983; Percent complete: 93.2%; Average loss: 0.9530\n",
            "Iteration: 13984; Percent complete: 93.2%; Average loss: 0.8306\n",
            "Iteration: 13985; Percent complete: 93.2%; Average loss: 0.7238\n",
            "Iteration: 13986; Percent complete: 93.2%; Average loss: 0.7521\n",
            "Iteration: 13987; Percent complete: 93.2%; Average loss: 0.9754\n",
            "Iteration: 13988; Percent complete: 93.3%; Average loss: 0.6622\n",
            "Iteration: 13989; Percent complete: 93.3%; Average loss: 0.7079\n",
            "Iteration: 13990; Percent complete: 93.3%; Average loss: 0.7288\n",
            "Iteration: 13991; Percent complete: 93.3%; Average loss: 0.7768\n",
            "Iteration: 13992; Percent complete: 93.3%; Average loss: 0.8918\n",
            "Iteration: 13993; Percent complete: 93.3%; Average loss: 0.8299\n",
            "Iteration: 13994; Percent complete: 93.3%; Average loss: 0.8144\n",
            "Iteration: 13995; Percent complete: 93.3%; Average loss: 1.0195\n",
            "Iteration: 13996; Percent complete: 93.3%; Average loss: 0.7231\n",
            "Iteration: 13997; Percent complete: 93.3%; Average loss: 0.8162\n",
            "Iteration: 13998; Percent complete: 93.3%; Average loss: 0.7201\n",
            "Iteration: 13999; Percent complete: 93.3%; Average loss: 1.0290\n",
            "Iteration: 14000; Percent complete: 93.3%; Average loss: 0.7624\n",
            "Iteration: 14001; Percent complete: 93.3%; Average loss: 0.6747\n",
            "Iteration: 14002; Percent complete: 93.3%; Average loss: 0.8361\n",
            "Iteration: 14003; Percent complete: 93.4%; Average loss: 0.9316\n",
            "Iteration: 14004; Percent complete: 93.4%; Average loss: 0.7735\n",
            "Iteration: 14005; Percent complete: 93.4%; Average loss: 0.8185\n",
            "Iteration: 14006; Percent complete: 93.4%; Average loss: 0.8029\n",
            "Iteration: 14007; Percent complete: 93.4%; Average loss: 0.8030\n",
            "Iteration: 14008; Percent complete: 93.4%; Average loss: 0.6149\n",
            "Iteration: 14009; Percent complete: 93.4%; Average loss: 0.7563\n",
            "Iteration: 14010; Percent complete: 93.4%; Average loss: 0.8656\n",
            "Iteration: 14011; Percent complete: 93.4%; Average loss: 0.9143\n",
            "Iteration: 14012; Percent complete: 93.4%; Average loss: 0.6452\n",
            "Iteration: 14013; Percent complete: 93.4%; Average loss: 0.8871\n",
            "Iteration: 14014; Percent complete: 93.4%; Average loss: 0.7165\n",
            "Iteration: 14015; Percent complete: 93.4%; Average loss: 0.6785\n",
            "Iteration: 14016; Percent complete: 93.4%; Average loss: 0.7823\n",
            "Iteration: 14017; Percent complete: 93.4%; Average loss: 0.8175\n",
            "Iteration: 14018; Percent complete: 93.5%; Average loss: 0.9190\n",
            "Iteration: 14019; Percent complete: 93.5%; Average loss: 0.8166\n",
            "Iteration: 14020; Percent complete: 93.5%; Average loss: 0.8115\n",
            "Iteration: 14021; Percent complete: 93.5%; Average loss: 0.8445\n",
            "Iteration: 14022; Percent complete: 93.5%; Average loss: 0.7397\n",
            "Iteration: 14023; Percent complete: 93.5%; Average loss: 0.9317\n",
            "Iteration: 14024; Percent complete: 93.5%; Average loss: 0.7195\n",
            "Iteration: 14025; Percent complete: 93.5%; Average loss: 0.9357\n",
            "Iteration: 14026; Percent complete: 93.5%; Average loss: 0.7077\n",
            "Iteration: 14027; Percent complete: 93.5%; Average loss: 0.7437\n",
            "Iteration: 14028; Percent complete: 93.5%; Average loss: 0.8068\n",
            "Iteration: 14029; Percent complete: 93.5%; Average loss: 0.6823\n",
            "Iteration: 14030; Percent complete: 93.5%; Average loss: 0.9438\n",
            "Iteration: 14031; Percent complete: 93.5%; Average loss: 0.8667\n",
            "Iteration: 14032; Percent complete: 93.5%; Average loss: 0.9048\n",
            "Iteration: 14033; Percent complete: 93.6%; Average loss: 0.7929\n",
            "Iteration: 14034; Percent complete: 93.6%; Average loss: 0.8803\n",
            "Iteration: 14035; Percent complete: 93.6%; Average loss: 0.7559\n",
            "Iteration: 14036; Percent complete: 93.6%; Average loss: 0.7490\n",
            "Iteration: 14037; Percent complete: 93.6%; Average loss: 0.6886\n",
            "Iteration: 14038; Percent complete: 93.6%; Average loss: 0.8786\n",
            "Iteration: 14039; Percent complete: 93.6%; Average loss: 0.8512\n",
            "Iteration: 14040; Percent complete: 93.6%; Average loss: 0.9571\n",
            "Iteration: 14041; Percent complete: 93.6%; Average loss: 0.8155\n",
            "Iteration: 14042; Percent complete: 93.6%; Average loss: 0.7750\n",
            "Iteration: 14043; Percent complete: 93.6%; Average loss: 0.7307\n",
            "Iteration: 14044; Percent complete: 93.6%; Average loss: 0.8361\n",
            "Iteration: 14045; Percent complete: 93.6%; Average loss: 0.8845\n",
            "Iteration: 14046; Percent complete: 93.6%; Average loss: 0.8697\n",
            "Iteration: 14047; Percent complete: 93.6%; Average loss: 0.7569\n",
            "Iteration: 14048; Percent complete: 93.7%; Average loss: 0.9065\n",
            "Iteration: 14049; Percent complete: 93.7%; Average loss: 0.6854\n",
            "Iteration: 14050; Percent complete: 93.7%; Average loss: 0.7602\n",
            "Iteration: 14051; Percent complete: 93.7%; Average loss: 0.8020\n",
            "Iteration: 14052; Percent complete: 93.7%; Average loss: 0.8034\n",
            "Iteration: 14053; Percent complete: 93.7%; Average loss: 0.9109\n",
            "Iteration: 14054; Percent complete: 93.7%; Average loss: 0.8039\n",
            "Iteration: 14055; Percent complete: 93.7%; Average loss: 0.8085\n",
            "Iteration: 14056; Percent complete: 93.7%; Average loss: 0.8214\n",
            "Iteration: 14057; Percent complete: 93.7%; Average loss: 0.7826\n",
            "Iteration: 14058; Percent complete: 93.7%; Average loss: 0.6414\n",
            "Iteration: 14059; Percent complete: 93.7%; Average loss: 0.8439\n",
            "Iteration: 14060; Percent complete: 93.7%; Average loss: 0.8289\n",
            "Iteration: 14061; Percent complete: 93.7%; Average loss: 0.9647\n",
            "Iteration: 14062; Percent complete: 93.7%; Average loss: 0.9477\n",
            "Iteration: 14063; Percent complete: 93.8%; Average loss: 0.8527\n",
            "Iteration: 14064; Percent complete: 93.8%; Average loss: 0.7104\n",
            "Iteration: 14065; Percent complete: 93.8%; Average loss: 0.8054\n",
            "Iteration: 14066; Percent complete: 93.8%; Average loss: 0.8959\n",
            "Iteration: 14067; Percent complete: 93.8%; Average loss: 0.7472\n",
            "Iteration: 14068; Percent complete: 93.8%; Average loss: 0.8528\n",
            "Iteration: 14069; Percent complete: 93.8%; Average loss: 0.8104\n",
            "Iteration: 14070; Percent complete: 93.8%; Average loss: 0.6305\n",
            "Iteration: 14071; Percent complete: 93.8%; Average loss: 0.8058\n",
            "Iteration: 14072; Percent complete: 93.8%; Average loss: 0.8675\n",
            "Iteration: 14073; Percent complete: 93.8%; Average loss: 0.9729\n",
            "Iteration: 14074; Percent complete: 93.8%; Average loss: 0.7568\n",
            "Iteration: 14075; Percent complete: 93.8%; Average loss: 0.7510\n",
            "Iteration: 14076; Percent complete: 93.8%; Average loss: 0.8787\n",
            "Iteration: 14077; Percent complete: 93.8%; Average loss: 0.7406\n",
            "Iteration: 14078; Percent complete: 93.9%; Average loss: 0.7420\n",
            "Iteration: 14079; Percent complete: 93.9%; Average loss: 0.7415\n",
            "Iteration: 14080; Percent complete: 93.9%; Average loss: 0.7548\n",
            "Iteration: 14081; Percent complete: 93.9%; Average loss: 0.7551\n",
            "Iteration: 14082; Percent complete: 93.9%; Average loss: 0.6977\n",
            "Iteration: 14083; Percent complete: 93.9%; Average loss: 0.8268\n",
            "Iteration: 14084; Percent complete: 93.9%; Average loss: 0.7924\n",
            "Iteration: 14085; Percent complete: 93.9%; Average loss: 0.7142\n",
            "Iteration: 14086; Percent complete: 93.9%; Average loss: 0.7634\n",
            "Iteration: 14087; Percent complete: 93.9%; Average loss: 0.8935\n",
            "Iteration: 14088; Percent complete: 93.9%; Average loss: 0.8491\n",
            "Iteration: 14089; Percent complete: 93.9%; Average loss: 0.9262\n",
            "Iteration: 14090; Percent complete: 93.9%; Average loss: 0.9326\n",
            "Iteration: 14091; Percent complete: 93.9%; Average loss: 0.6310\n",
            "Iteration: 14092; Percent complete: 93.9%; Average loss: 0.7868\n",
            "Iteration: 14093; Percent complete: 94.0%; Average loss: 0.8596\n",
            "Iteration: 14094; Percent complete: 94.0%; Average loss: 0.8836\n",
            "Iteration: 14095; Percent complete: 94.0%; Average loss: 0.5884\n",
            "Iteration: 14096; Percent complete: 94.0%; Average loss: 0.7445\n",
            "Iteration: 14097; Percent complete: 94.0%; Average loss: 0.8968\n",
            "Iteration: 14098; Percent complete: 94.0%; Average loss: 0.7151\n",
            "Iteration: 14099; Percent complete: 94.0%; Average loss: 1.0292\n",
            "Iteration: 14100; Percent complete: 94.0%; Average loss: 1.0411\n",
            "Iteration: 14101; Percent complete: 94.0%; Average loss: 0.7644\n",
            "Iteration: 14102; Percent complete: 94.0%; Average loss: 0.7524\n",
            "Iteration: 14103; Percent complete: 94.0%; Average loss: 0.8826\n",
            "Iteration: 14104; Percent complete: 94.0%; Average loss: 0.7539\n",
            "Iteration: 14105; Percent complete: 94.0%; Average loss: 0.8221\n",
            "Iteration: 14106; Percent complete: 94.0%; Average loss: 0.8097\n",
            "Iteration: 14107; Percent complete: 94.0%; Average loss: 0.6933\n",
            "Iteration: 14108; Percent complete: 94.1%; Average loss: 0.6972\n",
            "Iteration: 14109; Percent complete: 94.1%; Average loss: 0.6622\n",
            "Iteration: 14110; Percent complete: 94.1%; Average loss: 0.8013\n",
            "Iteration: 14111; Percent complete: 94.1%; Average loss: 0.7316\n",
            "Iteration: 14112; Percent complete: 94.1%; Average loss: 0.7643\n",
            "Iteration: 14113; Percent complete: 94.1%; Average loss: 0.8797\n",
            "Iteration: 14114; Percent complete: 94.1%; Average loss: 0.7517\n",
            "Iteration: 14115; Percent complete: 94.1%; Average loss: 0.7151\n",
            "Iteration: 14116; Percent complete: 94.1%; Average loss: 0.9697\n",
            "Iteration: 14117; Percent complete: 94.1%; Average loss: 0.7819\n",
            "Iteration: 14118; Percent complete: 94.1%; Average loss: 0.7350\n",
            "Iteration: 14119; Percent complete: 94.1%; Average loss: 0.7706\n",
            "Iteration: 14120; Percent complete: 94.1%; Average loss: 0.7930\n",
            "Iteration: 14121; Percent complete: 94.1%; Average loss: 0.9034\n",
            "Iteration: 14122; Percent complete: 94.1%; Average loss: 0.9046\n",
            "Iteration: 14123; Percent complete: 94.2%; Average loss: 0.9633\n",
            "Iteration: 14124; Percent complete: 94.2%; Average loss: 0.8046\n",
            "Iteration: 14125; Percent complete: 94.2%; Average loss: 0.7319\n",
            "Iteration: 14126; Percent complete: 94.2%; Average loss: 0.9327\n",
            "Iteration: 14127; Percent complete: 94.2%; Average loss: 0.8462\n",
            "Iteration: 14128; Percent complete: 94.2%; Average loss: 0.6895\n",
            "Iteration: 14129; Percent complete: 94.2%; Average loss: 0.7797\n",
            "Iteration: 14130; Percent complete: 94.2%; Average loss: 0.8283\n",
            "Iteration: 14131; Percent complete: 94.2%; Average loss: 0.7435\n",
            "Iteration: 14132; Percent complete: 94.2%; Average loss: 0.8455\n",
            "Iteration: 14133; Percent complete: 94.2%; Average loss: 0.8260\n",
            "Iteration: 14134; Percent complete: 94.2%; Average loss: 0.8241\n",
            "Iteration: 14135; Percent complete: 94.2%; Average loss: 0.7913\n",
            "Iteration: 14136; Percent complete: 94.2%; Average loss: 0.9220\n",
            "Iteration: 14137; Percent complete: 94.2%; Average loss: 0.8676\n",
            "Iteration: 14138; Percent complete: 94.3%; Average loss: 0.7569\n",
            "Iteration: 14139; Percent complete: 94.3%; Average loss: 0.8263\n",
            "Iteration: 14140; Percent complete: 94.3%; Average loss: 0.8055\n",
            "Iteration: 14141; Percent complete: 94.3%; Average loss: 0.7645\n",
            "Iteration: 14142; Percent complete: 94.3%; Average loss: 0.8073\n",
            "Iteration: 14143; Percent complete: 94.3%; Average loss: 0.9026\n",
            "Iteration: 14144; Percent complete: 94.3%; Average loss: 0.7232\n",
            "Iteration: 14145; Percent complete: 94.3%; Average loss: 0.8066\n",
            "Iteration: 14146; Percent complete: 94.3%; Average loss: 0.7077\n",
            "Iteration: 14147; Percent complete: 94.3%; Average loss: 0.8300\n",
            "Iteration: 14148; Percent complete: 94.3%; Average loss: 0.7074\n",
            "Iteration: 14149; Percent complete: 94.3%; Average loss: 0.7330\n",
            "Iteration: 14150; Percent complete: 94.3%; Average loss: 0.8035\n",
            "Iteration: 14151; Percent complete: 94.3%; Average loss: 0.8710\n",
            "Iteration: 14152; Percent complete: 94.3%; Average loss: 0.7711\n",
            "Iteration: 14153; Percent complete: 94.4%; Average loss: 0.8724\n",
            "Iteration: 14154; Percent complete: 94.4%; Average loss: 0.7046\n",
            "Iteration: 14155; Percent complete: 94.4%; Average loss: 0.7163\n",
            "Iteration: 14156; Percent complete: 94.4%; Average loss: 0.7636\n",
            "Iteration: 14157; Percent complete: 94.4%; Average loss: 0.7628\n",
            "Iteration: 14158; Percent complete: 94.4%; Average loss: 0.9007\n",
            "Iteration: 14159; Percent complete: 94.4%; Average loss: 0.7824\n",
            "Iteration: 14160; Percent complete: 94.4%; Average loss: 0.7446\n",
            "Iteration: 14161; Percent complete: 94.4%; Average loss: 0.7637\n",
            "Iteration: 14162; Percent complete: 94.4%; Average loss: 0.6919\n",
            "Iteration: 14163; Percent complete: 94.4%; Average loss: 0.8049\n",
            "Iteration: 14164; Percent complete: 94.4%; Average loss: 0.7335\n",
            "Iteration: 14165; Percent complete: 94.4%; Average loss: 0.8467\n",
            "Iteration: 14166; Percent complete: 94.4%; Average loss: 0.7485\n",
            "Iteration: 14167; Percent complete: 94.4%; Average loss: 0.9437\n",
            "Iteration: 14168; Percent complete: 94.5%; Average loss: 0.9402\n",
            "Iteration: 14169; Percent complete: 94.5%; Average loss: 0.8341\n",
            "Iteration: 14170; Percent complete: 94.5%; Average loss: 0.8973\n",
            "Iteration: 14171; Percent complete: 94.5%; Average loss: 0.7061\n",
            "Iteration: 14172; Percent complete: 94.5%; Average loss: 0.7281\n",
            "Iteration: 14173; Percent complete: 94.5%; Average loss: 0.7976\n",
            "Iteration: 14174; Percent complete: 94.5%; Average loss: 0.7305\n",
            "Iteration: 14175; Percent complete: 94.5%; Average loss: 0.7883\n",
            "Iteration: 14176; Percent complete: 94.5%; Average loss: 0.8894\n",
            "Iteration: 14177; Percent complete: 94.5%; Average loss: 0.8256\n",
            "Iteration: 14178; Percent complete: 94.5%; Average loss: 0.8194\n",
            "Iteration: 14179; Percent complete: 94.5%; Average loss: 0.8045\n",
            "Iteration: 14180; Percent complete: 94.5%; Average loss: 0.8222\n",
            "Iteration: 14181; Percent complete: 94.5%; Average loss: 0.7742\n",
            "Iteration: 14182; Percent complete: 94.5%; Average loss: 0.8948\n",
            "Iteration: 14183; Percent complete: 94.6%; Average loss: 0.6058\n",
            "Iteration: 14184; Percent complete: 94.6%; Average loss: 0.8318\n",
            "Iteration: 14185; Percent complete: 94.6%; Average loss: 0.8418\n",
            "Iteration: 14186; Percent complete: 94.6%; Average loss: 0.7108\n",
            "Iteration: 14187; Percent complete: 94.6%; Average loss: 0.7335\n",
            "Iteration: 14188; Percent complete: 94.6%; Average loss: 0.7869\n",
            "Iteration: 14189; Percent complete: 94.6%; Average loss: 0.7774\n",
            "Iteration: 14190; Percent complete: 94.6%; Average loss: 0.9131\n",
            "Iteration: 14191; Percent complete: 94.6%; Average loss: 0.7879\n",
            "Iteration: 14192; Percent complete: 94.6%; Average loss: 0.7769\n",
            "Iteration: 14193; Percent complete: 94.6%; Average loss: 0.8242\n",
            "Iteration: 14194; Percent complete: 94.6%; Average loss: 0.8153\n",
            "Iteration: 14195; Percent complete: 94.6%; Average loss: 0.7445\n",
            "Iteration: 14196; Percent complete: 94.6%; Average loss: 0.9444\n",
            "Iteration: 14197; Percent complete: 94.6%; Average loss: 0.8979\n",
            "Iteration: 14198; Percent complete: 94.7%; Average loss: 0.9080\n",
            "Iteration: 14199; Percent complete: 94.7%; Average loss: 0.6180\n",
            "Iteration: 14200; Percent complete: 94.7%; Average loss: 0.6985\n",
            "Iteration: 14201; Percent complete: 94.7%; Average loss: 0.9146\n",
            "Iteration: 14202; Percent complete: 94.7%; Average loss: 0.7383\n",
            "Iteration: 14203; Percent complete: 94.7%; Average loss: 0.6989\n",
            "Iteration: 14204; Percent complete: 94.7%; Average loss: 0.8918\n",
            "Iteration: 14205; Percent complete: 94.7%; Average loss: 0.8163\n",
            "Iteration: 14206; Percent complete: 94.7%; Average loss: 0.7957\n",
            "Iteration: 14207; Percent complete: 94.7%; Average loss: 0.7084\n",
            "Iteration: 14208; Percent complete: 94.7%; Average loss: 0.7640\n",
            "Iteration: 14209; Percent complete: 94.7%; Average loss: 0.7501\n",
            "Iteration: 14210; Percent complete: 94.7%; Average loss: 0.8978\n",
            "Iteration: 14211; Percent complete: 94.7%; Average loss: 0.7814\n",
            "Iteration: 14212; Percent complete: 94.7%; Average loss: 0.7374\n",
            "Iteration: 14213; Percent complete: 94.8%; Average loss: 0.7251\n",
            "Iteration: 14214; Percent complete: 94.8%; Average loss: 0.7248\n",
            "Iteration: 14215; Percent complete: 94.8%; Average loss: 0.7943\n",
            "Iteration: 14216; Percent complete: 94.8%; Average loss: 0.8108\n",
            "Iteration: 14217; Percent complete: 94.8%; Average loss: 0.8256\n",
            "Iteration: 14218; Percent complete: 94.8%; Average loss: 0.7412\n",
            "Iteration: 14219; Percent complete: 94.8%; Average loss: 0.8958\n",
            "Iteration: 14220; Percent complete: 94.8%; Average loss: 0.6533\n",
            "Iteration: 14221; Percent complete: 94.8%; Average loss: 0.8893\n",
            "Iteration: 14222; Percent complete: 94.8%; Average loss: 0.9802\n",
            "Iteration: 14223; Percent complete: 94.8%; Average loss: 0.8097\n",
            "Iteration: 14224; Percent complete: 94.8%; Average loss: 0.8049\n",
            "Iteration: 14225; Percent complete: 94.8%; Average loss: 0.6480\n",
            "Iteration: 14226; Percent complete: 94.8%; Average loss: 0.7312\n",
            "Iteration: 14227; Percent complete: 94.8%; Average loss: 0.8414\n",
            "Iteration: 14228; Percent complete: 94.9%; Average loss: 0.7752\n",
            "Iteration: 14229; Percent complete: 94.9%; Average loss: 0.7686\n",
            "Iteration: 14230; Percent complete: 94.9%; Average loss: 0.6670\n",
            "Iteration: 14231; Percent complete: 94.9%; Average loss: 0.9301\n",
            "Iteration: 14232; Percent complete: 94.9%; Average loss: 0.6795\n",
            "Iteration: 14233; Percent complete: 94.9%; Average loss: 0.8324\n",
            "Iteration: 14234; Percent complete: 94.9%; Average loss: 0.7974\n",
            "Iteration: 14235; Percent complete: 94.9%; Average loss: 0.6617\n",
            "Iteration: 14236; Percent complete: 94.9%; Average loss: 0.7198\n",
            "Iteration: 14237; Percent complete: 94.9%; Average loss: 0.7838\n",
            "Iteration: 14238; Percent complete: 94.9%; Average loss: 0.6580\n",
            "Iteration: 14239; Percent complete: 94.9%; Average loss: 0.7723\n",
            "Iteration: 14240; Percent complete: 94.9%; Average loss: 0.8703\n",
            "Iteration: 14241; Percent complete: 94.9%; Average loss: 0.8603\n",
            "Iteration: 14242; Percent complete: 94.9%; Average loss: 0.7345\n",
            "Iteration: 14243; Percent complete: 95.0%; Average loss: 0.8525\n",
            "Iteration: 14244; Percent complete: 95.0%; Average loss: 0.8912\n",
            "Iteration: 14245; Percent complete: 95.0%; Average loss: 0.7935\n",
            "Iteration: 14246; Percent complete: 95.0%; Average loss: 0.7093\n",
            "Iteration: 14247; Percent complete: 95.0%; Average loss: 0.7766\n",
            "Iteration: 14248; Percent complete: 95.0%; Average loss: 0.6993\n",
            "Iteration: 14249; Percent complete: 95.0%; Average loss: 0.8294\n",
            "Iteration: 14250; Percent complete: 95.0%; Average loss: 0.8837\n",
            "Iteration: 14251; Percent complete: 95.0%; Average loss: 0.8787\n",
            "Iteration: 14252; Percent complete: 95.0%; Average loss: 0.9031\n",
            "Iteration: 14253; Percent complete: 95.0%; Average loss: 0.9194\n",
            "Iteration: 14254; Percent complete: 95.0%; Average loss: 0.7591\n",
            "Iteration: 14255; Percent complete: 95.0%; Average loss: 0.7953\n",
            "Iteration: 14256; Percent complete: 95.0%; Average loss: 0.8129\n",
            "Iteration: 14257; Percent complete: 95.0%; Average loss: 0.8024\n",
            "Iteration: 14258; Percent complete: 95.1%; Average loss: 0.7223\n",
            "Iteration: 14259; Percent complete: 95.1%; Average loss: 0.9258\n",
            "Iteration: 14260; Percent complete: 95.1%; Average loss: 0.8413\n",
            "Iteration: 14261; Percent complete: 95.1%; Average loss: 0.7350\n",
            "Iteration: 14262; Percent complete: 95.1%; Average loss: 0.8563\n",
            "Iteration: 14263; Percent complete: 95.1%; Average loss: 0.8141\n",
            "Iteration: 14264; Percent complete: 95.1%; Average loss: 0.8221\n",
            "Iteration: 14265; Percent complete: 95.1%; Average loss: 0.8508\n",
            "Iteration: 14266; Percent complete: 95.1%; Average loss: 0.8589\n",
            "Iteration: 14267; Percent complete: 95.1%; Average loss: 0.8930\n",
            "Iteration: 14268; Percent complete: 95.1%; Average loss: 0.7602\n",
            "Iteration: 14269; Percent complete: 95.1%; Average loss: 0.7252\n",
            "Iteration: 14270; Percent complete: 95.1%; Average loss: 0.8578\n",
            "Iteration: 14271; Percent complete: 95.1%; Average loss: 0.6865\n",
            "Iteration: 14272; Percent complete: 95.1%; Average loss: 0.8523\n",
            "Iteration: 14273; Percent complete: 95.2%; Average loss: 0.8489\n",
            "Iteration: 14274; Percent complete: 95.2%; Average loss: 0.7463\n",
            "Iteration: 14275; Percent complete: 95.2%; Average loss: 0.9403\n",
            "Iteration: 14276; Percent complete: 95.2%; Average loss: 0.7450\n",
            "Iteration: 14277; Percent complete: 95.2%; Average loss: 0.7475\n",
            "Iteration: 14278; Percent complete: 95.2%; Average loss: 0.8611\n",
            "Iteration: 14279; Percent complete: 95.2%; Average loss: 0.6178\n",
            "Iteration: 14280; Percent complete: 95.2%; Average loss: 0.7446\n",
            "Iteration: 14281; Percent complete: 95.2%; Average loss: 0.7829\n",
            "Iteration: 14282; Percent complete: 95.2%; Average loss: 0.8050\n",
            "Iteration: 14283; Percent complete: 95.2%; Average loss: 0.7144\n",
            "Iteration: 14284; Percent complete: 95.2%; Average loss: 0.7734\n",
            "Iteration: 14285; Percent complete: 95.2%; Average loss: 0.9207\n",
            "Iteration: 14286; Percent complete: 95.2%; Average loss: 0.7140\n",
            "Iteration: 14287; Percent complete: 95.2%; Average loss: 0.8044\n",
            "Iteration: 14288; Percent complete: 95.3%; Average loss: 0.7625\n",
            "Iteration: 14289; Percent complete: 95.3%; Average loss: 0.8334\n",
            "Iteration: 14290; Percent complete: 95.3%; Average loss: 0.7585\n",
            "Iteration: 14291; Percent complete: 95.3%; Average loss: 0.8404\n",
            "Iteration: 14292; Percent complete: 95.3%; Average loss: 0.7535\n",
            "Iteration: 14293; Percent complete: 95.3%; Average loss: 0.7760\n",
            "Iteration: 14294; Percent complete: 95.3%; Average loss: 0.8360\n",
            "Iteration: 14295; Percent complete: 95.3%; Average loss: 0.7732\n",
            "Iteration: 14296; Percent complete: 95.3%; Average loss: 0.7196\n",
            "Iteration: 14297; Percent complete: 95.3%; Average loss: 0.8138\n",
            "Iteration: 14298; Percent complete: 95.3%; Average loss: 0.7243\n",
            "Iteration: 14299; Percent complete: 95.3%; Average loss: 0.7025\n",
            "Iteration: 14300; Percent complete: 95.3%; Average loss: 0.8082\n",
            "Iteration: 14301; Percent complete: 95.3%; Average loss: 0.7308\n",
            "Iteration: 14302; Percent complete: 95.3%; Average loss: 0.8080\n",
            "Iteration: 14303; Percent complete: 95.4%; Average loss: 0.6939\n",
            "Iteration: 14304; Percent complete: 95.4%; Average loss: 0.6273\n",
            "Iteration: 14305; Percent complete: 95.4%; Average loss: 0.8079\n",
            "Iteration: 14306; Percent complete: 95.4%; Average loss: 0.6774\n",
            "Iteration: 14307; Percent complete: 95.4%; Average loss: 0.6814\n",
            "Iteration: 14308; Percent complete: 95.4%; Average loss: 0.7067\n",
            "Iteration: 14309; Percent complete: 95.4%; Average loss: 0.8106\n",
            "Iteration: 14310; Percent complete: 95.4%; Average loss: 0.9195\n",
            "Iteration: 14311; Percent complete: 95.4%; Average loss: 0.7363\n",
            "Iteration: 14312; Percent complete: 95.4%; Average loss: 0.7973\n",
            "Iteration: 14313; Percent complete: 95.4%; Average loss: 0.7503\n",
            "Iteration: 14314; Percent complete: 95.4%; Average loss: 0.6746\n",
            "Iteration: 14315; Percent complete: 95.4%; Average loss: 0.7054\n",
            "Iteration: 14316; Percent complete: 95.4%; Average loss: 0.6996\n",
            "Iteration: 14317; Percent complete: 95.4%; Average loss: 0.9190\n",
            "Iteration: 14318; Percent complete: 95.5%; Average loss: 0.8274\n",
            "Iteration: 14319; Percent complete: 95.5%; Average loss: 0.8015\n",
            "Iteration: 14320; Percent complete: 95.5%; Average loss: 0.7241\n",
            "Iteration: 14321; Percent complete: 95.5%; Average loss: 0.7780\n",
            "Iteration: 14322; Percent complete: 95.5%; Average loss: 0.6870\n",
            "Iteration: 14323; Percent complete: 95.5%; Average loss: 0.8162\n",
            "Iteration: 14324; Percent complete: 95.5%; Average loss: 0.7736\n",
            "Iteration: 14325; Percent complete: 95.5%; Average loss: 0.7813\n",
            "Iteration: 14326; Percent complete: 95.5%; Average loss: 0.8505\n",
            "Iteration: 14327; Percent complete: 95.5%; Average loss: 0.7366\n",
            "Iteration: 14328; Percent complete: 95.5%; Average loss: 0.8584\n",
            "Iteration: 14329; Percent complete: 95.5%; Average loss: 0.6775\n",
            "Iteration: 14330; Percent complete: 95.5%; Average loss: 0.7105\n",
            "Iteration: 14331; Percent complete: 95.5%; Average loss: 0.7192\n",
            "Iteration: 14332; Percent complete: 95.5%; Average loss: 0.7920\n",
            "Iteration: 14333; Percent complete: 95.6%; Average loss: 0.9642\n",
            "Iteration: 14334; Percent complete: 95.6%; Average loss: 0.8498\n",
            "Iteration: 14335; Percent complete: 95.6%; Average loss: 0.8129\n",
            "Iteration: 14336; Percent complete: 95.6%; Average loss: 0.7737\n",
            "Iteration: 14337; Percent complete: 95.6%; Average loss: 0.8228\n",
            "Iteration: 14338; Percent complete: 95.6%; Average loss: 0.6681\n",
            "Iteration: 14339; Percent complete: 95.6%; Average loss: 0.8767\n",
            "Iteration: 14340; Percent complete: 95.6%; Average loss: 0.7816\n",
            "Iteration: 14341; Percent complete: 95.6%; Average loss: 0.8767\n",
            "Iteration: 14342; Percent complete: 95.6%; Average loss: 0.8517\n",
            "Iteration: 14343; Percent complete: 95.6%; Average loss: 0.6649\n",
            "Iteration: 14344; Percent complete: 95.6%; Average loss: 0.9494\n",
            "Iteration: 14345; Percent complete: 95.6%; Average loss: 0.8349\n",
            "Iteration: 14346; Percent complete: 95.6%; Average loss: 0.6918\n",
            "Iteration: 14347; Percent complete: 95.6%; Average loss: 0.6907\n",
            "Iteration: 14348; Percent complete: 95.7%; Average loss: 0.7914\n",
            "Iteration: 14349; Percent complete: 95.7%; Average loss: 0.8001\n",
            "Iteration: 14350; Percent complete: 95.7%; Average loss: 0.7608\n",
            "Iteration: 14351; Percent complete: 95.7%; Average loss: 0.7503\n",
            "Iteration: 14352; Percent complete: 95.7%; Average loss: 0.7207\n",
            "Iteration: 14353; Percent complete: 95.7%; Average loss: 0.6631\n",
            "Iteration: 14354; Percent complete: 95.7%; Average loss: 0.8221\n",
            "Iteration: 14355; Percent complete: 95.7%; Average loss: 0.7146\n",
            "Iteration: 14356; Percent complete: 95.7%; Average loss: 0.6438\n",
            "Iteration: 14357; Percent complete: 95.7%; Average loss: 0.8430\n",
            "Iteration: 14358; Percent complete: 95.7%; Average loss: 0.7227\n",
            "Iteration: 14359; Percent complete: 95.7%; Average loss: 0.8348\n",
            "Iteration: 14360; Percent complete: 95.7%; Average loss: 0.7384\n",
            "Iteration: 14361; Percent complete: 95.7%; Average loss: 0.6765\n",
            "Iteration: 14362; Percent complete: 95.7%; Average loss: 0.6624\n",
            "Iteration: 14363; Percent complete: 95.8%; Average loss: 0.8591\n",
            "Iteration: 14364; Percent complete: 95.8%; Average loss: 0.7524\n",
            "Iteration: 14365; Percent complete: 95.8%; Average loss: 0.7263\n",
            "Iteration: 14366; Percent complete: 95.8%; Average loss: 0.7456\n",
            "Iteration: 14367; Percent complete: 95.8%; Average loss: 0.7687\n",
            "Iteration: 14368; Percent complete: 95.8%; Average loss: 0.7133\n",
            "Iteration: 14369; Percent complete: 95.8%; Average loss: 0.8748\n",
            "Iteration: 14370; Percent complete: 95.8%; Average loss: 0.7240\n",
            "Iteration: 14371; Percent complete: 95.8%; Average loss: 1.0213\n",
            "Iteration: 14372; Percent complete: 95.8%; Average loss: 0.7788\n",
            "Iteration: 14373; Percent complete: 95.8%; Average loss: 0.6665\n",
            "Iteration: 14374; Percent complete: 95.8%; Average loss: 0.6190\n",
            "Iteration: 14375; Percent complete: 95.8%; Average loss: 0.6003\n",
            "Iteration: 14376; Percent complete: 95.8%; Average loss: 0.7760\n",
            "Iteration: 14377; Percent complete: 95.8%; Average loss: 0.7219\n",
            "Iteration: 14378; Percent complete: 95.9%; Average loss: 0.7059\n",
            "Iteration: 14379; Percent complete: 95.9%; Average loss: 0.8078\n",
            "Iteration: 14380; Percent complete: 95.9%; Average loss: 0.7340\n",
            "Iteration: 14381; Percent complete: 95.9%; Average loss: 0.7486\n",
            "Iteration: 14382; Percent complete: 95.9%; Average loss: 0.9056\n",
            "Iteration: 14383; Percent complete: 95.9%; Average loss: 0.8023\n",
            "Iteration: 14384; Percent complete: 95.9%; Average loss: 0.7670\n",
            "Iteration: 14385; Percent complete: 95.9%; Average loss: 0.8895\n",
            "Iteration: 14386; Percent complete: 95.9%; Average loss: 0.8155\n",
            "Iteration: 14387; Percent complete: 95.9%; Average loss: 0.9169\n",
            "Iteration: 14388; Percent complete: 95.9%; Average loss: 0.7397\n",
            "Iteration: 14389; Percent complete: 95.9%; Average loss: 0.6764\n",
            "Iteration: 14390; Percent complete: 95.9%; Average loss: 0.6718\n",
            "Iteration: 14391; Percent complete: 95.9%; Average loss: 0.7549\n",
            "Iteration: 14392; Percent complete: 95.9%; Average loss: 0.6905\n",
            "Iteration: 14393; Percent complete: 96.0%; Average loss: 0.7274\n",
            "Iteration: 14394; Percent complete: 96.0%; Average loss: 0.7554\n",
            "Iteration: 14395; Percent complete: 96.0%; Average loss: 0.6671\n",
            "Iteration: 14396; Percent complete: 96.0%; Average loss: 0.7884\n",
            "Iteration: 14397; Percent complete: 96.0%; Average loss: 0.7105\n",
            "Iteration: 14398; Percent complete: 96.0%; Average loss: 0.7404\n",
            "Iteration: 14399; Percent complete: 96.0%; Average loss: 0.8178\n",
            "Iteration: 14400; Percent complete: 96.0%; Average loss: 0.7214\n",
            "Iteration: 14401; Percent complete: 96.0%; Average loss: 0.7980\n",
            "Iteration: 14402; Percent complete: 96.0%; Average loss: 0.7427\n",
            "Iteration: 14403; Percent complete: 96.0%; Average loss: 0.7516\n",
            "Iteration: 14404; Percent complete: 96.0%; Average loss: 0.7192\n",
            "Iteration: 14405; Percent complete: 96.0%; Average loss: 0.7472\n",
            "Iteration: 14406; Percent complete: 96.0%; Average loss: 0.6762\n",
            "Iteration: 14407; Percent complete: 96.0%; Average loss: 0.8838\n",
            "Iteration: 14408; Percent complete: 96.1%; Average loss: 0.7263\n",
            "Iteration: 14409; Percent complete: 96.1%; Average loss: 0.7260\n",
            "Iteration: 14410; Percent complete: 96.1%; Average loss: 0.8301\n",
            "Iteration: 14411; Percent complete: 96.1%; Average loss: 0.8139\n",
            "Iteration: 14412; Percent complete: 96.1%; Average loss: 0.7808\n",
            "Iteration: 14413; Percent complete: 96.1%; Average loss: 0.7731\n",
            "Iteration: 14414; Percent complete: 96.1%; Average loss: 0.7251\n",
            "Iteration: 14415; Percent complete: 96.1%; Average loss: 0.6571\n",
            "Iteration: 14416; Percent complete: 96.1%; Average loss: 0.8356\n",
            "Iteration: 14417; Percent complete: 96.1%; Average loss: 0.7563\n",
            "Iteration: 14418; Percent complete: 96.1%; Average loss: 0.8008\n",
            "Iteration: 14419; Percent complete: 96.1%; Average loss: 0.7908\n",
            "Iteration: 14420; Percent complete: 96.1%; Average loss: 0.8088\n",
            "Iteration: 14421; Percent complete: 96.1%; Average loss: 0.8608\n",
            "Iteration: 14422; Percent complete: 96.1%; Average loss: 0.8450\n",
            "Iteration: 14423; Percent complete: 96.2%; Average loss: 0.6597\n",
            "Iteration: 14424; Percent complete: 96.2%; Average loss: 0.8401\n",
            "Iteration: 14425; Percent complete: 96.2%; Average loss: 0.6627\n",
            "Iteration: 14426; Percent complete: 96.2%; Average loss: 0.7591\n",
            "Iteration: 14427; Percent complete: 96.2%; Average loss: 0.8447\n",
            "Iteration: 14428; Percent complete: 96.2%; Average loss: 0.7843\n",
            "Iteration: 14429; Percent complete: 96.2%; Average loss: 0.8157\n",
            "Iteration: 14430; Percent complete: 96.2%; Average loss: 0.7042\n",
            "Iteration: 14431; Percent complete: 96.2%; Average loss: 0.7075\n",
            "Iteration: 14432; Percent complete: 96.2%; Average loss: 0.7658\n",
            "Iteration: 14433; Percent complete: 96.2%; Average loss: 0.6871\n",
            "Iteration: 14434; Percent complete: 96.2%; Average loss: 0.6266\n",
            "Iteration: 14435; Percent complete: 96.2%; Average loss: 0.6944\n",
            "Iteration: 14436; Percent complete: 96.2%; Average loss: 0.7553\n",
            "Iteration: 14437; Percent complete: 96.2%; Average loss: 0.8018\n",
            "Iteration: 14438; Percent complete: 96.3%; Average loss: 0.6960\n",
            "Iteration: 14439; Percent complete: 96.3%; Average loss: 0.7270\n",
            "Iteration: 14440; Percent complete: 96.3%; Average loss: 0.7551\n",
            "Iteration: 14441; Percent complete: 96.3%; Average loss: 0.6774\n",
            "Iteration: 14442; Percent complete: 96.3%; Average loss: 0.8533\n",
            "Iteration: 14443; Percent complete: 96.3%; Average loss: 0.7103\n",
            "Iteration: 14444; Percent complete: 96.3%; Average loss: 0.7785\n",
            "Iteration: 14445; Percent complete: 96.3%; Average loss: 0.5989\n",
            "Iteration: 14446; Percent complete: 96.3%; Average loss: 0.8035\n",
            "Iteration: 14447; Percent complete: 96.3%; Average loss: 0.7138\n",
            "Iteration: 14448; Percent complete: 96.3%; Average loss: 0.7493\n",
            "Iteration: 14449; Percent complete: 96.3%; Average loss: 0.6960\n",
            "Iteration: 14450; Percent complete: 96.3%; Average loss: 0.8461\n",
            "Iteration: 14451; Percent complete: 96.3%; Average loss: 0.7211\n",
            "Iteration: 14452; Percent complete: 96.3%; Average loss: 0.7606\n",
            "Iteration: 14453; Percent complete: 96.4%; Average loss: 0.8039\n",
            "Iteration: 14454; Percent complete: 96.4%; Average loss: 0.6723\n",
            "Iteration: 14455; Percent complete: 96.4%; Average loss: 0.6878\n",
            "Iteration: 14456; Percent complete: 96.4%; Average loss: 0.7047\n",
            "Iteration: 14457; Percent complete: 96.4%; Average loss: 0.8516\n",
            "Iteration: 14458; Percent complete: 96.4%; Average loss: 0.7720\n",
            "Iteration: 14459; Percent complete: 96.4%; Average loss: 0.7600\n",
            "Iteration: 14460; Percent complete: 96.4%; Average loss: 0.6686\n",
            "Iteration: 14461; Percent complete: 96.4%; Average loss: 0.6552\n",
            "Iteration: 14462; Percent complete: 96.4%; Average loss: 0.6974\n",
            "Iteration: 14463; Percent complete: 96.4%; Average loss: 0.9874\n",
            "Iteration: 14464; Percent complete: 96.4%; Average loss: 0.8967\n",
            "Iteration: 14465; Percent complete: 96.4%; Average loss: 0.6817\n",
            "Iteration: 14466; Percent complete: 96.4%; Average loss: 0.7087\n",
            "Iteration: 14467; Percent complete: 96.4%; Average loss: 0.6901\n",
            "Iteration: 14468; Percent complete: 96.5%; Average loss: 0.7156\n",
            "Iteration: 14469; Percent complete: 96.5%; Average loss: 0.7459\n",
            "Iteration: 14470; Percent complete: 96.5%; Average loss: 0.6566\n",
            "Iteration: 14471; Percent complete: 96.5%; Average loss: 0.7071\n",
            "Iteration: 14472; Percent complete: 96.5%; Average loss: 0.6063\n",
            "Iteration: 14473; Percent complete: 96.5%; Average loss: 0.7438\n",
            "Iteration: 14474; Percent complete: 96.5%; Average loss: 0.9284\n",
            "Iteration: 14475; Percent complete: 96.5%; Average loss: 0.7568\n",
            "Iteration: 14476; Percent complete: 96.5%; Average loss: 0.8188\n",
            "Iteration: 14477; Percent complete: 96.5%; Average loss: 0.9703\n",
            "Iteration: 14478; Percent complete: 96.5%; Average loss: 0.7780\n",
            "Iteration: 14479; Percent complete: 96.5%; Average loss: 0.8761\n",
            "Iteration: 14480; Percent complete: 96.5%; Average loss: 0.6156\n",
            "Iteration: 14481; Percent complete: 96.5%; Average loss: 0.7470\n",
            "Iteration: 14482; Percent complete: 96.5%; Average loss: 0.7071\n",
            "Iteration: 14483; Percent complete: 96.6%; Average loss: 0.7960\n",
            "Iteration: 14484; Percent complete: 96.6%; Average loss: 0.8683\n",
            "Iteration: 14485; Percent complete: 96.6%; Average loss: 0.7195\n",
            "Iteration: 14486; Percent complete: 96.6%; Average loss: 0.6769\n",
            "Iteration: 14487; Percent complete: 96.6%; Average loss: 0.7392\n",
            "Iteration: 14488; Percent complete: 96.6%; Average loss: 0.7620\n",
            "Iteration: 14489; Percent complete: 96.6%; Average loss: 0.6818\n",
            "Iteration: 14490; Percent complete: 96.6%; Average loss: 0.6995\n",
            "Iteration: 14491; Percent complete: 96.6%; Average loss: 0.6572\n",
            "Iteration: 14492; Percent complete: 96.6%; Average loss: 0.8410\n",
            "Iteration: 14493; Percent complete: 96.6%; Average loss: 0.7209\n",
            "Iteration: 14494; Percent complete: 96.6%; Average loss: 0.7111\n",
            "Iteration: 14495; Percent complete: 96.6%; Average loss: 0.6849\n",
            "Iteration: 14496; Percent complete: 96.6%; Average loss: 0.8959\n",
            "Iteration: 14497; Percent complete: 96.6%; Average loss: 0.7069\n",
            "Iteration: 14498; Percent complete: 96.7%; Average loss: 0.8624\n",
            "Iteration: 14499; Percent complete: 96.7%; Average loss: 0.7545\n",
            "Iteration: 14500; Percent complete: 96.7%; Average loss: 0.7665\n",
            "Iteration: 14501; Percent complete: 96.7%; Average loss: 0.7792\n",
            "Iteration: 14502; Percent complete: 96.7%; Average loss: 0.8057\n",
            "Iteration: 14503; Percent complete: 96.7%; Average loss: 0.8272\n",
            "Iteration: 14504; Percent complete: 96.7%; Average loss: 0.7763\n",
            "Iteration: 14505; Percent complete: 96.7%; Average loss: 0.8963\n",
            "Iteration: 14506; Percent complete: 96.7%; Average loss: 0.7825\n",
            "Iteration: 14507; Percent complete: 96.7%; Average loss: 0.7602\n",
            "Iteration: 14508; Percent complete: 96.7%; Average loss: 0.8133\n",
            "Iteration: 14509; Percent complete: 96.7%; Average loss: 0.7914\n",
            "Iteration: 14510; Percent complete: 96.7%; Average loss: 0.8967\n",
            "Iteration: 14511; Percent complete: 96.7%; Average loss: 0.6965\n",
            "Iteration: 14512; Percent complete: 96.7%; Average loss: 0.7931\n",
            "Iteration: 14513; Percent complete: 96.8%; Average loss: 0.8413\n",
            "Iteration: 14514; Percent complete: 96.8%; Average loss: 0.8316\n",
            "Iteration: 14515; Percent complete: 96.8%; Average loss: 0.7135\n",
            "Iteration: 14516; Percent complete: 96.8%; Average loss: 0.8495\n",
            "Iteration: 14517; Percent complete: 96.8%; Average loss: 0.7618\n",
            "Iteration: 14518; Percent complete: 96.8%; Average loss: 0.6422\n",
            "Iteration: 14519; Percent complete: 96.8%; Average loss: 0.8726\n",
            "Iteration: 14520; Percent complete: 96.8%; Average loss: 0.7632\n",
            "Iteration: 14521; Percent complete: 96.8%; Average loss: 0.5692\n",
            "Iteration: 14522; Percent complete: 96.8%; Average loss: 0.6939\n",
            "Iteration: 14523; Percent complete: 96.8%; Average loss: 0.6852\n",
            "Iteration: 14524; Percent complete: 96.8%; Average loss: 0.7236\n",
            "Iteration: 14525; Percent complete: 96.8%; Average loss: 0.8150\n",
            "Iteration: 14526; Percent complete: 96.8%; Average loss: 0.9286\n",
            "Iteration: 14527; Percent complete: 96.8%; Average loss: 0.9141\n",
            "Iteration: 14528; Percent complete: 96.9%; Average loss: 0.8082\n",
            "Iteration: 14529; Percent complete: 96.9%; Average loss: 0.7556\n",
            "Iteration: 14530; Percent complete: 96.9%; Average loss: 0.8197\n",
            "Iteration: 14531; Percent complete: 96.9%; Average loss: 0.7495\n",
            "Iteration: 14532; Percent complete: 96.9%; Average loss: 0.7821\n",
            "Iteration: 14533; Percent complete: 96.9%; Average loss: 0.8799\n",
            "Iteration: 14534; Percent complete: 96.9%; Average loss: 0.7427\n",
            "Iteration: 14535; Percent complete: 96.9%; Average loss: 0.6738\n",
            "Iteration: 14536; Percent complete: 96.9%; Average loss: 0.6242\n",
            "Iteration: 14537; Percent complete: 96.9%; Average loss: 0.8687\n",
            "Iteration: 14538; Percent complete: 96.9%; Average loss: 0.7039\n",
            "Iteration: 14539; Percent complete: 96.9%; Average loss: 0.7147\n",
            "Iteration: 14540; Percent complete: 96.9%; Average loss: 0.8400\n",
            "Iteration: 14541; Percent complete: 96.9%; Average loss: 0.7336\n",
            "Iteration: 14542; Percent complete: 96.9%; Average loss: 0.8831\n",
            "Iteration: 14543; Percent complete: 97.0%; Average loss: 0.8034\n",
            "Iteration: 14544; Percent complete: 97.0%; Average loss: 0.7912\n",
            "Iteration: 14545; Percent complete: 97.0%; Average loss: 0.6833\n",
            "Iteration: 14546; Percent complete: 97.0%; Average loss: 0.8219\n",
            "Iteration: 14547; Percent complete: 97.0%; Average loss: 0.7322\n",
            "Iteration: 14548; Percent complete: 97.0%; Average loss: 0.6828\n",
            "Iteration: 14549; Percent complete: 97.0%; Average loss: 0.6699\n",
            "Iteration: 14550; Percent complete: 97.0%; Average loss: 0.7450\n",
            "Iteration: 14551; Percent complete: 97.0%; Average loss: 0.7522\n",
            "Iteration: 14552; Percent complete: 97.0%; Average loss: 0.6499\n",
            "Iteration: 14553; Percent complete: 97.0%; Average loss: 0.7546\n",
            "Iteration: 14554; Percent complete: 97.0%; Average loss: 0.8119\n",
            "Iteration: 14555; Percent complete: 97.0%; Average loss: 0.7597\n",
            "Iteration: 14556; Percent complete: 97.0%; Average loss: 0.6617\n",
            "Iteration: 14557; Percent complete: 97.0%; Average loss: 0.8234\n",
            "Iteration: 14558; Percent complete: 97.1%; Average loss: 0.5309\n",
            "Iteration: 14559; Percent complete: 97.1%; Average loss: 0.7449\n",
            "Iteration: 14560; Percent complete: 97.1%; Average loss: 0.7530\n",
            "Iteration: 14561; Percent complete: 97.1%; Average loss: 0.6891\n",
            "Iteration: 14562; Percent complete: 97.1%; Average loss: 0.6506\n",
            "Iteration: 14563; Percent complete: 97.1%; Average loss: 0.8161\n",
            "Iteration: 14564; Percent complete: 97.1%; Average loss: 0.7388\n",
            "Iteration: 14565; Percent complete: 97.1%; Average loss: 0.7491\n",
            "Iteration: 14566; Percent complete: 97.1%; Average loss: 0.8551\n",
            "Iteration: 14567; Percent complete: 97.1%; Average loss: 0.9872\n",
            "Iteration: 14568; Percent complete: 97.1%; Average loss: 0.6076\n",
            "Iteration: 14569; Percent complete: 97.1%; Average loss: 0.7482\n",
            "Iteration: 14570; Percent complete: 97.1%; Average loss: 0.8737\n",
            "Iteration: 14571; Percent complete: 97.1%; Average loss: 0.6427\n",
            "Iteration: 14572; Percent complete: 97.1%; Average loss: 0.8335\n",
            "Iteration: 14573; Percent complete: 97.2%; Average loss: 0.7523\n",
            "Iteration: 14574; Percent complete: 97.2%; Average loss: 0.8757\n",
            "Iteration: 14575; Percent complete: 97.2%; Average loss: 0.7977\n",
            "Iteration: 14576; Percent complete: 97.2%; Average loss: 0.7449\n",
            "Iteration: 14577; Percent complete: 97.2%; Average loss: 0.6783\n",
            "Iteration: 14578; Percent complete: 97.2%; Average loss: 0.7785\n",
            "Iteration: 14579; Percent complete: 97.2%; Average loss: 0.7116\n",
            "Iteration: 14580; Percent complete: 97.2%; Average loss: 0.7814\n",
            "Iteration: 14581; Percent complete: 97.2%; Average loss: 0.8242\n",
            "Iteration: 14582; Percent complete: 97.2%; Average loss: 0.6844\n",
            "Iteration: 14583; Percent complete: 97.2%; Average loss: 0.8462\n",
            "Iteration: 14584; Percent complete: 97.2%; Average loss: 0.7749\n",
            "Iteration: 14585; Percent complete: 97.2%; Average loss: 0.8783\n",
            "Iteration: 14586; Percent complete: 97.2%; Average loss: 0.7278\n",
            "Iteration: 14587; Percent complete: 97.2%; Average loss: 0.5777\n",
            "Iteration: 14588; Percent complete: 97.3%; Average loss: 0.8224\n",
            "Iteration: 14589; Percent complete: 97.3%; Average loss: 0.7481\n",
            "Iteration: 14590; Percent complete: 97.3%; Average loss: 0.8038\n",
            "Iteration: 14591; Percent complete: 97.3%; Average loss: 0.7150\n",
            "Iteration: 14592; Percent complete: 97.3%; Average loss: 0.7798\n",
            "Iteration: 14593; Percent complete: 97.3%; Average loss: 0.7840\n",
            "Iteration: 14594; Percent complete: 97.3%; Average loss: 0.7404\n",
            "Iteration: 14595; Percent complete: 97.3%; Average loss: 0.7670\n",
            "Iteration: 14596; Percent complete: 97.3%; Average loss: 0.6793\n",
            "Iteration: 14597; Percent complete: 97.3%; Average loss: 0.7860\n",
            "Iteration: 14598; Percent complete: 97.3%; Average loss: 0.7373\n",
            "Iteration: 14599; Percent complete: 97.3%; Average loss: 0.7517\n",
            "Iteration: 14600; Percent complete: 97.3%; Average loss: 0.6826\n",
            "Iteration: 14601; Percent complete: 97.3%; Average loss: 0.6250\n",
            "Iteration: 14602; Percent complete: 97.3%; Average loss: 0.7123\n",
            "Iteration: 14603; Percent complete: 97.4%; Average loss: 0.7278\n",
            "Iteration: 14604; Percent complete: 97.4%; Average loss: 0.7049\n",
            "Iteration: 14605; Percent complete: 97.4%; Average loss: 0.7387\n",
            "Iteration: 14606; Percent complete: 97.4%; Average loss: 0.7801\n",
            "Iteration: 14607; Percent complete: 97.4%; Average loss: 0.7511\n",
            "Iteration: 14608; Percent complete: 97.4%; Average loss: 0.8745\n",
            "Iteration: 14609; Percent complete: 97.4%; Average loss: 0.7782\n",
            "Iteration: 14610; Percent complete: 97.4%; Average loss: 0.7752\n",
            "Iteration: 14611; Percent complete: 97.4%; Average loss: 0.8129\n",
            "Iteration: 14612; Percent complete: 97.4%; Average loss: 0.6711\n",
            "Iteration: 14613; Percent complete: 97.4%; Average loss: 0.8459\n",
            "Iteration: 14614; Percent complete: 97.4%; Average loss: 0.8982\n",
            "Iteration: 14615; Percent complete: 97.4%; Average loss: 0.6548\n",
            "Iteration: 14616; Percent complete: 97.4%; Average loss: 0.6140\n",
            "Iteration: 14617; Percent complete: 97.4%; Average loss: 0.8385\n",
            "Iteration: 14618; Percent complete: 97.5%; Average loss: 0.7018\n",
            "Iteration: 14619; Percent complete: 97.5%; Average loss: 0.8874\n",
            "Iteration: 14620; Percent complete: 97.5%; Average loss: 0.6941\n",
            "Iteration: 14621; Percent complete: 97.5%; Average loss: 0.7986\n",
            "Iteration: 14622; Percent complete: 97.5%; Average loss: 0.6973\n",
            "Iteration: 14623; Percent complete: 97.5%; Average loss: 0.5988\n",
            "Iteration: 14624; Percent complete: 97.5%; Average loss: 0.7825\n",
            "Iteration: 14625; Percent complete: 97.5%; Average loss: 0.6360\n",
            "Iteration: 14626; Percent complete: 97.5%; Average loss: 0.5981\n",
            "Iteration: 14627; Percent complete: 97.5%; Average loss: 0.7702\n",
            "Iteration: 14628; Percent complete: 97.5%; Average loss: 0.7675\n",
            "Iteration: 14629; Percent complete: 97.5%; Average loss: 0.7345\n",
            "Iteration: 14630; Percent complete: 97.5%; Average loss: 0.7814\n",
            "Iteration: 14631; Percent complete: 97.5%; Average loss: 0.7215\n",
            "Iteration: 14632; Percent complete: 97.5%; Average loss: 0.6988\n",
            "Iteration: 14633; Percent complete: 97.6%; Average loss: 0.8325\n",
            "Iteration: 14634; Percent complete: 97.6%; Average loss: 0.8316\n",
            "Iteration: 14635; Percent complete: 97.6%; Average loss: 0.7840\n",
            "Iteration: 14636; Percent complete: 97.6%; Average loss: 0.6876\n",
            "Iteration: 14637; Percent complete: 97.6%; Average loss: 0.6549\n",
            "Iteration: 14638; Percent complete: 97.6%; Average loss: 0.7108\n",
            "Iteration: 14639; Percent complete: 97.6%; Average loss: 0.8255\n",
            "Iteration: 14640; Percent complete: 97.6%; Average loss: 0.7493\n",
            "Iteration: 14641; Percent complete: 97.6%; Average loss: 0.8085\n",
            "Iteration: 14642; Percent complete: 97.6%; Average loss: 0.7875\n",
            "Iteration: 14643; Percent complete: 97.6%; Average loss: 0.8186\n",
            "Iteration: 14644; Percent complete: 97.6%; Average loss: 0.7333\n",
            "Iteration: 14645; Percent complete: 97.6%; Average loss: 0.7269\n",
            "Iteration: 14646; Percent complete: 97.6%; Average loss: 0.7879\n",
            "Iteration: 14647; Percent complete: 97.6%; Average loss: 0.8379\n",
            "Iteration: 14648; Percent complete: 97.7%; Average loss: 0.8146\n",
            "Iteration: 14649; Percent complete: 97.7%; Average loss: 0.6413\n",
            "Iteration: 14650; Percent complete: 97.7%; Average loss: 0.7113\n",
            "Iteration: 14651; Percent complete: 97.7%; Average loss: 0.7011\n",
            "Iteration: 14652; Percent complete: 97.7%; Average loss: 0.8711\n",
            "Iteration: 14653; Percent complete: 97.7%; Average loss: 0.9001\n",
            "Iteration: 14654; Percent complete: 97.7%; Average loss: 0.8062\n",
            "Iteration: 14655; Percent complete: 97.7%; Average loss: 0.7848\n",
            "Iteration: 14656; Percent complete: 97.7%; Average loss: 0.6368\n",
            "Iteration: 14657; Percent complete: 97.7%; Average loss: 0.7381\n",
            "Iteration: 14658; Percent complete: 97.7%; Average loss: 0.6788\n",
            "Iteration: 14659; Percent complete: 97.7%; Average loss: 0.6101\n",
            "Iteration: 14660; Percent complete: 97.7%; Average loss: 0.8014\n",
            "Iteration: 14661; Percent complete: 97.7%; Average loss: 0.7891\n",
            "Iteration: 14662; Percent complete: 97.7%; Average loss: 0.6682\n",
            "Iteration: 14663; Percent complete: 97.8%; Average loss: 0.6120\n",
            "Iteration: 14664; Percent complete: 97.8%; Average loss: 0.6807\n",
            "Iteration: 14665; Percent complete: 97.8%; Average loss: 0.6455\n",
            "Iteration: 14666; Percent complete: 97.8%; Average loss: 0.7225\n",
            "Iteration: 14667; Percent complete: 97.8%; Average loss: 0.6969\n",
            "Iteration: 14668; Percent complete: 97.8%; Average loss: 0.6560\n",
            "Iteration: 14669; Percent complete: 97.8%; Average loss: 0.6894\n",
            "Iteration: 14670; Percent complete: 97.8%; Average loss: 0.7038\n",
            "Iteration: 14671; Percent complete: 97.8%; Average loss: 0.6829\n",
            "Iteration: 14672; Percent complete: 97.8%; Average loss: 0.7588\n",
            "Iteration: 14673; Percent complete: 97.8%; Average loss: 0.6795\n",
            "Iteration: 14674; Percent complete: 97.8%; Average loss: 0.7802\n",
            "Iteration: 14675; Percent complete: 97.8%; Average loss: 0.8037\n",
            "Iteration: 14676; Percent complete: 97.8%; Average loss: 0.8172\n",
            "Iteration: 14677; Percent complete: 97.8%; Average loss: 0.7703\n",
            "Iteration: 14678; Percent complete: 97.9%; Average loss: 0.5603\n",
            "Iteration: 14679; Percent complete: 97.9%; Average loss: 0.7460\n",
            "Iteration: 14680; Percent complete: 97.9%; Average loss: 0.7464\n",
            "Iteration: 14681; Percent complete: 97.9%; Average loss: 0.7452\n",
            "Iteration: 14682; Percent complete: 97.9%; Average loss: 0.7952\n",
            "Iteration: 14683; Percent complete: 97.9%; Average loss: 0.7615\n",
            "Iteration: 14684; Percent complete: 97.9%; Average loss: 0.7745\n",
            "Iteration: 14685; Percent complete: 97.9%; Average loss: 0.8177\n",
            "Iteration: 14686; Percent complete: 97.9%; Average loss: 0.6252\n",
            "Iteration: 14687; Percent complete: 97.9%; Average loss: 0.7548\n",
            "Iteration: 14688; Percent complete: 97.9%; Average loss: 0.7514\n",
            "Iteration: 14689; Percent complete: 97.9%; Average loss: 0.7587\n",
            "Iteration: 14690; Percent complete: 97.9%; Average loss: 0.6977\n",
            "Iteration: 14691; Percent complete: 97.9%; Average loss: 0.7615\n",
            "Iteration: 14692; Percent complete: 97.9%; Average loss: 0.7367\n",
            "Iteration: 14693; Percent complete: 98.0%; Average loss: 0.8452\n",
            "Iteration: 14694; Percent complete: 98.0%; Average loss: 0.6526\n",
            "Iteration: 14695; Percent complete: 98.0%; Average loss: 0.9158\n",
            "Iteration: 14696; Percent complete: 98.0%; Average loss: 0.6399\n",
            "Iteration: 14697; Percent complete: 98.0%; Average loss: 0.9001\n",
            "Iteration: 14698; Percent complete: 98.0%; Average loss: 0.7566\n",
            "Iteration: 14699; Percent complete: 98.0%; Average loss: 0.7999\n",
            "Iteration: 14700; Percent complete: 98.0%; Average loss: 0.6550\n",
            "Iteration: 14701; Percent complete: 98.0%; Average loss: 0.7115\n",
            "Iteration: 14702; Percent complete: 98.0%; Average loss: 0.8245\n",
            "Iteration: 14703; Percent complete: 98.0%; Average loss: 0.7287\n",
            "Iteration: 14704; Percent complete: 98.0%; Average loss: 0.7764\n",
            "Iteration: 14705; Percent complete: 98.0%; Average loss: 0.6913\n",
            "Iteration: 14706; Percent complete: 98.0%; Average loss: 0.8133\n",
            "Iteration: 14707; Percent complete: 98.0%; Average loss: 0.7861\n",
            "Iteration: 14708; Percent complete: 98.1%; Average loss: 0.6710\n",
            "Iteration: 14709; Percent complete: 98.1%; Average loss: 0.7661\n",
            "Iteration: 14710; Percent complete: 98.1%; Average loss: 0.9049\n",
            "Iteration: 14711; Percent complete: 98.1%; Average loss: 0.7011\n",
            "Iteration: 14712; Percent complete: 98.1%; Average loss: 0.7471\n",
            "Iteration: 14713; Percent complete: 98.1%; Average loss: 0.5632\n",
            "Iteration: 14714; Percent complete: 98.1%; Average loss: 0.5673\n",
            "Iteration: 14715; Percent complete: 98.1%; Average loss: 0.7966\n",
            "Iteration: 14716; Percent complete: 98.1%; Average loss: 0.7146\n",
            "Iteration: 14717; Percent complete: 98.1%; Average loss: 0.6998\n",
            "Iteration: 14718; Percent complete: 98.1%; Average loss: 0.7277\n",
            "Iteration: 14719; Percent complete: 98.1%; Average loss: 0.7631\n",
            "Iteration: 14720; Percent complete: 98.1%; Average loss: 0.7560\n",
            "Iteration: 14721; Percent complete: 98.1%; Average loss: 0.7361\n",
            "Iteration: 14722; Percent complete: 98.1%; Average loss: 0.7006\n",
            "Iteration: 14723; Percent complete: 98.2%; Average loss: 0.5787\n",
            "Iteration: 14724; Percent complete: 98.2%; Average loss: 0.7030\n",
            "Iteration: 14725; Percent complete: 98.2%; Average loss: 0.6575\n",
            "Iteration: 14726; Percent complete: 98.2%; Average loss: 0.6194\n",
            "Iteration: 14727; Percent complete: 98.2%; Average loss: 0.7697\n",
            "Iteration: 14728; Percent complete: 98.2%; Average loss: 0.7440\n",
            "Iteration: 14729; Percent complete: 98.2%; Average loss: 0.8617\n",
            "Iteration: 14730; Percent complete: 98.2%; Average loss: 0.7299\n",
            "Iteration: 14731; Percent complete: 98.2%; Average loss: 0.7079\n",
            "Iteration: 14732; Percent complete: 98.2%; Average loss: 0.9584\n",
            "Iteration: 14733; Percent complete: 98.2%; Average loss: 0.7793\n",
            "Iteration: 14734; Percent complete: 98.2%; Average loss: 0.7883\n",
            "Iteration: 14735; Percent complete: 98.2%; Average loss: 0.8556\n",
            "Iteration: 14736; Percent complete: 98.2%; Average loss: 0.6778\n",
            "Iteration: 14737; Percent complete: 98.2%; Average loss: 0.7629\n",
            "Iteration: 14738; Percent complete: 98.3%; Average loss: 0.8469\n",
            "Iteration: 14739; Percent complete: 98.3%; Average loss: 0.6920\n",
            "Iteration: 14740; Percent complete: 98.3%; Average loss: 0.7738\n",
            "Iteration: 14741; Percent complete: 98.3%; Average loss: 0.8776\n",
            "Iteration: 14742; Percent complete: 98.3%; Average loss: 0.8207\n",
            "Iteration: 14743; Percent complete: 98.3%; Average loss: 0.7805\n",
            "Iteration: 14744; Percent complete: 98.3%; Average loss: 0.7374\n",
            "Iteration: 14745; Percent complete: 98.3%; Average loss: 0.8058\n",
            "Iteration: 14746; Percent complete: 98.3%; Average loss: 0.6061\n",
            "Iteration: 14747; Percent complete: 98.3%; Average loss: 0.9068\n",
            "Iteration: 14748; Percent complete: 98.3%; Average loss: 0.8258\n",
            "Iteration: 14749; Percent complete: 98.3%; Average loss: 0.6256\n",
            "Iteration: 14750; Percent complete: 98.3%; Average loss: 0.6446\n",
            "Iteration: 14751; Percent complete: 98.3%; Average loss: 0.6581\n",
            "Iteration: 14752; Percent complete: 98.3%; Average loss: 0.6861\n",
            "Iteration: 14753; Percent complete: 98.4%; Average loss: 0.6168\n",
            "Iteration: 14754; Percent complete: 98.4%; Average loss: 0.6990\n",
            "Iteration: 14755; Percent complete: 98.4%; Average loss: 0.8437\n",
            "Iteration: 14756; Percent complete: 98.4%; Average loss: 0.7996\n",
            "Iteration: 14757; Percent complete: 98.4%; Average loss: 0.7056\n",
            "Iteration: 14758; Percent complete: 98.4%; Average loss: 0.7835\n",
            "Iteration: 14759; Percent complete: 98.4%; Average loss: 0.6902\n",
            "Iteration: 14760; Percent complete: 98.4%; Average loss: 0.8197\n",
            "Iteration: 14761; Percent complete: 98.4%; Average loss: 0.7352\n",
            "Iteration: 14762; Percent complete: 98.4%; Average loss: 0.6603\n",
            "Iteration: 14763; Percent complete: 98.4%; Average loss: 0.7854\n",
            "Iteration: 14764; Percent complete: 98.4%; Average loss: 0.7735\n",
            "Iteration: 14765; Percent complete: 98.4%; Average loss: 0.5339\n",
            "Iteration: 14766; Percent complete: 98.4%; Average loss: 0.8852\n",
            "Iteration: 14767; Percent complete: 98.4%; Average loss: 0.8405\n",
            "Iteration: 14768; Percent complete: 98.5%; Average loss: 0.8028\n",
            "Iteration: 14769; Percent complete: 98.5%; Average loss: 0.7043\n",
            "Iteration: 14770; Percent complete: 98.5%; Average loss: 0.6105\n",
            "Iteration: 14771; Percent complete: 98.5%; Average loss: 0.7811\n",
            "Iteration: 14772; Percent complete: 98.5%; Average loss: 0.6694\n",
            "Iteration: 14773; Percent complete: 98.5%; Average loss: 0.6936\n",
            "Iteration: 14774; Percent complete: 98.5%; Average loss: 0.7172\n",
            "Iteration: 14775; Percent complete: 98.5%; Average loss: 0.7450\n",
            "Iteration: 14776; Percent complete: 98.5%; Average loss: 0.7596\n",
            "Iteration: 14777; Percent complete: 98.5%; Average loss: 0.7155\n",
            "Iteration: 14778; Percent complete: 98.5%; Average loss: 0.5417\n",
            "Iteration: 14779; Percent complete: 98.5%; Average loss: 0.5457\n",
            "Iteration: 14780; Percent complete: 98.5%; Average loss: 0.7730\n",
            "Iteration: 14781; Percent complete: 98.5%; Average loss: 0.7880\n",
            "Iteration: 14782; Percent complete: 98.5%; Average loss: 0.9008\n",
            "Iteration: 14783; Percent complete: 98.6%; Average loss: 0.7808\n",
            "Iteration: 14784; Percent complete: 98.6%; Average loss: 0.8136\n",
            "Iteration: 14785; Percent complete: 98.6%; Average loss: 0.7443\n",
            "Iteration: 14786; Percent complete: 98.6%; Average loss: 0.7408\n",
            "Iteration: 14787; Percent complete: 98.6%; Average loss: 0.6754\n",
            "Iteration: 14788; Percent complete: 98.6%; Average loss: 0.6680\n",
            "Iteration: 14789; Percent complete: 98.6%; Average loss: 0.7636\n",
            "Iteration: 14790; Percent complete: 98.6%; Average loss: 0.7142\n",
            "Iteration: 14791; Percent complete: 98.6%; Average loss: 0.8200\n",
            "Iteration: 14792; Percent complete: 98.6%; Average loss: 0.7076\n",
            "Iteration: 14793; Percent complete: 98.6%; Average loss: 0.7246\n",
            "Iteration: 14794; Percent complete: 98.6%; Average loss: 0.7819\n",
            "Iteration: 14795; Percent complete: 98.6%; Average loss: 0.6003\n",
            "Iteration: 14796; Percent complete: 98.6%; Average loss: 0.7254\n",
            "Iteration: 14797; Percent complete: 98.6%; Average loss: 0.7456\n",
            "Iteration: 14798; Percent complete: 98.7%; Average loss: 0.8321\n",
            "Iteration: 14799; Percent complete: 98.7%; Average loss: 0.6899\n",
            "Iteration: 14800; Percent complete: 98.7%; Average loss: 0.6805\n",
            "Iteration: 14801; Percent complete: 98.7%; Average loss: 0.6863\n",
            "Iteration: 14802; Percent complete: 98.7%; Average loss: 0.7311\n",
            "Iteration: 14803; Percent complete: 98.7%; Average loss: 0.7589\n",
            "Iteration: 14804; Percent complete: 98.7%; Average loss: 0.7271\n",
            "Iteration: 14805; Percent complete: 98.7%; Average loss: 0.6007\n",
            "Iteration: 14806; Percent complete: 98.7%; Average loss: 0.7762\n",
            "Iteration: 14807; Percent complete: 98.7%; Average loss: 0.7212\n",
            "Iteration: 14808; Percent complete: 98.7%; Average loss: 0.7732\n",
            "Iteration: 14809; Percent complete: 98.7%; Average loss: 0.6492\n",
            "Iteration: 14810; Percent complete: 98.7%; Average loss: 0.7468\n",
            "Iteration: 14811; Percent complete: 98.7%; Average loss: 0.9067\n",
            "Iteration: 14812; Percent complete: 98.7%; Average loss: 0.6470\n",
            "Iteration: 14813; Percent complete: 98.8%; Average loss: 0.6504\n",
            "Iteration: 14814; Percent complete: 98.8%; Average loss: 0.6222\n",
            "Iteration: 14815; Percent complete: 98.8%; Average loss: 0.7628\n",
            "Iteration: 14816; Percent complete: 98.8%; Average loss: 0.6761\n",
            "Iteration: 14817; Percent complete: 98.8%; Average loss: 0.6706\n",
            "Iteration: 14818; Percent complete: 98.8%; Average loss: 0.7842\n",
            "Iteration: 14819; Percent complete: 98.8%; Average loss: 0.5706\n",
            "Iteration: 14820; Percent complete: 98.8%; Average loss: 1.0225\n",
            "Iteration: 14821; Percent complete: 98.8%; Average loss: 0.6497\n",
            "Iteration: 14822; Percent complete: 98.8%; Average loss: 0.7195\n",
            "Iteration: 14823; Percent complete: 98.8%; Average loss: 0.6432\n",
            "Iteration: 14824; Percent complete: 98.8%; Average loss: 0.7349\n",
            "Iteration: 14825; Percent complete: 98.8%; Average loss: 0.7210\n",
            "Iteration: 14826; Percent complete: 98.8%; Average loss: 0.5850\n",
            "Iteration: 14827; Percent complete: 98.8%; Average loss: 0.6799\n",
            "Iteration: 14828; Percent complete: 98.9%; Average loss: 0.7999\n",
            "Iteration: 14829; Percent complete: 98.9%; Average loss: 0.7386\n",
            "Iteration: 14830; Percent complete: 98.9%; Average loss: 0.6818\n",
            "Iteration: 14831; Percent complete: 98.9%; Average loss: 0.7540\n",
            "Iteration: 14832; Percent complete: 98.9%; Average loss: 0.7420\n",
            "Iteration: 14833; Percent complete: 98.9%; Average loss: 0.7594\n",
            "Iteration: 14834; Percent complete: 98.9%; Average loss: 0.7291\n",
            "Iteration: 14835; Percent complete: 98.9%; Average loss: 0.7424\n",
            "Iteration: 14836; Percent complete: 98.9%; Average loss: 0.6936\n",
            "Iteration: 14837; Percent complete: 98.9%; Average loss: 0.7449\n",
            "Iteration: 14838; Percent complete: 98.9%; Average loss: 0.7005\n",
            "Iteration: 14839; Percent complete: 98.9%; Average loss: 0.6399\n",
            "Iteration: 14840; Percent complete: 98.9%; Average loss: 0.7774\n",
            "Iteration: 14841; Percent complete: 98.9%; Average loss: 0.6926\n",
            "Iteration: 14842; Percent complete: 98.9%; Average loss: 0.7451\n",
            "Iteration: 14843; Percent complete: 99.0%; Average loss: 0.6464\n",
            "Iteration: 14844; Percent complete: 99.0%; Average loss: 0.6911\n",
            "Iteration: 14845; Percent complete: 99.0%; Average loss: 0.7510\n",
            "Iteration: 14846; Percent complete: 99.0%; Average loss: 0.7533\n",
            "Iteration: 14847; Percent complete: 99.0%; Average loss: 0.8076\n",
            "Iteration: 14848; Percent complete: 99.0%; Average loss: 0.7516\n",
            "Iteration: 14849; Percent complete: 99.0%; Average loss: 0.6899\n",
            "Iteration: 14850; Percent complete: 99.0%; Average loss: 0.7139\n",
            "Iteration: 14851; Percent complete: 99.0%; Average loss: 0.7655\n",
            "Iteration: 14852; Percent complete: 99.0%; Average loss: 0.7219\n",
            "Iteration: 14853; Percent complete: 99.0%; Average loss: 0.6490\n",
            "Iteration: 14854; Percent complete: 99.0%; Average loss: 0.7559\n",
            "Iteration: 14855; Percent complete: 99.0%; Average loss: 0.7471\n",
            "Iteration: 14856; Percent complete: 99.0%; Average loss: 0.6982\n",
            "Iteration: 14857; Percent complete: 99.0%; Average loss: 0.6285\n",
            "Iteration: 14858; Percent complete: 99.1%; Average loss: 0.7865\n",
            "Iteration: 14859; Percent complete: 99.1%; Average loss: 0.7231\n",
            "Iteration: 14860; Percent complete: 99.1%; Average loss: 0.7320\n",
            "Iteration: 14861; Percent complete: 99.1%; Average loss: 0.6545\n",
            "Iteration: 14862; Percent complete: 99.1%; Average loss: 0.7514\n",
            "Iteration: 14863; Percent complete: 99.1%; Average loss: 0.7890\n",
            "Iteration: 14864; Percent complete: 99.1%; Average loss: 0.6286\n",
            "Iteration: 14865; Percent complete: 99.1%; Average loss: 0.7175\n",
            "Iteration: 14866; Percent complete: 99.1%; Average loss: 0.8573\n",
            "Iteration: 14867; Percent complete: 99.1%; Average loss: 0.9038\n",
            "Iteration: 14868; Percent complete: 99.1%; Average loss: 0.6275\n",
            "Iteration: 14869; Percent complete: 99.1%; Average loss: 0.8334\n",
            "Iteration: 14870; Percent complete: 99.1%; Average loss: 0.7856\n",
            "Iteration: 14871; Percent complete: 99.1%; Average loss: 0.7051\n",
            "Iteration: 14872; Percent complete: 99.1%; Average loss: 0.6602\n",
            "Iteration: 14873; Percent complete: 99.2%; Average loss: 0.6838\n",
            "Iteration: 14874; Percent complete: 99.2%; Average loss: 0.7689\n",
            "Iteration: 14875; Percent complete: 99.2%; Average loss: 0.7262\n",
            "Iteration: 14876; Percent complete: 99.2%; Average loss: 0.7267\n",
            "Iteration: 14877; Percent complete: 99.2%; Average loss: 0.6666\n",
            "Iteration: 14878; Percent complete: 99.2%; Average loss: 0.6311\n",
            "Iteration: 14879; Percent complete: 99.2%; Average loss: 0.7268\n",
            "Iteration: 14880; Percent complete: 99.2%; Average loss: 0.8123\n",
            "Iteration: 14881; Percent complete: 99.2%; Average loss: 0.7665\n",
            "Iteration: 14882; Percent complete: 99.2%; Average loss: 0.7936\n",
            "Iteration: 14883; Percent complete: 99.2%; Average loss: 0.6541\n",
            "Iteration: 14884; Percent complete: 99.2%; Average loss: 0.7024\n",
            "Iteration: 14885; Percent complete: 99.2%; Average loss: 0.6808\n",
            "Iteration: 14886; Percent complete: 99.2%; Average loss: 0.8313\n",
            "Iteration: 14887; Percent complete: 99.2%; Average loss: 0.7658\n",
            "Iteration: 14888; Percent complete: 99.3%; Average loss: 0.7155\n",
            "Iteration: 14889; Percent complete: 99.3%; Average loss: 0.7230\n",
            "Iteration: 14890; Percent complete: 99.3%; Average loss: 0.7967\n",
            "Iteration: 14891; Percent complete: 99.3%; Average loss: 0.7594\n",
            "Iteration: 14892; Percent complete: 99.3%; Average loss: 0.7940\n",
            "Iteration: 14893; Percent complete: 99.3%; Average loss: 0.8265\n",
            "Iteration: 14894; Percent complete: 99.3%; Average loss: 0.7420\n",
            "Iteration: 14895; Percent complete: 99.3%; Average loss: 0.6877\n",
            "Iteration: 14896; Percent complete: 99.3%; Average loss: 0.6241\n",
            "Iteration: 14897; Percent complete: 99.3%; Average loss: 0.7367\n",
            "Iteration: 14898; Percent complete: 99.3%; Average loss: 0.6386\n",
            "Iteration: 14899; Percent complete: 99.3%; Average loss: 0.6760\n",
            "Iteration: 14900; Percent complete: 99.3%; Average loss: 0.6492\n",
            "Iteration: 14901; Percent complete: 99.3%; Average loss: 0.8583\n",
            "Iteration: 14902; Percent complete: 99.3%; Average loss: 0.7939\n",
            "Iteration: 14903; Percent complete: 99.4%; Average loss: 0.7977\n",
            "Iteration: 14904; Percent complete: 99.4%; Average loss: 0.7439\n",
            "Iteration: 14905; Percent complete: 99.4%; Average loss: 0.5616\n",
            "Iteration: 14906; Percent complete: 99.4%; Average loss: 0.6790\n",
            "Iteration: 14907; Percent complete: 99.4%; Average loss: 0.6291\n",
            "Iteration: 14908; Percent complete: 99.4%; Average loss: 0.7963\n",
            "Iteration: 14909; Percent complete: 99.4%; Average loss: 0.8185\n",
            "Iteration: 14910; Percent complete: 99.4%; Average loss: 0.8041\n",
            "Iteration: 14911; Percent complete: 99.4%; Average loss: 0.6542\n",
            "Iteration: 14912; Percent complete: 99.4%; Average loss: 0.6262\n",
            "Iteration: 14913; Percent complete: 99.4%; Average loss: 0.7701\n",
            "Iteration: 14914; Percent complete: 99.4%; Average loss: 0.8061\n",
            "Iteration: 14915; Percent complete: 99.4%; Average loss: 0.7536\n",
            "Iteration: 14916; Percent complete: 99.4%; Average loss: 0.6875\n",
            "Iteration: 14917; Percent complete: 99.4%; Average loss: 0.7450\n",
            "Iteration: 14918; Percent complete: 99.5%; Average loss: 0.8702\n",
            "Iteration: 14919; Percent complete: 99.5%; Average loss: 0.7773\n",
            "Iteration: 14920; Percent complete: 99.5%; Average loss: 0.7953\n",
            "Iteration: 14921; Percent complete: 99.5%; Average loss: 0.7759\n",
            "Iteration: 14922; Percent complete: 99.5%; Average loss: 0.6877\n",
            "Iteration: 14923; Percent complete: 99.5%; Average loss: 0.7948\n",
            "Iteration: 14924; Percent complete: 99.5%; Average loss: 0.7962\n",
            "Iteration: 14925; Percent complete: 99.5%; Average loss: 0.7218\n",
            "Iteration: 14926; Percent complete: 99.5%; Average loss: 0.6639\n",
            "Iteration: 14927; Percent complete: 99.5%; Average loss: 0.7390\n",
            "Iteration: 14928; Percent complete: 99.5%; Average loss: 0.7264\n",
            "Iteration: 14929; Percent complete: 99.5%; Average loss: 0.7977\n",
            "Iteration: 14930; Percent complete: 99.5%; Average loss: 0.7117\n",
            "Iteration: 14931; Percent complete: 99.5%; Average loss: 0.7104\n",
            "Iteration: 14932; Percent complete: 99.5%; Average loss: 0.7490\n",
            "Iteration: 14933; Percent complete: 99.6%; Average loss: 0.7466\n",
            "Iteration: 14934; Percent complete: 99.6%; Average loss: 1.0062\n",
            "Iteration: 14935; Percent complete: 99.6%; Average loss: 0.7647\n",
            "Iteration: 14936; Percent complete: 99.6%; Average loss: 0.6811\n",
            "Iteration: 14937; Percent complete: 99.6%; Average loss: 0.8342\n",
            "Iteration: 14938; Percent complete: 99.6%; Average loss: 0.6764\n",
            "Iteration: 14939; Percent complete: 99.6%; Average loss: 0.6997\n",
            "Iteration: 14940; Percent complete: 99.6%; Average loss: 0.7616\n",
            "Iteration: 14941; Percent complete: 99.6%; Average loss: 0.8063\n",
            "Iteration: 14942; Percent complete: 99.6%; Average loss: 0.7134\n",
            "Iteration: 14943; Percent complete: 99.6%; Average loss: 0.6913\n",
            "Iteration: 14944; Percent complete: 99.6%; Average loss: 0.7803\n",
            "Iteration: 14945; Percent complete: 99.6%; Average loss: 0.7313\n",
            "Iteration: 14946; Percent complete: 99.6%; Average loss: 0.7409\n",
            "Iteration: 14947; Percent complete: 99.6%; Average loss: 0.7341\n",
            "Iteration: 14948; Percent complete: 99.7%; Average loss: 0.5971\n",
            "Iteration: 14949; Percent complete: 99.7%; Average loss: 0.7277\n",
            "Iteration: 14950; Percent complete: 99.7%; Average loss: 0.7753\n",
            "Iteration: 14951; Percent complete: 99.7%; Average loss: 0.7107\n",
            "Iteration: 14952; Percent complete: 99.7%; Average loss: 0.7425\n",
            "Iteration: 14953; Percent complete: 99.7%; Average loss: 0.8240\n",
            "Iteration: 14954; Percent complete: 99.7%; Average loss: 0.8001\n",
            "Iteration: 14955; Percent complete: 99.7%; Average loss: 0.6976\n",
            "Iteration: 14956; Percent complete: 99.7%; Average loss: 0.7738\n",
            "Iteration: 14957; Percent complete: 99.7%; Average loss: 0.7579\n",
            "Iteration: 14958; Percent complete: 99.7%; Average loss: 0.7495\n",
            "Iteration: 14959; Percent complete: 99.7%; Average loss: 0.7234\n",
            "Iteration: 14960; Percent complete: 99.7%; Average loss: 0.8078\n",
            "Iteration: 14961; Percent complete: 99.7%; Average loss: 0.6696\n",
            "Iteration: 14962; Percent complete: 99.7%; Average loss: 0.7385\n",
            "Iteration: 14963; Percent complete: 99.8%; Average loss: 0.8080\n",
            "Iteration: 14964; Percent complete: 99.8%; Average loss: 0.7485\n",
            "Iteration: 14965; Percent complete: 99.8%; Average loss: 0.7663\n",
            "Iteration: 14966; Percent complete: 99.8%; Average loss: 0.8001\n",
            "Iteration: 14967; Percent complete: 99.8%; Average loss: 0.6218\n",
            "Iteration: 14968; Percent complete: 99.8%; Average loss: 0.7546\n",
            "Iteration: 14969; Percent complete: 99.8%; Average loss: 0.6827\n",
            "Iteration: 14970; Percent complete: 99.8%; Average loss: 0.7168\n",
            "Iteration: 14971; Percent complete: 99.8%; Average loss: 0.7106\n",
            "Iteration: 14972; Percent complete: 99.8%; Average loss: 0.9519\n",
            "Iteration: 14973; Percent complete: 99.8%; Average loss: 0.6949\n",
            "Iteration: 14974; Percent complete: 99.8%; Average loss: 0.8120\n",
            "Iteration: 14975; Percent complete: 99.8%; Average loss: 0.7324\n",
            "Iteration: 14976; Percent complete: 99.8%; Average loss: 0.7772\n",
            "Iteration: 14977; Percent complete: 99.8%; Average loss: 0.5931\n",
            "Iteration: 14978; Percent complete: 99.9%; Average loss: 0.6370\n",
            "Iteration: 14979; Percent complete: 99.9%; Average loss: 0.6918\n",
            "Iteration: 14980; Percent complete: 99.9%; Average loss: 0.7538\n",
            "Iteration: 14981; Percent complete: 99.9%; Average loss: 0.7152\n",
            "Iteration: 14982; Percent complete: 99.9%; Average loss: 0.6664\n",
            "Iteration: 14983; Percent complete: 99.9%; Average loss: 0.7623\n",
            "Iteration: 14984; Percent complete: 99.9%; Average loss: 0.8226\n",
            "Iteration: 14985; Percent complete: 99.9%; Average loss: 0.7678\n",
            "Iteration: 14986; Percent complete: 99.9%; Average loss: 0.7193\n",
            "Iteration: 14987; Percent complete: 99.9%; Average loss: 0.7467\n",
            "Iteration: 14988; Percent complete: 99.9%; Average loss: 0.7838\n",
            "Iteration: 14989; Percent complete: 99.9%; Average loss: 0.6504\n",
            "Iteration: 14990; Percent complete: 99.9%; Average loss: 0.6548\n",
            "Iteration: 14991; Percent complete: 99.9%; Average loss: 0.7496\n",
            "Iteration: 14992; Percent complete: 99.9%; Average loss: 0.6347\n",
            "Iteration: 14993; Percent complete: 100.0%; Average loss: 0.6223\n",
            "Iteration: 14994; Percent complete: 100.0%; Average loss: 0.7054\n",
            "Iteration: 14995; Percent complete: 100.0%; Average loss: 0.7015\n",
            "Iteration: 14996; Percent complete: 100.0%; Average loss: 0.7568\n",
            "Iteration: 14997; Percent complete: 100.0%; Average loss: 0.7325\n",
            "Iteration: 14998; Percent complete: 100.0%; Average loss: 0.6432\n",
            "Iteration: 14999; Percent complete: 100.0%; Average loss: 0.7322\n",
            "Iteration: 15000; Percent complete: 100.0%; Average loss: 0.5989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout 레이어를 평가( ``eval`` ) 모드로 설정합니다\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# 탐색 모듈을 초기화합니다\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# 채팅을 시작합니다 (다음 줄의 주석을 제거하면 시작해볼 수 있습니다)\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_2aQhvczpYE",
        "outputId": "7453f6f3-b5cc-4b1a-fe4b-8e009cebb732"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> hello\n",
            "Bot: hello . . . . .\n",
            "> what are you doing\n",
            "Bot: saving your life . i m busy .\n",
            "> it's trash\n",
            "Bot: what ? what s happening ?\n",
            "> your face\n",
            "Bot: do you want to kill it ?\n",
            "> yes\n",
            "Bot: and you think i m not ?\n",
            "> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.arange(1, 15001)\n",
        "fig = plt.figure()\n",
        "plt.plot(x, loss)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss as iteration increases')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j3R88LFX5lt4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "995605a3-46ef-47dd-8e0b-11dec092c7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHHCAYAAACRAnNyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWN5JREFUeJzt3Xl8TOceBvBnsk0S2SOJRFYJIkEEte8UFZQuSlXRqlKq1KVU7SWoKq1WVVt0sbW2FrXHvosgsSSICBERkV3Wee8fkSMjk1WSM0me7+eTz82ceefM78xV83i3oxBCCBARERFpIR25CyAiIiIqCIMKERERaS0GFSIiItJaDCpERESktRhUiIiISGsxqBAREZHWYlAhIiIircWgQkRERFqLQYWIiIi0FoMKUTWzZs0aKBQK3L59W+5Sim3WrFlQKBSyvLdCocCsWbNkeW8iYlChaiT3C/rcuXNyl6J1fvjhB6xZs0bWGlJTUzFr1iwcOnRI1jqISLsoeK8fqi7WrFmD4cOH4+zZs2jevLnc5cgmOzsbmZmZUCqVUi9Fw4YNUbNmTVlDQmxsLGxsbDBz5sx8PRhZWVnIysqCoaFhhdeVlpYGPT096OnpVfh7ExHA//KIqhldXV3o6uqW+/tkZWVBpVLBwMDghc8lZ1Co6HAkhEBaWhqMjIwq9H2JtBWHfoiec+HCBbzyyiswMzODiYkJunbtilOnTqm1yczMxOzZs1G3bl0YGhrC2toa7dq1w759+6Q20dHRGD58OBwdHaFUKmFvb49XX321yLkhly5dwrBhw1CnTh0YGhqiVq1aeO+99/Do0SO1dklJSRg/fjxcXV2hVCpha2uLl19+GYGBgYWe//k5Kq6urggJCcHhw4ehUCigUCjQqVMnqX18fDzGjx8PJycnKJVKeHh4YOHChVCpVFKb27dvQ6FQYPHixVi6dCnc3d2hVCpx5coVZGRkYMaMGWjWrBnMzc1Ro0YNtG/fHgEBAWqvt7GxAQDMnj1bqiO3Z0XTHJWsrCzMnTtXei9XV1d8/vnnSE9PV2vn6uqK3r1749ixY2jRogUMDQ1Rp04d/Pbbb4V+Trmen6OSW8uNGzcwbNgwWFhYwNzcHMOHD0dqamq+1//xxx9o0aIFjI2NYWlpiQ4dOmDv3r356tuzZw+aN28OIyMjrFy5stifPQAsXrwYbdq0gbW1NYyMjNCsWTP8/fff+WrZt28f2rVrBwsLC5iYmKB+/fr4/PPP1dqkp6dj5syZ8PDwgFKphJOTEyZPnpzvcy3OuYjKAntUiPIICQlB+/btYWZmhsmTJ0NfXx8rV65Ep06dcPjwYbRs2RJAzpeVv78/RowYgRYtWiAxMRHnzp1DYGAgXn75ZQDA66+/jpCQEHz88cdwdXVFTEwM9u3bhzt37sDV1bXAGvbt24dbt25h+PDhqFWrFkJCQvDTTz8hJCQEp06dkr6wR40ahb///htjx46Fl5cXHj16hGPHjuHq1ato2rRpsa956dKl+Pjjj2FiYoJp06YBAOzs7ADkzBvp2LEj7t27hw8//BDOzs44ceIEpk6divv372Pp0qVq51q9ejXS0tIwcuRIKJVKWFlZITExET///DMGDRqEDz74AElJSfjll1/Qo0cPnDlzBk2aNIGNjQ1WrFiB0aNHo3///njttdcAAI0bNy6w7hEjRmDt2rV44403MHHiRJw+fRr+/v64evUqtm7dqtb2xo0beOONN/D+++9j6NCh+PXXXzFs2DA0a9YM3t7exf6s8howYADc3Nzg7++PwMBA/Pzzz7C1tcXChQulNrNnz8asWbPQpk0bzJkzBwYGBjh9+jQOHjyI7t27S+2uX7+OQYMG4cMPP8QHH3yA+vXrl+izX7ZsGfr27YvBgwcjIyMDGzZswJtvvokdO3bAz88PQM6f7d69e6Nx48aYM2cOlEolbty4gePHj0vnUalU6Nu3L44dO4aRI0eiQYMGuHz5Mr755huEhoZi27ZtxT4XUZkRRNXE6tWrBQBx9uzZAtv069dPGBgYiJs3b0rHoqKihKmpqejQoYN0zMfHR/j5+RV4nsePHwsA4quvvipxnampqfmOrV+/XgAQR44ckY6Zm5uLMWPGlPj8uZ9DeHi4dMzb21t07NgxX9u5c+eKGjVqiNDQULXjU6ZMEbq6uuLOnTtCCCHCw8MFAGFmZiZiYmLU2mZlZYn09HS1Y48fPxZ2dnbivffek449fPhQABAzZ87MV8fMmTNF3r+ugoKCBAAxYsQItXb/+9//BABx8OBB6ZiLi0u+zy4mJkYolUoxceLEfO/1vOdryq0lb+1CCNG/f39hbW0tPQ4LCxM6Ojqif//+Ijs7W62tSqXKV9/u3bvV2hT3sxci/5+ZjIwM0bBhQ9GlSxfp2DfffCMAiIcPHxZ4rb///rvQ0dERR48eVTv+448/CgDi+PHjxT4XUVnh0A/RU9nZ2di7dy/69euHOnXqSMft7e3x9ttv49ixY0hMTAQAWFhYICQkBGFhYRrPZWRkBAMDAxw6dAiPHz8uUR155yakpaUhNjYWrVq1AgC1YR0LCwucPn0aUVFRJTp/Sfz1119o3749LC0tERsbK/1069YN2dnZOHLkiFr7119/XRrCyaWrqyvNU1GpVIiLi0NWVhaaN29e5DBVQXbt2gUA+PTTT9WOT5w4EQCwc+dOteNeXl5o37699NjGxgb169fHrVu3SvX+QE6PVl7t27fHo0ePpD8j27Ztg0qlwowZM6Cjo/5X7fPDWG5ubujRo4fasZJ89nn/zDx+/BgJCQlo3759vj8vALB9+/Z8Q0d537NBgwbw9PRUe88uXboAgDRcV5xzEZUVBhWipx4+fIjU1FTUr18/33MNGjSASqVCZGQkAGDOnDmIj49HvXr10KhRI0yaNAmXLl2S2iuVSixcuBD//fcf7Ozs0KFDByxatAjR0dFF1hEXF4dPPvkEdnZ2MDIygo2NDdzc3AAACQkJUrtFixYhODgYTk5OaNGiBWbNmvVCX7yahIWFYffu3bCxsVH76datGwAgJiZGrX1unc9bu3YtGjduLM3nsbGxwc6dO9WupyQiIiKgo6MDDw8PteO1atWChYUFIiIi1I47OzvnO4elpWWJQ2Rh57S0tAQA6Zw3b96Ejo4OvLy8ijyXps+tJJ/9jh070KpVKxgaGsLKykoaSsv7+b711lto27YtRowYATs7OwwcOBCbNm1SCxphYWEICQnJ95716tVTe8/inIuorHCOClEpdOjQATdv3sT27duxd+9e/Pzzz/jmm2/w448/YsSIEQCA8ePHo0+fPti2bRv27NmD6dOnw9/fHwcPHoSvr2+B5x4wYABOnDiBSZMmoUmTJjAxMYFKpULPnj3VvggGDBiA9u3bY+vWrdi7dy+++uorLFy4EFu2bMErr7xSJtepUqnw8ssvY/LkyRqfz/0Cy6Vppcoff/yBYcOGoV+/fpg0aRJsbW2hq6sLf39/3Lx584XqK+4mcAWtchIvsDtDWZ5T0+dW3M/+6NGj6Nu3Lzp06IAffvgB9vb20NfXx+rVq7Fu3Tq19zhy5AgCAgKwc+dO7N69Gxs3bkSXLl2wd+9e6OrqQqVSoVGjRliyZInG93Rycir2uYjKCoMK0VM2NjYwNjbG9evX8z137do16OjoSH9RA4CVlRWGDx+O4cOHIzk5GR06dMCsWbOkoAIA7u7umDhxIiZOnIiwsDA0adIEX3/9Nf744w+NNTx+/BgHDhzA7NmzMWPGDOl4QUNM9vb2+Oijj/DRRx8hJiYGTZs2xbx580ocVAr6wnd3d0dycrL0r/jS+Pvvv1GnTh1s2bJF7X1mzpxZrBo0cXFxgUqlQlhYGBo0aCAdf/DgAeLj4+Hi4lLqesuKu7s7VCoVrly5giZNmpTq9cX57Ddv3gxDQ0Ps2bMHSqVSOr569ep8bXV0dNC1a1d07doVS5Yswfz58zFt2jQEBASgW7ducHd3x8WLF9G1a9ci//8o6lxEZYVDP0RP6erqonv37ti+fbvaEuIHDx5g3bp1aNeuHczMzAAg31JhExMTeHh4SEs4U1NTkZaWptbG3d0dpqam+ZZ5Pl8DkP9f5c+vrsnOzs43bGJrawsHB4dCz1+QGjVqID4+Pt/xAQMG4OTJk9izZ0++5+Lj45GVlVXkuTVd0+nTp3Hy5Em1dsbGxtJ5i9KrVy8A+T+X3J6A3JUucurXrx90dHQwZ86cfEMixel1Ke5nr6urC4VCgezsbOn527dvSyt0csXFxeU7T26Ayv0zM2DAANy7dw+rVq3K1/bJkydISUkp9rmIygp7VKja+fXXX7F79+58xz/55BN8+eWX0v4QH330EfT09LBy5Uqkp6dj0aJFUlsvLy906tQJzZo1g5WVFc6dOyctFQaA0NBQdO3aFQMGDICXlxf09PSwdetWPHjwAAMHDiywNjMzM2k+S2ZmJmrXro29e/ciPDxcrV1SUhIcHR3xxhtvwMfHByYmJti/fz/Onj2Lr7/+usSfSbNmzbBixQp8+eWX8PDwgK2tLbp06YJJkybhn3/+Qe/evaXlvCkpKbh8+TL+/vtv3L59GzVr1iz03L1798aWLVvQv39/+Pn5ITw8HD/++CO8vLyQnJwstTMyMoKXlxc2btyIevXqwcrKCg0bNkTDhg3zndPHxwdDhw7FTz/9hPj4eHTs2BFnzpzB2rVr0a9fP3Tu3LnEn0FZ8/DwwLRp0zB37ly0b98er732GpRKJc6ePQsHBwf4+/sX+vrifvZ+fn5YsmQJevbsibfffhsxMTH4/vvv4eHhoTZvas6cOThy5Aj8/Pzg4uKCmJgY/PDDD3B0dES7du0AAEOGDMGmTZswatQoBAQEoG3btsjOzsa1a9ewadMmaa+X4pyLqMzIuuaIqALlLsst6CcyMlIIIURgYKDo0aOHMDExEcbGxqJz587ixIkTauf68ssvRYsWLYSFhYUwMjISnp6eYt68eSIjI0MIIURsbKwYM2aM8PT0FDVq1BDm5uaiZcuWYtOmTUXWeffuXdG/f39hYWEhzM3NxZtvvimioqLUlsmmp6eLSZMmCR8fH2Fqaipq1KghfHx8xA8//FDszyHv8uTo6Gjh5+cnTE1NBQC1pcpJSUli6tSpwsPDQxgYGIiaNWuKNm3aiMWLF0vXm7s8WdNybJVKJebPny9cXFyEUqkUvr6+YseOHWLo0KHCxcVFre2JEydEs2bNhIGBgdr1Pr88WQghMjMzxezZs4Wbm5vQ19cXTk5OYurUqSItLU2tnYuLi8al5B07dtS4JPt5KGB58vNLczV9rkII8euvvwpfX1+hVCqFpaWl6Nixo9i3b1+R9QlRvM9eCCF++eUXUbduXaFUKoWnp6dYvXp1vs/swIED4tVXXxUODg7CwMBAODg4iEGDBuVb/pyRkSEWLlwovL29pZqbNWsmZs+eLRISEkp0LqKywHv9EBERkdbiHBUiIiLSWgwqREREpLUYVIiIiEhrMagQERGR1mJQISIiIq3FoEJERERaq1Jv+KZSqRAVFQVTU9MSbb9NRERE8hFCICkpCQ4ODvnuLv68Sh1UoqKi1O69QkRERJVHZGQkHB0dC21TqYOKqakpgJwLzb0HCxEREWm3xMREODk5Sd/jhanUQSV3uMfMzIxBhYiIqJIpzrQNTqYlIiIircWgQkRERFqLQYWIiIi0FoMKERERaS0GFSIiItJaDCpERESktRhUiIiISGsxqBAREZHWYlAhIiIircWgQkRERFqLQYWIiIi0FoMKERERaa1KfVPC8pKakYW4lAwo9XRhY6qUuxwiIqJqiz0qGuy/GoN2CwPwyYYLcpdCRERUrTGoFEIIuSsgIiKq3hhUNFDIXQAREREBYFAplAC7VIiIiOTEoKKBgl0qREREWoFBpRCco0JERCQvBhUNFJylQkREpBUYVArBDhUiIiJ5MahowDkqRERE2oFBpTDsUiEiIpIVg4oGuR0qXJ5MREQkLwYVIiIi0loMKhrkzlHh8mQiIiJ5MagQERGR1mJQ0SinS4UdKkRERPJiUCEiIiKtxaCiwbM5KuxTISIikhODChEREWktBhUNnu2jQkRERHJiUCEiIiKtxaCigeLpJBVOUSEiIpIXg4oGHPohIiLSDgwqREREpLUYVDRQSF0q7FMhIiKSE4MKERERaS0GFQ2kDd/kLYOIiKjaY1AhIiIircWgooECXJ5MRESkDRhUiIiISGsxqGgizVFhlwoREZGcGFSIiIhIa8kaVLKzszF9+nS4ubnByMgI7u7umDt3LoTMk0O4jQoREZF20JPzzRcuXIgVK1Zg7dq18Pb2xrlz5zB8+HCYm5tj3LhxcpYGgEGFiIhIbrIGlRMnTuDVV1+Fn58fAMDV1RXr16/HmTNn5CxLuikhERERyUvWoZ82bdrgwIEDCA0NBQBcvHgRx44dwyuvvKKxfXp6OhITE9V+yhM7VIiIiOQla4/KlClTkJiYCE9PT+jq6iI7Oxvz5s3D4MGDNbb39/fH7Nmzy70u9qcQERFpB1l7VDZt2oQ///wT69atQ2BgINauXYvFixdj7dq1GttPnToVCQkJ0k9kZGS51if3pF4iIqLqTtYelUmTJmHKlCkYOHAgAKBRo0aIiIiAv78/hg4dmq+9UqmEUqks97o4RYWIiEg7yNqjkpqaCh0d9RJ0dXWhUqlkqoiIiIi0iaw9Kn369MG8efPg7OwMb29vXLhwAUuWLMF7770nZ1nSvX6IiIhIXrIGle+++w7Tp0/HRx99hJiYGDg4OODDDz/EjBkz5CxLwikqRERE8pI1qJiammLp0qVYunSpnGXkwzkqRERE2oH3+ikEb0pIREQkLwYVDXivHyIiIu3AoKIJh36IiIi0AoNKIdihQkREJC8GFQ1ylydzZ1oiIiJ5MahowFU/RERE2oFBpRDsTyEiIpIXg4oGUocKkwoREZGsGFQ0UHDsh4iISCswqBSCHSpERETyYlDRILdDhat+iIiI5MWgogEHfoiIiLQDg0oh2J9CREQkLwYVDZ4N/chbBxERUXXHoKIRB3+IiIi0AYNKIQQHf4iIiGTFoKIBh36IiIi0A4OKBrkDPwwqRERE8mJQ0YA70xIREWkHBhUiIiLSWgwqGjwb+uHYDxERkZwYVDTgyA8REZF2YFApBPtTiIiI5MWgooHi6eAPR36IiIjkxaCiAYd+iIiItAODSiG4My0REZG8GFQKwaEfIiIieTGoaMChHyIiIu3AoFIIdqgQERHJi0FFA676ISIi0g4MKho8G/phUiEiIpITg4oGnKNCRESkHRhUCsGhHyIiInkxqGggzVGRuQ4iIqLqjkFFAw79EBERaQcGlUIIjv0QERHJikFFg9wOFcYUIiIieTGoaMChHyIiIu3AoFIIjvwQERHJi0FFo9ydaZlUiIiI5MSgogGHfoiIiLQDg0oh2J9CREQkLwYVDXirHyIiIu3AoKKBgmM/REREWoFBpRDsUCEiIpIXg4oG0oZvXPVDREQkKwYVDXJHfhhTiIiI5MWgooECnKNCRESkDRhUCsGRHyIiInkxqGjwbOiHSYWIiEhODCpERESktRhUCsGhHyIiInkxqGjAVT9ERETagUFFA+5MS0REpB0YVArDLhUiIiJZMahoIO1My6RCREQkKwYVDTjyQ0REpB0YVArBVT9ERETyYlDRIHcLfeYUIiIieTGoaMChHyIiIu3AoFIIwbEfIiIiWTGoaPBs1Q8RERHJiUFFk9ydaZlUiIiIZMWgooECnKRCRESkDRhUiIiISGsxqGiQd9UPJ9QSERHJh0FFAw78EBERaQcGlSKwQ4WIiEg+DCoaKPKM/TCnEBERyUf2oHLv3j288847sLa2hpGRERo1aoRz587JWlPeoR/OUSEiIpKPnpxv/vjxY7Rt2xadO3fGf//9BxsbG4SFhcHS0lLOstQn08pXBhERUbUna1BZuHAhnJycsHr1aumYm5ubjBXlUBv6YVIhIiKSjaxDP//88w+aN2+ON998E7a2tvD19cWqVavkLAmAeo+KikmFiIhINrIGlVu3bmHFihWoW7cu9uzZg9GjR2PcuHFYu3atxvbp6elITExU+ykPXJ5MRESkHWQd+lGpVGjevDnmz58PAPD19UVwcDB+/PFHDB06NF97f39/zJ49u9zr0uHQDxERkVaQtUfF3t4eXl5eascaNGiAO3fuaGw/depUJCQkSD+RkZHlUpf6ZFomFSIiIrnI2qPStm1bXL9+Xe1YaGgoXFxcNLZXKpVQKpXlXlfemxKqmFOIiIhkI2uPyoQJE3Dq1CnMnz8fN27cwLp16/DTTz9hzJgxcpbFe/0QERFpCVmDyksvvYStW7di/fr1aNiwIebOnYulS5di8ODBcpbFfVSIiIi0hKxDPwDQu3dv9O7dW+4y1OQd+mGHChERkXxk30JfG3Hoh4iISDswqGjA5clERETagUFFA7WbEspWBRERETGoaMChHyIiIu3AoKJB3psSch8VIiIi+TCoFCA3q3BnWiIiIvkwqBRA6lNhTiEiIpINg0oBcod/mFOIiIjkw6BSAJ2nXSoqTqYlIiKSDYNKAXJ3p2VOISIikg+DSkGkybREREQkFwaVAuROpuU+KkRERPJhUClA7jb6zClERETyYVApgLSPCoMKERGRbBhUCiAN/XCWChERkWwYVAqQu48Kt9AnIiKSD4NKAZ4N/TCpEBERyYVBpQDPhn6IiIhILgwqBVBw1Q8REZHsGFQKoMOhHyIiItkxqBSANyUkIiKSH4NKAZ7tTCtrGURERNUag0oBpFU/7FMhIiKSDYNKAaR9VFQyF0JERFSNMagUgDvTEhERyY9BpQC8KSEREZH8GFQKoMObEhIREcmOQaUAz+71w6RCREQkFwaVAug8/WSyGVSIiIhkw6BSAF1pjgqDChERkVwYVAqgIw39yFwIERFRNcagUoBbsSkAgLiUDJkrISIiqr4YVIowbWuw3CUQERFVWwwqRYhNTpe7BCIiomqLQYWIiIi0FoMKERERaS0GFSIiItJapQoqa9euxc6dO6XHkydPhoWFBdq0aYOIiIgyK05bcC8VIiIieZQqqMyfPx9GRkYAgJMnT+L777/HokWLULNmTUyYMKFMC9QGe688kLsEIiKiakmvNC+KjIyEh4cHAGDbtm14/fXXMXLkSLRt2xadOnUqy/q0wtGwh+jhXUvuMoiIiKqdUvWomJiY4NGjRwCAvXv34uWXXwYAGBoa4smTJ2VXnZa4+7jqXRMREVFlUKoelZdffhkjRoyAr68vQkND0atXLwBASEgIXF1dy7I+rZCRpZK7BCIiomqpVD0q33//PVq3bo2HDx9i8+bNsLa2BgCcP38egwYNKtMCtYGujkLuEoiIiKolhajES1oSExNhbm6OhIQEmJmZlem5XafsVHsc7t8LCgUDCxER0Ysqyfd3qXpUdu/ejWPHjkmPv//+ezRp0gRvv/02Hj9+XJpTar1D1x/KXQIREVG1U6qgMmnSJCQmJgIALl++jIkTJ6JXr14IDw/Hp59+WqYFaour0Ylyl0BERFTtlGoybXh4OLy8vAAAmzdvRu/evTF//nwEBgZKE2urmso7QEZERFR5lapHxcDAAKmpqQCA/fv3o3v37gAAKysrqaelqvl673W5SyAiIqp2StWj0q5dO3z66ado27Ytzpw5g40bNwIAQkND4ejoWKYFysXMUA+JaVnSYxV7VIiIiCpcqXpUli9fDj09Pfz9999YsWIFateuDQD477//0LNnzzItUC4+ThZyl0BERFTtlapHxdnZGTt27Mh3/JtvvnnhgrSFq3UNHA2LlbsMIiKiaq1UQQUAsrOzsW3bNly9ehUA4O3tjb59+0JXV7fMipOTsTL/dfx+KgJDWrnIUA0REVH1VKqhnxs3bqBBgwZ49913sWXLFmzZsgXvvPMOvL29cfPmzbKuURYtXK3yHZu+LRjRCWkyVENERFQ9lSqojBs3Du7u7oiMjERgYCACAwNx584duLm5Ydy4cWVdoyy6eNpqPH7xbnzFFkJERFSNlWro5/Dhwzh16hSsrJ71OlhbW2PBggVo27ZtmRUnp4K2y998/i5a1bGGga4OjAyqxjAXERGRtipVUFEqlUhKSsp3PDk5GQYGBi9clDZLzciGz+y90FEAt/z95C6HiIioSivV0E/v3r0xcuRInD59GkIICCFw6tQpjBo1Cn379i3rGrVKeGwKAO6rQkREVBFKFVS+/fZbuLu7o3Xr1jA0NIShoSHatGkDDw8PLF26tIxL1C734p/IXQIREVG1UaqhHwsLC2zfvh03btyQlic3aNAAHh4eZVqc3F5v6ojNgXcLfF4IUeBcFiIiInpxxQ4qRd0VOSAgQPp9yZIlpa9Ii8zr37DQoKISgC5zChERUbkpdlC5cOFCsdpVpR4GXZ3Cr0UlBHRRda6XiIhI2xQ7qOTtMakuioogghNqiYiIylWpJtNWF0X1DmVmqyqoEiIiouqJQaUQRfWonL0dVyF1EBERVVelvilhdVDUdJvlB29ACMDGVImGtc0rpigiIqJqhEGlEEUN/ZyLeIzha84CAG4v4C61REREZY1DP0RERKS1GFSIiIhIazGolJFZ/4QgLTNb7jKIiIiqFAaVMrLmxG14Tt+Njl8FcNkyERFRGdGaoLJgwQIoFAqMHz9e7lJeSMSjVGwpZNt9IiIiKj6tCCpnz57FypUr0bhxY7lLKROfbb6Ms7fjcOluPCZuuogHiWlyl0RERFQpyR5UkpOTMXjwYKxatQqWlpZyl1Nm3vzxJPouP47NgXcxcdPFYr9OpRJYuj8UR0IflmN1RERElYPsQWXMmDHw8/NDt27d5C5Fo96N7V/4HFfvJxZ73squ4PtYuj8M7/565oXfl4iIqLKTNahs2LABgYGB8Pf3L1b79PR0JCYmqv2Ut+VvN0W4f68XOsejlAzUnfYf7jxKLbLtvcdPXui9iIiIqhLZgkpkZCQ++eQT/PnnnzA0NCzWa/z9/WFubi79ODk5lXOVORQKBX4Z2vyFz9Nj6ZEi2+jqFHWHISIioupDtqBy/vx5xMTEoGnTptDT04Oenh4OHz6Mb7/9Fnp6esjOzr8nydSpU5GQkCD9REZGVli9XTxtX/gcTzTss6JSCaRnPTuuU9QNhoiIiKoR2e7107VrV1y+fFnt2PDhw+Hp6YnPPvsMurq6+V6jVCqhVCorqkQ1Rd33p7hcp+zEznHtMOqP83i3lSv2hETj0t0EnJveDWaG+uxRISIiykO2oGJqaoqGDRuqHatRowasra3zHa9q/L49BgCYt+uqdOxYWCx6NbIHcwoREdEzsq/6IXU6eZLK76cicDEyPl8bbtVPRETVhWw9KpocOnRI7hJkI0TO/+rmGWKavi0YAHB7gZ90bNHua/jh0E2s/6AVWrtbV2iNREREFY09Kloid58VnSLGfn44dBMA8OXOK+VeExERkdy0qkelOhu/MQhRCU9gY5J/svDluwlo5Giudiy3B4aIiKgqY49KCZTFEuXCLNp9HZP+vpTveJ/lxxCkYa4KERFRVcegUgI/v9scTZ0tZHnvft8fx8OkdOnx8x0qgl0sRERUBTGolICOjgJbPmor2/t3/+aw9HveYPLVnmtotzAAj5LTNb2MiIio0mJQqUQep2ZKv9+IScbaE7eRma3C9wE3cS/+CVYdDZexOiIiorLHybSlUN/OFNcfJMlaQ5ZKYOY/IchSPetZEfkGhHJ6XsZtCIJSTweL3/SpyBKJiIheGHtUSsHcSF/uEiQnb8ZKv688fAu7g6MBABlZKvx28jZO3YrDvxej8Pf5u0hOz5KrTCIiolJhj0olt/9qjNrjUX+cx+0FflhzIhzzd11Tey5bxQm3RERUubBHpYoKjIjPd0xVRFBRqUSRbYiIiCoSe1SqoAbTd+OJhvsBqZ6uFErLzMZXe66jawNbtHGvmfOcSqDnsiPQUSiwa1z7InfIJSIiqgjsUXlBG0e2kruEfDSFFODZ3is/H72FX46F4+1Vp5GtEth0LhKnw+MQ+iAZ16KTkPAkZ3VR8L0EdFgUgB2XoiqociIiInUMKqWRp7OhZR1rdC3nHWvLSu6wzq3YFOnY5vN3MfnvSxi06lS+9mPWBeJOXCrGrrtQYTUSERHlxaBSCp3q2wAATA1zRs5mv+otZznFJk0/yTMNZfLm/Fv254p4lFq+BRERERWBQaUUPmhfB8sGNsG+CR0BAI6WxjJXVDyDVp1Cela2NFelIIpCpqfceZSKLosPYf2ZOwW2SUnPwm8nbyMq/klpSyUiIgLAybSloq+rg1eb1Ja7jBILj01B/S92w9K49PvAzPwnGLdiUzB1y2XUr2WK5LQs+DhawDzPOeftuop1p+9g9r9XcHN+r7IonYiIqin2qJSRzaNbY9W7zVGnZg25SylS3q34CxL63M673wfcwOOUDKRnqaRjr/1wAu/+egb9VxxXa3v4+kMAOfu2xKdmlEHFRERUXbFHpYw0c7ECkLO9foevAmSu5sWMWHsO5yIeqx37as91fLXnusb2tx6mqD3OO3R05X6itASaiIiopNijUsacrSvHfJXCPB9SiiMu5VnPSWFzXIiIiEqCQaUcvN/OTe4SKlzTufuk3xVgUiEiorLBoFIOvvBrgDOfd8XQ1i5ylyK7X4+F41FyOlYfD8fjFM5XISKikmFQKQcKhQK2ZoaY/WpDNKptLnc5FeZBYhqyslXIu/v+/qsxeH/tOcz+9wrGbeDGcUREVDIKIYrYVEOLJSYmwtzcHAkJCTAzM5O7HI0SnmTCZ/ZeucvQGrcX+MldAhERyawk399c9VPOzI30cXFGd6RnZSPwzmOM+iNQ7pJklZqRhSyVgJnhs31X0jKzYaCrwxshEhFRPhz6qQDmxvqwNTNEz4b2asc3jmyF0Z3cZapKHl4z9qDxrL04fesR/Hddxf2EJ2gwYzf6rzghd2lERKSFGFRk1NzVqtquj3nrp1NYeeQWXll2FEIAFyPj1Z5/mJSOGzHJGl9782Eypm65hMg43ouIiKiqY1CpYJs+bI0e3nY4PqULdHUU1X7Pkfg8u+Ruu3BP+v2lefvRbclhjfcLemPFCaw/E4n31pytkBqJiEg+DCoVrIWbFVYOaY7aFkZyl6J1xm8MAgD8cixcOhYSlQgAOHnzES7cydmILvcWAGEF9LgQEVHVwcm0Mquh5P8Fef3vr4v4+/xd6XF0YhoeJadj0KpTADSvGspWCehyIi4RUZXEHhWZDWvjCiN9XdSzM8FH1WxirSZ5QwoATN8WjCNhD6XHz6+mP3s7Dg1m7MbvpyIqpD4iIqpY/Oe8zIwN9HB1bk8AQEp6FqIT0+DrbInp24Jlrkx7TNh4Ufr9+V1/Pl53ARlZKkzfFow+je2x7cI99PFxgKWxAWKS0lHL3LBU76lSCS6XJiLSAtzwTUu5TtkpdwlaaUBzR2w696zXxdRQD0lpWQCAmiYGiE3OgIu1MZo5W2LLhXtY/rYvejd2KNF7BN9LwFsrT2LCy/Uwon2dMq2fiIhK9v3NoR8t1dixeFvveztUrYBWlLwhBYAUUgAgNjnnXkIRj1Kx5ekKou8O3Cjxe0zZcgkpGdn4cufVF6iUiIjKAoOKlhrZIedf8t297GD6dMLt/k87qrUx0NPBa00dK7y2yuT6g6QSv6by9jESEVU9nKOipXo3doCPowUcLIzUVrTcXuCH7UH3MH1bMH4c0gwt3awxd8cVtdf+MLgpPvqzem/Vn9ft2BS41qwhdxlERFQK7FHRYk5WxhqX3b7apDaCZnRHG/eaGp/v1cgezVwsK6LESiE3yG27cA/j1l9AelY2ACDiUQriUzPyta/um/AREWkT9qhUUnlXpLR0s8Lp8Di15//6sDVSMrIQcP0hxq2/UNHlaZWMbBU2nLmDKVsuAwBCHyTh12EvoeNXhwDwjs5ERNqMQaUK0NSroqOjgKmhPvr6OKBTfRuYGOghI1uFRbuv49fj4RrOUnUdDYvF0bBY6fG16CS8kecmiAHXY9C5vi2yslXcOI6ISMtw6KcKyDtU8eeIlvmeNzPUh46OAob6unCwKN2+IlVNVEKa9Pvw1WeRma1Cx68O4ZVlR/EgMV2tbbaq5LNrz0c8xsjfzvHGiUREL4hBpQp4r60bAKCdR0209ahZaNshrV3wZjNHrBjcFANfcqqI8iqFzzZfwr34J7gWnYSHSc+CSvC9BDScuQc/Hbmp8XWPUzI0hpHXV5zA3isPMHYdJzUTEb0IbvhWRUTGpcLe3BB6uiXLntxYrnA+Tha4GBkPQPNcltzP7/TnXWFnZpjvuKWxPi7M6F7+hRIRVSLc8K0acrIyLnFIoWJ4LsfffZwKlYahoOB7CUWeKitbhSX7QnHq1qMyK4+IqKrjN1s1Z5rn7s0+jub4X/d6OPZZ5wLbT+hWryLK0hoX7z4LIBvP3kG7hQGYvPkS3l9zFl9suyw9l5KRXeS5Np27i28PhGHgT6fKpVYioqqIq36qOTMjfSSl52xDv21MWyiezswd1sYVQZHxWP62L46GxeJlLzvcj09Dw9pmyMxWYXlAybemr+yW7AsFkP8OzwAwbv0FNK5tDteaNbBsf5jG199+lFKu9RERVUXsUanmVrzTFA7mhlg2sIkUUgBgVl9vbBvTFo6WxhjUwhk1TZRo5GgOhUKB//WoL2PF8nl+NdDzOi0+BJVK4Jv9oRVUERFR1cegUs01drTAiald8WqT2iV6XVNnCwCAl73mSVC5K5Gqm/aLAgp8Lu8OLQ8S0zBjezDeX3O2VMufiYiqCwYVKpWVQ5rjf93rYc3wlzQ+P6OPFxwtjSq4Kvndi3+i9jg5/dndnbcF3ZN+bzn/AH47GYED12Jw8iYn1xIRFYRBhUrFxlSJsV3qwtas4A3kdn7cXvp934QOFVGW1snMFvjzdARG/nauwKGjwu7wHBX/BJV4BwEiohfGoEJl6viULrg4M/++IeZG+miXZzO6ua96V2RZspq2NRh7rzwo8Pm5O65gT0g0Npy5gxsxz0LLysM30WbBQXy9l3NeiKj6YlChMlXbwgjmRvoAgBpKXem4hbGBWrshrV0rsiyt9+Hv5zFly2V0W3JEOub/3zUAKPEKq9SMrKIbERFVEgwqVG70dHVwcWZ3XJzRHQZ6+f+onZnWFdN6NcD5L7rBw9ZEhgq1U2a2qtSvPXD1Abxm7JGWUufFISQiqowYVKhcmRvpw9w4p4fF2kS9V8XW1BAfdKgDaxMl9n/aUY7ytFLdaf/lu7VB4J3HGPNnoDRZt+/yY3CdshM3YpLV2k3fFgwA+PaA+l4uiWmZaLcwQG2TOiKiyoBBhSrMF35e6FTfBj8NaSZ3KZXOaz+cwM7L9/HJ+gt4lJyOS093zB39x3m1dpr6TG7EJGP8hiDci3+CP07dqYBqiYjKDnempRfW1dMWB67FwK+RfaHtbEyVWDO8RbHOuePjdjgc+hAj2ruh/he7y6LMKuFcxGNsOvdsZ9yHyenIyFJBoQCmbb2M+wlp+V7TbcnhiiyRiKhMMajQC1s2yBeHrsegc33bMjtnw9rmaFjbvMzOV5Us3H1N+j0+NRP1vvhPY7uAazH47qDm7fyJiCoLDv3QCzNR6qF3YwfUUL5Y7l36VhMAgP9rjQps89OQZtjyURu85lsbPk4WL/R+Vd3wNWcReCde7jKIiF4Ie1RIa/TzrY2eDWvBUF9X7fjm0W2w90o0xnb2gKlhzsTcps6WyFYJHL8Ri8aO5vg+4AZWHQ2Xo+xK7+TNR9h49g6m9/aCtYlS7nKIiNQwqJBWeT6kAEAzF0s0c7HMd1xXR4EO9WwAAM5WxmrPDWjuqDaXg575ZMMF3I5NwebRbaCnq4NBq04BAFQC+HaQr8zVERGpY1ChKmFgC2dEJaShuYslLIwN4OtkwaBSgO1BUQCAN1eexLcDnwWTyMepJT7XF9su49ztx9g2pq3GkElE9KIYVKhK0NfVwWc9PeUuo1K5cCde6k0BgLRMFXZcikKHejYwezrEVpTc5c57QqJLfAduIqLi4GRaqnaau1jCQLfoP/ofd/GogGrkdffxs7s9X72fiLHrLuC1H06o7WIb9iAJI9aeRfC9BDlKJKJqjkGFqpXbC/zw9+g2ODy5U5Ftx3Su+kFFkxsxyZj5Twgys1W4HZuCl785gv1XY9Dv++OFvi4jS4WE1MwKqpKIqgsGFaqyHC2NAADvtXUDACx+00d6zt7cSJaaKovfTkag46IAdFp8SDqWpSr8XkEvf3MYPnP2IiYx/6ZzzzsTHoeQKPbQEFHROEeFqqz9n3ZEbHI6HC2NMbln/XyTPSf3rI89IQ/Q07sWHiSmYZpfA4z+IxD7rz6QqWLtEqVhl1sAUKkETtx8hOsPkqRjOgoFIh7lTMY9EhaLN5o5FnjemKQ0DFh5EkBODxcRUWEYVKjKMtTXhaOlsfT78z7q5IGPOqkP79iYPrtxIm82nJ9KJdBn+TGERCWqHVcoin+O+/FF97gQEeXi0A9RHnnDidB4iz/AUF8Hs/p4Yc/4DhVUlfao8/mufCEFAB4mpUu/77/yAEN+OY3QPD0uAddicOXp60oSaoiI2KNClIdTno3jCupRqWVmiGFP573UMjNEdDHmZFR1s/+9Iv2+OyQaAPDuL2fwz8dtMXDlKdyKTQEA+DWyx6iO7rLUSESVE4MKUR7vt3PDw6R0dG1gCz1dzf/07+f7bL+QI5M7Y+6OK3CxNsaXO69WVJmVQnRiGlrMO6B2bOfl++jduPC7bBMR5cWhH6I8DPV1MauvN9rXtYFSTxc/v9scP77TVHre19lCbdmygZ4O5vZrCL88X77vt3ODuZE+Fr3euEJrryzy3v0ZAIQQavu2EBHlxR4VokJ087JTe9ypni30NWwWZ2n8bBLuZz098YVfAygUCjhaGeHtVafLvc7K5PajZ1v1CyHgNnUXAGDP+A5ITMuEmaE+6tcylas8ItIysvao+Pv746WXXoKpqSlsbW3Rr18/XL9+Xc6SiApV8ARbXRye1AlHJnWGgZ4OFE9njOrpPPtP7K3mTgWe99+x7cq20EoiLiVD+r3H0iN488eT6LH0CP48HVGs19+Lf4KEJ9xkjqgqkzWoHD58GGPGjMGpU6ewb98+ZGZmonv37khJSZGzLKICFbbnmYt1DThbq9/FuZmLJdrXrYl3W7sUet66diZlUV6lM2+X5nk907YG40ZMksbnbj1MRnpWNh4kpqHtgoPwmb23PEskIpnJOvSze/dutcdr1qyBra0tzp8/jw4dqt/ST9J+RiW8Q7CujgK/v98SALDoubkZADC6kztecrXUOJxUHWwJvFfgc/2+P4Hg2T3Ujh24+gDvrz0Hz1qmmPByvfIuj4i0gFbNUUlIyNlS28rKSuZKiNTN6O2FPSHRRfaMFGZ0J3fciElGHx8H2JoqkZGtQvu6NtLzS99qgv+C72NPCHfGBYDk9Cy4TtkJB3NDNKxtjvQsFQ6HPgQAXItOws5L92WukIgqgkJoyXR7lUqFvn37Ij4+HseOHdPYJj09HenpzzaWSkxMhJOTExISEmBmZlZRpRKVq8t3E9Bnueb/Bkiz4mzFHx6bgpn/hGBsZw+0cOM/hojklJiYCHNz82J9f2tNf/OYMWMQHByMDRs2FNjG398f5ubm0o+TU8GTE4kqq0aO5nKXUOm4TtmJrGwVfj95G0v3h0L1dDLRjO3B+OC3c7gRk4we3xzBkdCH0n2GiuP4jVi8uvwYb6BIJCOt6FEZO3Ystm/fjiNHjsDNza3AduxRoeqi0aw9SErLeuHzvN3SGetO3ymDirTfwJecsOFspPR42cAm+GRDkMa2ByZ2hLuNCVLSs3DzYTIa1TaHQqHAzkv3sfFcJJa+1QRWNQzgOmUnAMDWVIkz07pVxGUQVQuVpkdFCIGxY8di69atOHjwYKEhBQCUSiXMzMzUfoiqogOfdsQPg5vi4szucLepgQ871EHYvFew4+PiL2OuY1MDM3p7lWOV2iVvSAFQYEgBgK5fH0bAtRj0WX4MfZcfx67LOdv+j1kXiCOhD/HVHvWJz49TMzSdhogqgKxBZcyYMfjjjz+wbt06mJqaIjo6GtHR0Xjy5ImcZRHJztbMEL0a2cPcSB8HJnbC1F4NoK+rg4a11YeFlg1sgiOTOsNTwwZpy97yhaG+LvZ/2hE2psqKKr3S2HQuErce5myFsD1IffVRXEoGMrJUcpRFRM+RNaisWLECCQkJ6NSpE+zt7aWfjRs3ylkWkVZzfnrjRB9Hc7zapDacrY3ROM+8lum9vfBeWzc0rJ3T4+hha4LDkzrJUapWO/J0BREA7L3yAG89N3dlwqagCq6IiDSRdXmyFkyPIap01n3QEn+cuoPhbV01Pv9+u/xDqMYGWrUTgVZIychWe3w6PE76nUvEibSH1qz6IaLicbQ0xpRXPGFnZvjC5/rmLR/UtjAqg6qqtsxsgXvxTzB+wwVcuPM43/OHrsdgwI8nER6rvqs2/zFG9OIYVIiqGTszJc583hU35r2C/r6O+HNES7lLqhTaLjiIbUFR6P/DCQA5+7K8++sZXIlKxLDVZ3HmdhzGrb8gtd8edA8t5h9AoIZgQ0TFx/5goiogd95KYaa84okF/13DkgFNYJunN8bF2hietUxxLToJ6z5oCUtjA/x3+T7+vXQ/Xw8BPdN58SEA6nNd4lIysPXCXUzYeFE69uHv53F2WjcIIaSbVRJR8TGoEFUBI9rXQWxyBl72siuwzaiO7hje1hVKPfX7FSkUCuwa1x4AoKOT80XawN4MH3X2QMSjVNyJS8UHv50rv+KrmLwhBQAeJqXjdmwKxq4PhIO5EX56tznWHA/Hw+R0ZGULfNq9Xr7/Twryx6kILD94A3+MaAkP2+p5I0uqfrRiw7fSKsmGMURUejGJaej17VHEJufsJ1Lbwgj34qvvNgK/DmuO99bkD2/F+Vzeb+eGX46FS48/6+mJ0Z3ci/W+uRvQtXSzwsYPW5egYiLtUmk2fCOiysHWzBB/jWoDB3NDtHCzwtYxbfD1mz5ylyUbTSEFQLHCW96QAgARj1JwJPQhfj8VUez3z1IV79+X607fgf9/Vzmplyo1Dv0QUbG41ayBE1O7So9fb+aI2OR0+P93rZBXUVGEAN799QwAoL6dqdoNExPTMrHi0E309XFAA/tn/+pMSc/C6VuP0NzVCro6Bc97+XzrZQDAKw3t0cTJonwugKicsUeFiEptaBtX9PFxwLKBTXBmWld0rGeDRW80lp6vYVC8uRfVWWrms/1cBqw8KfV+CCEwf+dVrDh0E68sO6r2mmvRSXjrp1P4+eitYr1H4pPMsiuYqIIxqBBRqRnq6+K7Qb54tUlt2JoaYu17LTCg+bO7mvdqZC9jdZXDvxej1B53XXIY4bEpaLvgoNr9izQN36w/U/ANJ+Pz3J9od0g0Vh6+WQbVElU8BhUiKnPLBjZBSzcrTO7pKR0b3tYVu8e319h+7XstsGxgkwqqTrvdepiCzosPISohTe2429RdBb4mPjUDFyPj1Y6tyBNMcuaqXEPQc22IKgPOUSGiMvdqk9p4tUlttWN9fBzgWUvz7P6O9Wyk1+WubKHia7cwAMnpWVg3oiXaeNQEAKQ9d4sAAIhNSq/o0oheGHtUiKhCnfm8K/wKGRJ6fl7Lv2Pb4fqXPcu7rErp9qNUxCSmITk9CwDwX3A0unx9CB+vvwBNC4PORsTlP5iHSiXwfcANHL8RWx7lEpUKgwoRVQgrYwMAOUudlw5sAs9aphrb/f7clv6NHM2h1NPFrnHt4WD+4vc3qmpazD8g/f77qQjcepiCfy9GaVzuvPJw4ZNv/wuOxld7rmPwz6fLvE6i0mJQIaJytWJwU8zr3xCuNWtIx/R1dbD+g1ZwtTbG+G511do3dbbUeB4vBzO1EPPpy/XKp+Aq7ssdVzD73xC1ybnRCWlISc/CnbhUGSsj0oxzVIioXL1SwDCPZQ0DHJrUuUTncrcxwd4JHWBdwwDWJkos2RcqPbd7fHv0XHq0kFcDH3Vyxw+Hqvfql5+fbjjXqLY5XmvqiKj4J2iz4CAAFNjLpcmKQzeRnpWNoa1dYVnDoFxqJQIYVIiokqln9+zLdM3wl3DlfiJGd3Qv8oZ//X1rY3JPz2ofVHJ9uukiwmKSkXfV87XoJLU2h67HYMOZSMzs6wULIwMcDXuIth418TApHQt352z0t3R/GP4Z2xaNHS2K9b5L94ci8UkWZvTxKqtLoSqOQYWIKq1O9W3Rqb6t9LhbA1vsvxqDYW1c0d3LDq3qWKPO5znLent420n/uyfkgSz1apsVhYS2EzdjMWz1WQDAkbCHSH26iqh93Zr4X/f6am1/OxmBxW9aFPl+Qggs3R8GABjS2gVueYYDiQrCoEJEVcbyt5si+F4CfJ0t820t72RlDAD4sl8jBpVieHvVswm1qXmWOh8Ni8XE54JKdjHuPZSelY3tQc82t0t5ulKJqCgMKkSkddxtauDmwxQ4Pw0XxWWor4vmrlZqxzaObIWohCfwdjAHANiYKjW+ds3wl9CqjjU8p+8uXdHVSL/vj6s93hsSjd7fHUUPr1qwMjFAXx8HJKZlYeelKAxs4Qx9HR00mKH+uWZkq3D1fiLuPX6Cbl52Ja4hKS0TJkq9Iof8qPJTiEp8W82S3CaaiCqPyLhU/HIsHO+3c5N6QspSUGQ8ZmwPxvz+jRAUGQ9dHQUGtXBGWmY2g0o56OppiwPXYtSObfqwNQasPAkAeLOZI/xfawQ93fwLUeNSMnA/T9AEcoal3l51GoNbOmNe/0blWzyVi5J8fzOoEBE99SQjO9+//Kl8/PhOU4z6I1B6bG6kjyOTO8NEqScN26lUQppjtOPjdmhYOyesvL7iBM5HPAYA3F7gV8GVU1koyfc391EhInpKoHT/bpvQjXu6lFTekAIACU8yMeffK2g8aw82ns252eLf5+9Kz+fdLbesBnuOhD7EnpDoMjoblRcGFSKip4wN9PBa09pFN3xKR5GzN8u4rh5Y8JrmIYijk0u2V0x1tjnwLlIysvHZ5stwnbITkzdfkp7Lftr5r1IJBN55XOA5jt+IxdBfzyCyiM3rVCqBd389gw9/P4/YZN4DSZtxMi0RUR5LBjRBVrbAPxej0Lm+DQKuPwQA1KlZA7s+aS/NYenWwA7fDfKF0dN7Ew1s4YwmzhZYsjcUn3avV+ANGKl0Fu2+joBrMXC3Mcl3H6PEtEy8svQoWtaxwpbAewCA8RuDsHl0m3znUakEsoWATp5JuPGpmahponmSNcmPQYWI6DkLXm+Eng1roUM9GzScuUc6bqj/7IaJvRvbSyEll2ctM/z0bvNivYeHrQluxCSXTcHVxNnbj3H2tnpvSpfFh3D7UQpUAlJIAYCo+Ccaz9Fn+TE8SEzHsc/y9nQ9660JiUpEA3tTjRN7SR78f4KI6DnGBnro1cgeJso8/5Z7+g9wcyN9AECrOtalOne4fy8cntRJ47/2qeRuxaZovFP0/YQ0jfNPQqISEZucrhZq7iekAQC+2R+KPsuPYcqWy2qvScvMxuI91xEUGZ/vfCduxOLtVacQHpvyYhdCBeKqHyKiQrhO2QkAqGNTAwcndkJaZjaS0rIK3I9Fk4mbLmJz4F0MaeWCuf0aSsd3XIrC7uBoDGjuhHd/PVPmtRNw5vOuSHiSiQt34nEnLhXLA25obPdJ17pYdiBMeuzrbIHIuFT4v9YYwfcSpOeuze2p1rOW++cDABa81ggDWziX05VULVyeTERURr7eex3fHbyB399vgfZ1bUp1jowsFYIi4+HrbAH9AoYUcr/wahjoIiXPTrDvtHLGH6fulOp9qezp6ShwbW5PaWgob1ABuFy6uEry/c05KkREhZjYvT4+6uSRbz5KSRjo6aCFm1XRDQGcmNIVWy/cRURcKrp71UKrOlYY0soVPZYeKfX7U9nJUgkkPMmEhbEBMrNV+Z7v8vUhrBjcDPWLeSfq69FJsDVV8g7UhWCPChGRFrif8ARpmaoCb9TXa9lRXLmfCABqq5Go4i1/2xdj110o8HlTQz38/G5z7A6JhoGuDqb2agAAyMpWYeO5SCSlZaG5iyXMjPTR/ZucAFrdemLYo0JEVMnYmxsV+vzGD1thwsYg9PFxwKtNamPOv1ew9uRt6YaAbdytceLmo3yvM9DVQYaGf/lT6RUWUgAgKS0Lb/10SnrsaW+K/r6O+ONUBGb9e0U6/lZzpwLP8SQjG0o9Hejo8F5GXPVDRFQJmBrq4+ehL+HVJjkb0s3o44Wb83vh6OTO2DmuHdZ90Eqt/daP2uAlV0v8Nao1RndyL/TcX73RGPXsTMqt9upuwsaLyMpWYcel+2rHN56LzNf2QWIaYpLS0GDGbrzx4wkAgBACN2KSoFIJ3HmUinHrLyAkKqFCatcGHPohIqoiFu6+hn+CorDj43b55jzEpWRgd3A0Dlx9kO8GgeH+vaBQKDB9WzB+PxVRkSXTU3+Pao11p+9gy4V7ascdLY1w93HOnjBdPW0R+TgVoQ9y9t/RNFwUFf8EQ389g6FtXPFOK5div7//rqvYExKN7WPbwVSph/N3HsPbwQzGBuUz8MJVP0RE1ZQQAgpF4cMFj1MyYGakDwUAhQJS+31XHuCD385VQJVUWjoKSPvG3F7gh+T0LEQnpMHDNqdHbMy6QOx82nNzcmoXbAm8h4EvOUFPVwd/no5An8YOGu9Inrt6aXLP+jDQ1cGXO68+7ZErn/1+OEeFiKiaKiqkAChwhUm3Brb4c0RLGBvoov8POcMOX7/pg2yVULvvTq68X5pUMfJ+3lnZKnT66hBik9Ox9aM2qG1pJIUUABj40ylEPErFV3uuo6+PA/65GIVVR27hwozuAHI2sjsaFovW7s82L1SpBNafz1kOf/b242IF3/LGHhUiIipS8L0E/HMxCudux6FrAzv8eSoCGz9sjfaLAuQujUood8god6ivfd2aOBqWc3fq/3Wvh21BUdLtHbo1sMXPQ18q8xrYo0JERGWqYW1zNKxtLj0e09lDxmroRYU+SJLmI+WGFACISUpH3v6L/Vdj8r22onHVDxERldrSt5qoPZ7R20vt8XeDfNUeh8zuUd4lURFcp+yU9m953m8nI3DzoXbdt4g9KkREVGr9fGvD2EAXI38/j3n9G2LQS85wsjKGj6M5bM0MAQCJaZmYtjUYAFBDqYd5/Rti2tZgtHC1wpnbcXKWT5UAgwoREb2Q7t61EPrlKzDQy+mkf9nLTu35zvVt1R4PbumCwS1zls4+TsnArH9DsD0oSq3NsDauuJ/wBF/4eWFL4D18sz+0HK+AtBkn0xIRUbm7fDcB5kb6cLbOvzQWAGZuD8bakzlzJjTtDyKEQFRCGtouOCgdWzmkGT78/Xz5FEySsHmvFHgzzdIqyfc356gQEVG5a+RoXmBIAYBJPT3xcRcP/PdJe43PKxQK1LYwQpunS2ltTZXo4V0Ltxf4wcte/Yvu9/dbSL97FvPmgFSwgGvyTqjl0A8REcnORKmHid3rF9luxeBmWHfmDvr42EvHbM2UuPJ0+5CJL9dD+7o20nPutia4Fp1U5vVWJ3LfK4o9KkREVGmYG+tjdCd3OFo+650Z3THnXka9G9vj4651AQCbR7fGoBbOmNevId5v5ya1Hd+trvR7F0/1uTOk2YSNQbK+P+eoEBFRpfc4JQMWxvoF7qKarRK4EZOMenYm2HflAQKuP8Ssvl4IjIjHoFWn8rXvUM8GK99phgYzdpd36ZWCpnlDL4IbvhERUbVS0G0BcunqKFD/6XyV7t610N27FgCgtbs1zkzrCiN9Xey/+gATN11E78YOWDawSXmXTMXEoR8iIqrWbE0NYWqoj/6+jrg5vxe+HeQLhUIBhUKBw5M6qbXNvflfcQxtrX734j9HtCyLcqsd9qgQERE99fzQkYt1DWwY2Qr+u65ibr+GUAmg/w/HMaaTB27FJiPxSRaO3YjVeK7BrVwwq683Ptt8CSoBuORZ9bR5dBvYmipx6W4CxqwLLNdrquwYVIiIiArRqo41to9tJz0O/fLZviJBkfEag8rGka1Qzy5nqGnRGz4AgHvxT6Tn69cyhYlSD05Wxmju2hUt5x8oz0uo1Dj0Q0REVAJ5Nz9r4mSBZQObYOtHbaRjPo7maFnHOt/rTA2f9Q3o6z7rubEzM0TI7B5wsTbG4JbOCJ7dA12LWJFkUMYbsGkz9qgQERG9gFeb1AYAvNa0NrYE3sPYLnU1tjMz1McvQ5tDR0cBpZ6u2nM1lHo49L9O0tDTkgFN4DNnr/T88LaueLVJbfT/4TiEALo2sIWOQoGdl+/j0qzuCL6bgBpKPdiYKtFmwUF42Zvhyv3EMrm+3o3ti25Ujrg8mYiIqAwIIRCbnAEbU2WZnK/p3H2IS8nAnvEdpBVLN2KS8Nf5uxjVwb3AlU5JaZkwNtBD4pNMKPV14DVjj9rz8/s3wudbL2t8bes61jh56xEAYHLP+jA30kc7j5pwsa5RJteUqyTf3wwqREREWipbJaCro3lvmOK6+zgV7RYGAAA617fBr8NewpwdV7D/6gNExj2bN7N9TFvUr2WKOTuu4OUGduhcjhviMagQERGR5M/TEfjlaDjWvtcCTlbPVh9di05Ez6VH8b/u9QocsioPDCpERESktXj3ZCIiIqoSGFSIiIhIazGoEBERkdZiUCEiIiKtxaBCREREWotBhYiIiLQWgwoRERFpLQYVIiIi0loMKkRERKS1GFSIiIhIazGoEBERkdZiUCEiIiKtxaBCREREWotBhYiIiLSWntwFvAghBICc20UTERFR5ZD7vZ37PV6YSh1UkpKSAABOTk4yV0JEREQllZSUBHNz80LbKERx4oyWUqlUiIqKgqmpKRQKRZmeOzExEU5OToiMjISZmVmZnlsb8XqrNl5v1cbrrfqq2jULIZCUlAQHBwfo6BQ+C6VS96jo6OjA0dGxXN/DzMysSvyhKC5eb9XG663aeL1VX1W65qJ6UnJxMi0RERFpLQYVIiIi0loMKgVQKpWYOXMmlEql3KVUCF5v1cbrrdp4vVVfdbzmXJV6Mi0RERFVbexRISIiIq3FoEJERERai0GFiIiItBaDChEREWktBhUNvv/+e7i6usLQ0BAtW7bEmTNn5C6pSP7+/njppZdgamoKW1tb9OvXD9evX1drk5aWhjFjxsDa2homJiZ4/fXX8eDBA7U2d+7cgZ+fH4yNjWFra4tJkyYhKytLrc2hQ4fQtGlTKJVKeHh4YM2aNeV9eUVasGABFAoFxo8fLx2ratd77949vPPOO7C2toaRkREaNWqEc+fOSc8LITBjxgzY29vDyMgI3bp1Q1hYmNo54uLiMHjwYJiZmcHCwgLvv/8+kpOT1dpcunQJ7du3h6GhIZycnLBo0aIKub7nZWdnY/r06XBzc4ORkRHc3d0xd+5ctXuDVOZrPnLkCPr06QMHBwcoFAps27ZN7fmKvLa//voLnp6eMDQ0RKNGjbBr164Kvd7MzEx89tlnaNSoEWrUqAEHBwe8++67iIqKqpLX+7xRo0ZBoVBg6dKlascr0/WWK0FqNmzYIAwMDMSvv/4qQkJCxAcffCAsLCzEgwcP5C6tUD169BCrV68WwcHBIigoSPTq1Us4OzuL5ORkqc2oUaOEk5OTOHDggDh37pxo1aqVaNOmjfR8VlaWaNiwoejWrZu4cOGC2LVrl6hZs6aYOnWq1ObWrVvC2NhYfPrpp+LKlSviu+++E7q6umL37t0Ver15nTlzRri6uorGjRuLTz75RDpela43Li5OuLi4iGHDhonTp0+LW7duiT179ogbN25IbRYsWCDMzc3Ftm3bxMWLF0Xfvn2Fm5ubePLkidSmZ8+ewsfHR5w6dUocPXpUeHh4iEGDBknPJyQkCDs7OzF48GARHBws1q9fL4yMjMTKlSsr9HqFEGLevHnC2tpa7NixQ4SHh4u//vpLmJiYiGXLlkltKvM179q1S0ybNk1s2bJFABBbt25Ve76iru348eNCV1dXLFq0SFy5ckV88cUXQl9fX1y+fLnCrjc+Pl5069ZNbNy4UVy7dk2cPHlStGjRQjRr1kztHFXlevPasmWL8PHxEQ4ODuKbb76ptNdbnhhUntOiRQsxZswY6XF2drZwcHAQ/v7+MlZVcjExMQKAOHz4sBAi5y8CfX198ddff0ltrl69KgCIkydPCiFy/sPS0dER0dHRUpsVK1YIMzMzkZ6eLoQQYvLkycLb21vtvd566y3Ro0eP8r4kjZKSkkTdunXFvn37RMeOHaWgUtWu97PPPhPt2rUr8HmVSiVq1aolvvrqK+lYfHy8UCqVYv369UIIIa5cuSIAiLNnz0pt/vvvP6FQKMS9e/eEEEL88MMPwtLSUrr+3PeuX79+WV9Skfz8/MR7772nduy1114TgwcPFkJUrWt+/ousIq9twIABws/PT62eli1big8//LBMrzGvwr64c505c0YAEBEREUKIqnm9d+/eFbVr1xbBwcHCxcVFLahU5ustaxz6ySMjIwPnz59Ht27dpGM6Ojro1q0bTp48KWNlJZeQkAAAsLKyAgCcP38emZmZatfm6ekJZ2dn6dpOnjyJRo0awc7OTmrTo0cPJCYmIiQkRGqT9xy5beT6fMaMGQM/P798NVW16/3nn3/QvHlzvPnmm7C1tYWvry9WrVolPR8eHo7o6Gi1Ws3NzdGyZUu167WwsEDz5s2lNt26dYOOjg5Onz4ttenQoQMMDAykNj169MD169fx+PHj8r5MNW3atMGBAwcQGhoKALh48SKOHTuGV155BUDVvOZcFXlt2vJn/HkJCQlQKBSwsLAAUPWuV6VSYciQIZg0aRK8vb3zPV/VrvdFMKjkERsbi+zsbLUvLgCws7NDdHS0TFWVnEqlwvjx49G2bVs0bNgQABAdHQ0DAwPpP/pcea8tOjpa47XnPldYm8TERDx58qQ8LqdAGzZsQGBgIPz9/fM9V9Wu99atW1ixYgXq1q2LPXv2YPTo0Rg3bhzWrl2rVm9hf3ajo6Nha2ur9ryenh6srKxK9JlUlClTpmDgwIHw9PSEvr4+fH19MX78eAwePFitnqp0zbkq8toKaiPn33lpaWn47LPPMGjQIOkGfFXtehcuXAg9PT2MGzdO4/NV7XpfRKW+ezJpNmbMGAQHB+PYsWNyl1JuIiMj8cknn2Dfvn0wNDSUu5xyp1Kp0Lx5c8yfPx8A4Ovri+DgYPz4448YOnSozNWVj02bNuHPP//EunXr4O3tjaCgIIwfPx4ODg5V9popZ2LtgAEDIITAihUr5C6nXJw/fx7Lli1DYGAgFAqF3OVoPfao5FGzZk3o6urmWxny4MED1KpVS6aqSmbs2LHYsWMHAgIC4OjoKB2vVasWMjIyEB8fr9Y+77XVqlVL47XnPldYGzMzMxgZGZX15RTo/PnziImJQdOmTaGnpwc9PT0cPnwY3377LfT09GBnZ1elrtfe3h5eXl5qxxo0aIA7d+5IdebWltfz1xsTE6P2fFZWFuLi4kr0mVSUSZMmSb0qjRo1wpAhQzBhwgSpB60qXnOuiry2gtrIce25ISUiIgL79u2TelOAqnW9R48eRUxMDJydnaW/vyIiIjBx4kS4urpKdVaV631RDCp5GBgYoFmzZjhw4IB0TKVS4cCBA2jdurWMlRVNCIGxY8di69atOHjwINzc3NSeb9asGfT19dWu7fr167hz5450ba1bt8bly5fV/uPI/csi90uydevWaufIbVPRn0/Xrl1x+fJlBAUFST/NmzfH4MGDpd+r0vW2bds233Lz0NBQuLi4AADc3NxQq1YttVoTExNx+vRpteuNj4/H+fPnpTYHDx6ESqVCy5YtpTZHjhxBZmam1Gbfvn2oX78+LC0ty+36NElNTYWOjvpfUbq6ulCpVACq5jXnqshr05Y/47khJSwsDPv374e1tbXa81XpeocMGYJLly6p/f3l4OCASZMmYc+ePVKdVeV6X5jcs3m1zYYNG4RSqRRr1qwRV65cESNHjhQWFhZqK0O00ejRo4W5ubk4dOiQuH//vvSTmpoqtRk1apRwdnYWBw8eFOfOnROtW7cWrVu3lp7PXa7bvXt3ERQUJHbv3i1sbGw0LtedNGmSuHr1qvj+++9lX56cK++qHyGq1vWeOXNG6OnpiXnz5omwsDDx559/CmNjY/HHH39IbRYsWCAsLCzE9u3bxaVLl8Srr76qcTmrr6+vOH36tDh27JioW7eu2nLH+Ph4YWdnJ4YMGSKCg4PFhg0bhLGxsSzLk4cOHSpq164tLU/esmWLqFmzppg8ebLUpjJfc1JSkrhw4YK4cOGCACCWLFkiLly4IK1yqahrO378uNDT0xOLFy8WV69eFTNnziyX5auFXW9GRobo27evcHR0FEFBQWp/h+Vd0VJVrleT51f9VLbrLU8MKhp89913wtnZWRgYGIgWLVqIU6dOyV1SkQBo/Fm9erXU5smTJ+Kjjz4SlpaWwtjYWPTv31/cv39f7Ty3b98Wr7zyijAyMhI1a9YUEydOFJmZmWptAgICRJMmTYSBgYGoU6eO2nvI6fmgUtWu999//xUNGzYUSqVSeHp6ip9++knteZVKJaZPny7s7OyEUqkUXbt2FdevX1dr8+jRIzFo0CBhYmIizMzMxPDhw0VSUpJam4sXL4p27doJpVIpateuLRYsWFDu16ZJYmKi+OSTT4Szs7MwNDQUderUEdOmTVP74qrM1xwQEKDxv9mhQ4dW+LVt2rRJ1KtXTxgYGAhvb2+xc+fOCr3e8PDwAv8OCwgIqHLXq4mmoFKZrrc8KYTIs80jERERkRbhHBUiIiLSWgwqREREpLUYVIiIiEhrMagQERGR1mJQISIiIq3FoEJERERai0GFiIiItBaDChEVW6dOnTB+/Hi5y1CjUCiwbds2ucsgonLCDd+IqNji4uKgr68PU1NTuLq6Yvz48RUWXGbNmoVt27YhKChI7Xh0dDQsLS2hVCorpA4iqlh6chdARJWHlZVVmZ8zIyMDBgYGpX59ZboLLBGVHId+iKjYcod+OnXqhIiICEyYMAEKhQIKhUJqc+zYMbRv3x5GRkZwcnLCuHHjkJKSIj3v6uqKuXPn4t1334WZmRlGjhwJAPjss89Qr149GBsbo06dOpg+fbp0V9g1a9Zg9uzZuHjxovR+a9asAZB/6Ofy5cvo0qULjIyMYG1tjZEjRyI5OVl6ftiwYejXrx8WL14Me3t7WFtbY8yYMWp3oCUi7cGgQkQltmXLFjg6OmLOnDm4f/8+7t+/DwC4efMmevbsiddffx2XLl3Cxo0bcezYMYwdO1bt9YsXL4aPjw8uXLiA6dOnAwBMTU2xZs0aXLlyBcuWLcOqVavwzTffAADeeustTJw4Ed7e3tL7vfXWW/nqSklJQY8ePWBpaYmzZ8/ir7/+wv79+/O9f0BAAG7evImAgACsXbsWa9askYIPEWkXDv0QUYlZWVlBV1cXpqamakMv/v7+GDx4sDRvpW7duvj222/RsWNHrFixAoaGhgCALl26YOLEiWrn/OKLL6TfXV1d8b///Q8bNmzA5MmTYWRkBBMTE+jp6RU61LNu3TqkpaXht99+Q40aNQAAy5cvR58+fbBw4ULY2dkBACwtLbF8+XLo6urC09MTfn5+OHDgAD744IMy+XyIqOwwqBBRmbl48SIuXbqEP//8UzomhIBKpUJ4eDgaNGgAAGjevHm+127cuBHffvstbt68ieTkZGRlZcHMzKxE73/16lX4+PhIIQUA2rZtC5VKhevXr0tBxdvbG7q6ulIbe3t7XL58uUTvRUQVg0GFiMpMcnIyPvzwQ4wbNy7fc87OztLveYMEAJw8eRKDBw/G7Nmz0aNHD5ibm2PDhg34+uuvy6VOfX19tccKhQIqlapc3ouIXgyDChGVioGBAbKzs9WONW3aFFeuXIGHh0eJznXixAm4uLhg2rRp0rGIiIgi3+95DRo0wJo1a5CSkiKFoePHj0NHRwf169cvUU1EpB04mZaISsXV1RVHjhzBvXv3EBsbCyBn5c6JEycwduxYBAUFISwsDNu3b883mfV5devWxZ07d7BhwwbcvHkT3377LbZu3Zrv/cLDwxEUFITY2Fikp6fnO8/gwYNhaGiIoUOHIjg4GAEBAfj4448xZMgQadiHiCoXBhUiKpU5c+bg9u3bcHd3h42NDQCgcePGOHz4MEJDQ9G+fXv4+vpixowZcHBwKPRcffv2xYQJEzB27Fg0adIEJ06ckFYD5Xr99dfRs2dPdO7cGTY2Nli/fn2+8xgbG2PPnj2Ii4vDSy+9hDfeeANdu3bF8uXLy+7CiahCcWdaIiIi0lrsUSEiIiKtxaBCREREWotBhYiIiLQWgwoRERFpLQYVIiIi0loMKkRERKS1GFSIiIhIazGoEBERkdZiUCEiIiKtxaBCREREWotBhYiIiLQWgwoRERFprf8DTspnytZs+nAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(encoder.state_dict(), 'encoder.pth')\n",
        "torch.save(decoder.state_dict(), 'decoder.pth')\n",
        "\n",
        "# GreedySearchDecoder 객체 저장\n",
        "torch.save(searcher, 'searcher.pth')"
      ],
      "metadata": {
        "id": "wXCFVfpPMdjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sb-qPg-Rcq15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}